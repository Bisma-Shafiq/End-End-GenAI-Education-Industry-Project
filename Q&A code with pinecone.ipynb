{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "import pinecone\n",
    "import langchain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure Google Generative AI\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY not found. Please set it in your .env file.\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Initialize Language Model\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash', google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='1\\nParameter-Efficient Fine-Tuning Methods for\\nPretrained Language Models: A Critical\\nReview and Assessment\\nLingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee Wang\\nAbstract—With the continuous growth in the number of\\nparameters of transformer-based pretrained language models\\n(PLMs), particularly the emergence of large language models\\n(LLMs) with billions of parameters, many natural language\\nprocessing (NLP) tasks have demonstrated remarkable success.\\nHowever, the enormous size and computational demands of\\nthese models pose significant challenges for adapting them\\nto specific downstream tasks, especially in environments with\\nlimited computational resources. Parameter Efficient Fine-Tuning\\n(PEFT) offers an effective solution by reducing the number\\nof fine-tuning parameters and memory usage while achieving\\ncomparable performance to full fine-tuning. The demands for\\nfine-tuning PLMs, especially LLMs, have led to a surge in the\\ndevelopment of PEFT methods, as depicted in Fig. 1. In this\\npaper, we present a comprehensive and systematic review of\\nPEFT methods for PLMs. We summarize these PEFT methods,\\ndiscuss their applications, and outline future directions. Further-\\nmore, we conduct experiments using several representative PEFT\\nmethods to better understand their effectiveness in parameter\\nefficiency and memory efficiency. By offering insights into the\\nlatest advancements and practical applications, this survey serves\\nas an invaluable resource for researchers and practitioners\\nseeking to navigate the challenges and opportunities presented\\nby PEFT in the context of PLMs.\\nIndex Terms—Parameter-efficient, fine-tuning, pretrained lan-\\nguage model, large language model, memory usage.\\nI. I NTRODUCTION\\nT\\nRANSFORMER-BASED PLMs [1], [2], [3], [4] have\\ndemonstrated remarkable performance across a wide\\nrange of NLP tasks. To fully harness the potential of PLMs,\\nfine-tuning is employed to adapt the PLMs to task-specific\\ndata to enhance performance on downstream tasks. However,\\ntraditional fine-tuning involves updating all the pretrained\\nparameters of PLMs, which is time-consuming and computa-\\ntionally expensive. As the size of PLMs continues to increase,\\nfrom models like BERT [1] with 110 million parameters to\\nT5 [4] with 770 million parameters, computational resource\\nrequirements become a significant challenge. The advent of\\nThis work was supported by a research grant entitled ”Medical Text Feature\\nRepresentations based on Pre-trained Language Models” (871238) and Faculty\\nResearch Grant (DB24A4) at Lingnan University, Hong Kong.(Corresponding\\nauthor: Haoran Xie.)\\nLingling Xu and Fu Lee Wang are with the Hong Kong Metropolitan Uni-\\nversity, Hong Kong (email: xxiao199409@gmail.com; pwang@hkmu.edu.hk).\\nHaoran Xie and Si-Zhao Joe Qin are with Lingnan University, Hong Kong\\n(email: hrxie@ln.edu.hk; joeqin@ln.edu.hk).\\nXiaohui Tao is with University of Southern Queensland, Queensland,\\nAustralia (email: xtao@usq.edu.au).\\nLLMs [5], [6], [7], exemplified by Falcon [8] with a stag-\\ngering 180 billion parameters, further exacerbates the com-\\nputational demands. To perform task-specific full fine-tuning\\nwith Falcon-180B, a minimum of 5120GB of computational\\nresources may be required 1. The enormous computational\\nresource requirements are prohibitive for anyone but the su-\\nperpower players to utilize LLMs for task-specific fine-tuning.\\nTo address this challenge, a prominent method known as\\nPEFT [9] has emerged as a viable solution to compensate\\nfor the tremendous computational cost of full parameter\\nfine-tuning. PEFT involves employing various deep learning\\ntechniques [9], [10], [11] to reduce the number of trainable\\nparameters while still maintaining comparable performance to\\nthe full fine-tuning. In addition, PEFT updates only a small\\nnumber of additional parameters or updates a subset of the\\npretrained parameters, preserving the knowledge captured by\\nthe PLM while adapting it to the target task and reducing\\nthe risk of catastrophic forgetting. Furthermore, since the size\\nof the fine-tuned dataset is typically much smaller than the\\npretrained dataset, performing full fine-tuning to update all\\nthe pretrained parameters may lead to overfitting, which is\\ncircumvented by the PEFT through selectively or not updating\\npretrained parameters.\\nRecently, there has been a significant surge in interest\\nregarding PEFT methods, as demonstrated by the growing\\nnumber of studies depicted in Fig. 1. This also leads to a\\nfew surveys on PEFT approaches for the PLMs. However,\\nthe existing surveys have certain limitations. Ding et al.\\n[12] conducted a comprehensive study on PEFT methods,\\nbut this survey did not cover much of the latest work in\\nthe field and only four PEFT methods were quantitatively\\nexperimented with. Lialin et al. [13] delved into the ideas and\\noperational implementations of PEFT methods in detail but\\ndo not perform relevant experiments. In this work, we address\\nthese gaps comprehensively. We meticulously categorize the\\nPEFT methods, providing detailed explanations of the ideas\\nand specific implementations of each method. We compare\\nthe similarities and differences among various types of PEFT\\nmethods, facilitating a better understanding of the evolving\\nlandscape of PEFT. Moreover, we conduct extensive fine-\\ntuning experiments with 11 representative PEFT methods.\\nIn this paper, we aim to provide a comprehensive and\\nsystematic study of PEFT methods for PLMs in NLP. We\\nundertake an in-depth exploration of these PEFT methods and\\n1https://huggingface.co/blog/falcon-180b#hardware-requirements\\narXiv:2312.12148v1  [cs.CL]  19 Dec 2023'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='2\\nMAM \\nAdapter \\nProPETL \\n2019 \\n2020 \\n2021 \\n2022 \\n2023 \\nAdapter \\nDrop \\nHyperFormer++ \\nResidual \\nAdapter \\n(IA) 3 \\nPrefix- \\ntuning \\nMPT \\nA TTEMPT \\nSPoT \\nLoRA \\nKronA \\nIntrinsic \\nSAID \\nSAM \\nPrompt- \\ntuning W ARP \\nUniPEL T \\nAutoPEFT \\nThreshold \\nMASK \\nBitFit \\nFISH \\nMask \\nChild \\ntuning \\nL T -FST \\nAdditive Fine-tuning\\nHybrid Fine-tuning\\nSequential \\nAdapter \\nDif f \\nPruning \\nAdapter \\nFusion \\nParallel \\nAdapter \\nKernel-mix-lite(qvo) \\nP-tuning \\nP AST A \\nAttention \\nFusion \\nT iny-Attn \\nAdapter \\nLST \\nCoDA \\nMerA \\nAdapterSoup \\nHardamard \\nAdapter \\nU/S-SAM \\nSparse \\nAdapter \\nCompacter \\nS 3 Delta-M \\nS 4 \\nAdaMix \\nDyLoRA \\nLOFTQ \\nLoRA-F A \\nLaplace-LoRA \\nMoELoRA \\nLoRAPrune \\nIncreLoRA U/S-BitFit \\nQLoRA \\nQA-LoRA \\nL-LoRA \\nLoRA-Hub \\nAdaLoRA \\nDelta-LoRA \\nPartial Fine-tuning\\nReparameterized Fine-tuning\\nUnified Fine-tuning\\nFig. 1: The evolutionary development of PEFT methods in recent years. Models on the same branch have some common\\nfeatures. The vertical position of the models shows the timeline of their release dates. Notably, the year of the paper’s initial\\npublication is shown as the reference. For instance, if a paper is published in ACL 2022 but listed on arXiv in 2021, the year\\n2021 will be considered as the reference date.\\npresent a comprehensive taxonomy scheme in Section III. By\\ncategorizing PEFT methods into additive fine-tuning, partial\\nfine-tuning, reparameterized fine-tuning, hybrid fine-tuning,\\nand unified fine-tuning, we establish a structured framework\\nfor understanding these PEFT approaches, as depicted in\\nFig. 2. In Section IV, we conduct quantitative investigations\\nand analyses to assess the performance, parameters efficiency,\\nand memory usage of these PEFT approaches. Our quantitative\\nstudies primarily focus on natural language understanding\\n(NLU), machine translation (MT), and natural language gen-\\neration (NLG) tasks. Additionally, we extensively explore\\nthe applications of PEFT in multi-task learning, cross-lingual\\ntransfer, and backdoor attack and defense, underscoring its\\neffectiveness. Furthermore, our research also unveils potential\\ndirections for future investigations in this rapidly evolving\\nfield. To summarize, the main contributions of this survey can\\nbe outlined as follows:\\n• We present a comprehensive analysis and review of PEFT\\nmethods for transformer-based PLMs.\\n• We identify the key techniques and approaches employed\\nin PEFT methods, and classify them into additive, partial,\\nreparameterized, hybrid, and unified fine-tuning methods.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='3\\nPEFT Methods for PLMs\\nAdditive\\nFine-tuning\\nAdapter-based\\nFine-tuning\\nSequential Adapter [9], Residual Adapter [14], CoDA [15], Parallel Adapter [16], AdapterDrop [17],\\nTiny-Attn Adapter [18], AdapterFusion [19], MerA [20], Hyperformer++ [21], AdapterSoup [22]\\nSoft Prompt-based\\nFine-tuning W ARP [23], Promt-tuning [24], Prefix-tuning [10], P-tuning [25], SPOT [26], ATTEMPT [27], MPT [28]\\nOthers LST [29], (IA) 3 [30], PASTA [31], AttentionFusion [32], Hadamard Adapter [33]\\nPartial\\nFine-tuning\\nBias Update BitFit [34], U/S-BitFit [35],\\nPretrained Weight\\nMasking Threshold-Mask [36], FISH Mask [37]\\nDelta Weight\\nMasking LT-SFT [38], Child-Tuning [39], Diff Pruning [40], SAM [41]\\nReparameterized\\nFine-tuning\\nLow-rank\\nDecomposition Intrinsic SAID [42], LoRA [11], KronA [43]\\nLoRA Derivatives\\nLow-rank Adjustment DyLoRA [44], AdaLoRA [45], IncreLoRA [46]\\nLoRA-guided Pretrained\\nWeight Update Delta-LoRA [47], LoRAPrune [48]\\nQuantization Adaption QLoRA [49], QA-LoRA [50], LOFTQ [51]\\nLoRA-based\\nImprovements Kernel-mix-lite(qv)/(qvo) [52], Laplace-LoRA [53], LoRA-FA [54]\\nLoRA-based\\nMulti-task Fine-tuning LoRAHub [55], MoELoRA [56], L-LoRA [57]\\nHybrid\\nFine-tuning\\nManual Combination MAM Adapter [16], U/S-MAM [35], Compacter [58], UniPELT [59],\\nAutomatic Combination AutoPEFT [60], S 3Delta-M [61], S4 [62]\\nUnified\\nFine-tuning AdaMix [63], SparseAdapter [64], ProPETL [65]\\nFig. 2: Taxonomy of Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models.\\n• We conduct extensive experiments to evaluate the effec-\\ntiveness of several representative PEFT methods, specifi-\\ncally examining their impact on parameter efficiency and\\nmemory usage.\\nII. P RELIMINARIES\\nA. Transformer\\nTransformer [66] has emerged as a foundational architecture\\nfor numerous PLMs, it adopts an encoder-decoder architecture,\\ncomprised of a stack of encoder and decoder layers, each\\nequipped with the self-attention mechanism. Both the encoder\\nand decoder in the Transformer architecture consist of a multi-\\nhead self-attention layer and a feed-forward network (FFN)\\nlayer, interconnected by a residual connection [67] followed\\nby layer normalization [68]. Residual connection allows the\\nmodel to effectively propagate information from one layer\\nto the subsequent layer without losing valuable information.\\nLayer normalization further stabilizes the training process by\\nnormalizing the inputs of each layer.\\nMulti-head self-attention layer employs the self-attention\\nfunction with h heads in parallel. For an input sequence\\nX ∈ Rn×d with the sentence length n and hidden dimension\\nsize of d. The query ( Q), key ( K), and value ( V) vectors are\\nthe transformation of input sequence X,\\nK = XWk + bk, Q = XWq + bq, V = XWv + bv, (1)\\nwhere Q, K, V∈ Rn×d, bk, bq and bv are typically learnable\\nparameter vectors that help model to better capture specific\\ninformation in the input vector X and adjust the value of the\\nquery vector Q to better match the key vector K, thereby\\nimproving performance of the self-attention mechanism. The\\nself-attention output of input X is computed as:\\nAttn(Q, K, V) = Softmax(QKT\\n√\\nd\\n)V, (2)\\nthen multi-head self-attention can be described as follows:\\nMHA(Q, K, V) = Concat(head1, ··· , headh)WO, (3)\\nheadi = Attn(QWi\\nQ, KWi\\nK, V Wi\\nV ). (4)\\nWhile the FFN consists of two linear transformations with\\na non-linear ReLU activation function in between:\\nFFN(X) = ReLU(XW1 + b1)W2 + b2, (5)\\nwhere W1, b1, W2 and b2 are the weight matrices of two\\nlinear transformations. Most PEFT methods primarily focus on\\nthe self-attention layer and FFN layer, allowing models like\\nencoder-based RoBERTa [2], encoder-decoder-based T5 [4],\\nand decoder-based LLaMA [7] to leverage relevant techniques\\nfor parameters reduction.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='4\\nB. Full Fine-tuning of PLMs\\nFull fine-tuning of transformer-based PLMs involves train-\\ning the entire model, including all layers and parameters, on\\na specific downstream task using task-specific data. Initially,\\nPLMs are trained on large-scale datasets with unsupervised\\nlearning objectives like language modeling or masked lan-\\nguage modeling, to learn general language representations [1],\\n[2], [4], [7]. However, these PLMs may not perform optimally\\nwhen applied to specific tasks like sentiment analysis, question\\nanswering, or translation due to a lack of appropriate domain\\nknowledge [69], [70], [71]. Full fine-tuning provides an effec-\\ntive solution to address this limitation.\\nDuring full fine-tuning, the PLM is initialized with pre-\\ntrained weights and subsequently trained on task-specific data\\nusing techniques like backpropagation and gradient descent\\n[72], [73]. All model parameters, including pretrained weights,\\nare updated to minimize a task-specific loss that quantifies\\nthe disparity between predicted outputs and ground truth. In\\nthis way, full fine-tuning enables the model to learn task-\\nspecific patterns and nuances from the labeled data, facili-\\ntating predictions or outputs tailored to the target tasks [74].\\nNotably, full fine-tuning necessitates substantial computational\\nresources and labeled data, as the model is trained from scratch\\nfor the specific target task. Moreover, as PLMs grow in size\\nand with the advent of LLMs containing billions of parameters,\\nfull fine-tuning places even greater demands on computational\\nresources. In contrast, PEFT methods aim to alleviate these re-\\nquirements by selectively updating or modifying specific parts\\nof the PLMs while still achieving performance comparable to\\nfull fine-tuning [34], [39]. Furthermore, full fine-tuning may\\ngive rise to overfitting when the task-specific dataset is small\\nor when the PLMs are already well-suited to the target task\\n[19], [75].\\nIII. P ARAMETER -EFFICIENT FINE -TUNING METHODS\\nA. Additive Fine-tuning\\nAdditive fine-tuning approaches involve introducing new\\nextra trainable parameters for task-specific fine-tuning. We\\nclassify additive fine-tuning into three groups: Adapter-based\\nFine-tuning [9], [14], [15], [16], [17], [18], [19], [20], [21],\\n[22], [76], in which the adapter module is incorporated into\\nthe transformer, allowing for fine-tuning without modifying\\nthe pretrained parameters, Soft Prompt-based Fine-tuning\\n[10], [23], [24], [25], [26], [27], [28], where soft prompts or\\nprefix vectors are appended to the input embeddings or hidden\\nstates during fine-tuning, and Others [29], [30], [31], [32],\\n[33], in which various methods that introduce supplementary\\nparameters for model fine-tuning are fall into this category.\\n1) Adapters-based Fine-tuning: The idea of Adapter is\\nfirst introduced in multi-domain image classification [77],\\nallowing for the efficient transfer of knowledge across multiple\\nvisual domains. Sequential Adapter [9] extends and applies\\nit to NLP tasks by inserting the adapter (trainable modules)\\ninto the transformer block and fine-tuning the parameters\\nof adapters to make the PLMs adapt to the downstream\\ntasks. Specifically, adapter networks are inserted after the\\nself-attention layer and feed-forward layer of the Transformer\\nsequentially. Each adapter are low-rank module that consists\\nof a down-projection, a non-linear activation function, and an\\nup-projection as well as a residual connection. For the inputX,\\nthe output of a sequential adapter with the ReLU non-linear\\nactivation function can be defined with Equation 6. During\\nfine-tuning, only the parameters of adapter network Wup and\\nWdown need to be updated to make the PLMs adapt to the\\nspecific downstream tasks. The specific architecture of the\\nsequential adapter is presented in Fig. 3.\\nX = (ReLU(XWdown))Wup + X, (6)\\nWdown ∈ Rd×k, Wup ∈ Rk×d.\\nInspired by sequential adapter, many adapter-based PEFT\\nmethods have been proposed. Residual Adapter [14] fur-\\nther improves efficiency by inserting the adapter module\\nonly after the feed-forward and layer normalization. Parallel\\nAdapter [16], [76] inserts the adapter network in parallel\\nwith both the attention layer and the feed-forward layer,\\nallowing for more efficient integration of the adapter module\\ninto the transformer. AdapterDrop [17] removes adapters\\nin each layer of the transformer that are not important to\\nthe given task to improve inference efficiency. While CoDA\\n(Condition Adapter) [15] employs parallel adapter for task-\\nspecific parameter fine-tuning and remains most pretrained\\nparameters fixed. However, unlike prior methods in which all\\ninput tokens are processed with pretrained transformer, CoDA\\nutilizes a router function to select k important input tokens\\nfor conditional computation. In this way, CoDA not only\\nenhances parameter efficiency but also inference efficiency.\\nTiny-Attn Adapter (Tiny-Attention Adapter) [18] introduces\\na dot-product attention module between the down- and up-\\nprojections, which can also be seen as a multi-head attention\\nmodule with its per-head dimensionality to be extremely small.\\nMoreover, the Tiny-Attn Adapter regards its multiple attention\\nheads as a mixture of experts and averages their weights to\\nfurther reduce inference costs. Akin to the sequential adapter,\\nthe Tiny-Attn Adapter is also injected right after the multi-\\nhead attention layer.\\nAdapterFusion [19] integrates multiple task-specific\\nadapters into a single adapter module, allowing for effective\\nknowledge transfer across related tasks without modifying\\nthe original pretrained model. AdapterFusion provides a prac-\\ntical and efficient approach to task composition, enabling\\nthe transferability of pretrained models across multiple tasks\\nwhile minimizing the computational costs associated with fine-\\ntuning the entire model. However, AdapterFusion requires\\nadditional trainable parameters in the composition layers,\\nincreasing computational costs. MerA (Merging Pretrained\\nAdapters) [20] adopts summation and averaging strategies to\\nmerge the parameters of pretrained adapters without introduc-\\ning extra trainable parameters. It employs the optimal transport\\nmethod [78], [79] to align the parameters of adapters based\\non weights and activations, which gives better performance\\nwith fewer trainable parameters compared to AdapterFusion.\\nHyperformer++ [21] utilizes the shared hypernetwork [80]\\nto learn task-specific and layer-specific adapter parameters\\nthat condition on task and layer id embeddings. By sharing'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='5\\nMulti-Head Attention\\nLayer Normalization\\nLayer Normalization\\nFeed-Forward Network\\n+\\n+\\nAdapter Network\\nAdapter Network\\nNonlinear \\nActivation\\n+\\nDown-projection\\nUp-projection\\nkd\\nk d\\n(a) Sequential Adapter\\nMulti-Head Attention\\nLayer Normalization\\nLayer Normalization\\nFeed-Forward Network\\n+ \\n+ \\nHidden States\\nW q W k W v \\nQ K V P k P v \\nAttention\\nPr efix-tuning (b) Prefix-tuning\\nMulti-Head Attention\\nLayer Normalization\\nLayer Normalization\\nFeed-Forward Network\\n+ \\n+ \\nHidden States\\nW q W k W v \\nQ K V \\nAttention LoRA \\nLoRA \\nLoRA \\n+ + + \\nDown- \\nprojection \\nUp- \\nprojection * \\nLoRA (c) LoRA\\nFig. 3: The detailed architecture of (a) Sequential Adapter, (b) Prefix-tuning, and (c) LoRA.\\nknowledge across tasks via hypernetworks while enabling the\\nmodel to adapt to each task through task-specific adapters,\\nsignificantly reducing the number of trainable parameters.\\nAdapterSoup [22] is developed to address cross-domain task\\nadaptation, which first trains multiple adapters based on var-\\nious domains and then employs domain clustering [81] to\\nselect the most appropriate top-k adapters for the new domain.\\nFine-tuning parameters for the new domain in AdapterSoup\\nare determined by calculating the weighted average of the\\nselected k adapters. Apart from cross-domain task adaptation,\\nAdapterSoup can also be used to strengthen in-domain results\\nvia weight averaging of adapters trained on the same domain\\nbut with different hyperparameters.\\n2) Soft Prompt-based Fine-tuning: Soft prompt fine-tuning\\nis a class of methods in which trainable continuous vectors,\\nknown as soft prompts, are inserted into the input or hidden\\nstate of the model. Unlike manually designed hard prompts,\\nsoft prompts are generated by searching for prompts in a\\ndiscrete token space based on task-specific training data. Soft\\nprompts exhibit more flexibility and adaptability during fine-\\ntuning, as these prompts can be optimized and adjusted based\\non the specific task and training data.\\nW ARP (Word-level Adversarial RePrograming) [23] in-\\nserts special prompt tokens [P1], [P2], ··· , [Pl] and [Mask]\\ntoken before or after the sentences relying on the prompt\\ntemplate. The training objective is to minimize the cross-\\nentropy loss between the output of MLM and the verbalizer\\ntokens [V1], [V2], ··· , [Vc] for classes {1, 2, ··· , c}. Only the\\nparameters of [P1], [P2], ··· , [Pl] and [V1], [V2], ··· , [Vc] are\\ntrainable, resulting in a significant reduction in the number\\nof fine-tuning parameters. Prompt-tuning [24] incorporates\\nadditional l learnable prompt tokens, P = [P1], [P2], ··· , [Pl],\\ninto the model input X ∈ Rn×d and then concatenates them to\\ngenerate the final input ˆX, the new input can be expressed with\\nEquation 7. During fine-tuning, only the prompt parameters\\nof P are updated through gradient descent, while pretrained\\nparameters remain frozen. Thus, the parameter cost of prompt-\\ntuning is determined by multiplying the prompt length by\\nthe token embedding dimension, and extending the prompt\\nlength beyond a single token is critical for achieving good\\nperformance.\\nˆX = Concat(P, X) = [P, X] ∈ R(l+n)×d. (7)\\nPrefix-tuning [10] proposes to prepend soft prompts P =\\n[P1], [P2], ··· , [Pl] (l denotes the length of the prefix) to\\nthe hidden states of the multi-head attention layer, differing\\nfrom prompt-tuning that adds soft prompts to the input. To\\nensure stable training, a FFN is introduced to parameterize\\nthe soft prompts, as direct optimization of the soft prompts\\ncan lead to instability. Two sets of prefix vectors ˆPk and\\nˆPv are concatenated to the original key ( K) and value\\n(V ) vectors of the attention layer. The self-attention mech-\\nanism with prefix-tuning can be represented by Equation 8.\\nDuring training, only ˆPk, ˆPv, and the parameters of FFN\\nare optimized, while all other parameters of PLMs remain\\nfrozen. The structure of prefix-tuning is illustrated in Fig. 3.\\nAfter training, the FFN is discarded, and only Pk and Pv\\nare used for inference. P-tuning [25] also considers insert-\\ning the soft prompts [P1], ··· , [Pi], [Pi+1], ··· , [Pl] into the\\nmodel input. Nonetheless, P-tuning differs by concatenating\\nthese prompts to form a template and maps it to obtain\\n{h1, ··· , hi, e(x), hi+1, ··· , hl, e(x)}, in which e represents\\npretrained embedding layer. The training goal is to optimize\\nthe continuous prompts {h1, ··· , hl}. As the weights of PLMs\\nare fixed and only a few parameters need to be fine-tuned,\\nthe template can be effectively learned in few-shot learning\\nscenarios. P-tuning employs a bidirectional long short-term\\nmemory network (LSTM) with a ReLU-activated multilayer\\nperceptron (MLP) to initialize the embedding of soft prompts\\nthrough MLP(LSTM(h1, ··· , hi): LSTM(hi, ··· , hl)).\\nhead = Attn(XWq, [ ˆPk, XWk], [ ˆPv, XWv]), (8)\\nˆPk = FFN(Pk), ˆPv = FFN(Pv). (9)\\nSPOT (Soft Prompt Transfer) [26] is a multitask prompt\\nmethod that builds upon the prompt-tuning, in which “prompt\\npertaining” is introduced between PLMs and prompt-tuning\\nof target tasks. There are two variants of SPOT: generic'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='6\\nSPOT and targeted SPOT. Generic SPOT first learns a generic\\nprompt on one or more source tasks and then employs the\\nlearned prompt to initialize target prompts for specific target\\ntasks. Targeted SPOT learns separate prompts for various\\nsource tasks, creating a source prompt library. Subsequently,\\nthe optimal source prompt, which exhibits higher similarity to\\nthe target task embedding, is retrieved and used to initialize\\nthe target prompt for the target task. ATTEMPT (ATTEntional\\nMixtures of Prompt Tuning) [27] begins by pretraining trans-\\nferable soft prompts (source prompts) on large-scale source\\ntasks that possess valuable knowledge applicable to other\\ntasks. The new target prompt is initialized specifically for a\\ngiven target task. ATTEMPT employs a shared and lightweight\\nnetwork that is trained simultaneously to learn an attention-\\nweighted combination of source prompts and target prompt.\\nThis enables modular multi-task learning, as pretrained soft\\nprompts can be flexibly combined, reused, or removed to\\nleverage knowledge from different tasks. MPT (multitask\\nprompt tuning) [28] utilizes multitask data to decompose and\\ndistill knowledge from the source prompts to learn a single\\nshared prompt. MPT then learns a multiplicative low-rank\\nmatrix to update the shared prompt, efficiently adapting it to\\neach downstream target task. Specifically, MPT assumes that\\nthe soft prompts for the k-th source task, denoted as ˆPk, can be\\ndecomposed into the shared prompts across all source tasks P∗\\nand a task-specific low-rank matrix Wk. The decomposition is\\ngiven by ˆPk = P∗ ⊙Wk = P∗ ⊙(uk ⊗vT\\nk ), where ⊙ denotes\\nthe Hadamard product, ⊗ denotes the Kronecker product, and\\nuk and vk are task-specific vectors for the task k.\\n3) Others: Apart from adapters family and soft prompts\\nfine-tuning methods, there are some other approaches that also\\nincorporate extra trainable parameters during fine-tuning. They\\ninvolve adding a ladder side network operating alongside the\\ntransformer, introducing an additional diff vector to rescale\\nthe attention, incorporating an extra vector to the special token\\nrepresentations, using the late fusion technique to integrate ad-\\nditional attention weight, or combing an extra joint importance\\nweight for each token representation.\\nLST (Ladder Side-Tuning) [29] trains a ladder side network\\nin conjunction with the pretrained network and takes interme-\\ndiate activations as input via shortcut connections, known as\\nladders, from pretrained network. Since all training parameters\\nare stored in the ladder side network, back-propagation is\\nachieved through side networks and ladder connections rather\\nthan pretrained networks, reducing the number of fine-tuned\\nparameters. In addition, LST further boosts parameter effi-\\nciency by utilizing structural pruning [82] to retrieve a smaller\\npruned network to initialize the side network and dropping\\ncertain layers of the side network. (IA)3 (Infused Adapter\\nby Inhibiting and Amplifying Inner Activations) [30] lever-\\nages learned vectors to scale activations, leading to improved\\nperformance while introducing a relatively small number of\\nnew parameters. (IA)3 introduces three learned vectors, lk, lv,\\nand lff , to rescale the key (K) and value (V) vectors in the\\nattention networks and hidden activations in the position-wise\\nFFN. As a result, the attention output and hidden activation\\noutput can be rescaled using the following expressions:\\nAttn(Q, K, V) = (Q(lk ⊙ KT )√dk\\n)(lv ⊙ V ), (10)\\nFFN(X) = (lff ⊙ γ(XW1))W2, (11)\\nin which ⊙ represents element-wise multiplication, W1 and\\nW2 are the weight matrices of FFN, and γ is activation\\nfunction. (IA)3 only optimizes three learned vectors lk, lv, and\\nlff for each transformer block, resulting in great parameter\\nefficiency. Notably, (IA)3 incurs minimal overhead because lk\\nand lv can be seamlessly integrated into the corresponding\\nlinear layers, with the only additional overhead arising from\\nlff . PASTA (PArameter-efficient tuning with Special Token\\nAdaptation) [31] improves parameter efficiency by modifying\\nthe special token representations (e.g., [SEP] and [CLS] in\\nBERT) with an extra special trainable vector before the self-\\nattention layer at each transformer layer. Assuming that the\\ninputs to the transformer layer are denoted as H = {hi}N\\ni=1,\\nPASTA modifies the inputs as follows:\\nHmod = {hi + mi}N\\ni=1, (12)\\nmi = e(vp), i is the p-th special token ; otherwise, mi = 0,\\nmi is the special token adaptation. PASTA enables a re-\\nmarkable reduction in trainable parameters by training only\\nthe trainable vector e(vp) to update the representations of\\nspecial tokens. The reasons for using [CLS] and [SEP] as\\nspecial tokens in PASTA are that the [CLS] representation\\nprovides a global representation of the input text and that\\nthe attention scores in PLMs are primarily allocated to the\\n[CLS] or [SEP] tokens across attention heads [83], [84].\\nAttentionFusion [32] introduces the late fusion technique,\\nwhich involves combining features or representations from\\ndiverse tasks or layers to generate a final joint representation,\\nto adjust the the importance of each token representation. For\\na given task t, let the attention query vector be denoted by Qt\\nand the representation of token i at layer j be V j\\ni , then the\\nrepresentation of token i for task t, ˆV j\\ni , is expressed as:\\nˆV j\\ni =\\nX\\nj\\nαj\\ni (t)V j\\ni , α j\\ni (t) = exp(QtV j\\ni )P\\nk exp(QtV j\\ni )\\n, (13)\\nwhere αj\\ni (t) represents the attention weight of token i at layer\\nj for task t. The number of extra parameters that need to be\\nupdated in AttentionFusion is determined by the size of the\\nquery vector Qt, which is the same as the hidden dimension of\\nthe pretrained encoder. By employing the attention weight as\\nextra trainable parameters, AttentionFusion adjusts the impor-\\ntance of each token representation dynamically. Hadamard\\nAdapter [33] is an additive fine-tuning method that intro-\\nduces a weight vector and a bias vector in each transformer\\nwith the same dimensions as the output of the multi-head\\nattention module for fine-tuning. A weight vector and a bias\\nvector are injected right after the multi-head attention layer\\nto perform element-wise multiplication (Hadamard product)\\nwith the multi-head attention outputs. Notably, the number\\nof Hadamard adapter is the same as that of the transformer\\nlayers in the PLM. During fine-tuning, only the parameters in'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='7\\nTABLE I: The weight update in pretrained weight masking and delta weight masking. ⊙ denotes the Hadamard product.\\nMethod Weight Update Mask Criterion Mask Matrix\\nThreshold-Mask ˆW = M ⊙ W Threshold M = Isi,j>τ\\nFISH Mask ˆW = M ⊙ W Fisher information M = Itop−k(fi,j)\\nLT-SFT ˆW = W + M ⊙ ∇W L(W) Absolute difference of parameters M = Itop−k(|W1−W0|)\\nChild-TuningF ˆW = W − M ⊙ η∇W L(W) Bernoulli distribution M = {0, 1}n\\nChild-TuningD ˆW = W − M ⊙ η∇W L(W) Fisher information M = Itop−k(fi,j)\\nDiff Pruning ˆW = W + M ⊙ ∆W Fixed sparsity M = {0, 1}n\\nSAM ˆW = W + M∆W Analytical solution Mi,j = 0, ∀i ̸= j; Mi,i ∈ {0, 1}\\nthe Hadamard adapter, layer normalization, and classifier are\\nupdated.\\nB. Partial Fine-tuning\\nPartial fine-tuning methods aim to reduce the number of\\nfine-tuned parameters by selecting a subset of pre-trained pa-\\nrameters that are critical to downstream tasks while discarding\\nunimportant ones. We categorize partial fine-tuning methods\\ninto three groups: Bias Update [34], [35], in which only\\nthe bias term in the attention layer, feed-forward layer and\\nlayer normalization of the transformer is updated, Pretrained\\nWeight Masking [36], [37], where the pretrained weights are\\nmasked using various pruning criterion, and Delta Weight\\nMasking [38], [39], [40], [41], in which delta weights are\\nmasked via pruning techniques and optimization approxima-\\ntion. A detailed analysis of pretrained weight and delta weight\\nmasking is provided in Table I.\\n1) Bias Update: Bit-Fit (Bias-term Fine-tuning) [34]\\nachieves parameter efficiency by only updating the bias terms\\nand the task-specific classification layer while keeping the\\nmajority of parameters in the transformer-based PLMs frozen.\\nThe bias parameters are involved in the attention layer, where\\nthey are involved in calculating query, key, value, and combin-\\ning multiple attention heads, as well as in the fee-forward and\\nlayer normalization layers. Further, U/S-BitFit [35] combines\\nNAS algorithm [85] and pruning technique to automatically\\ndetermine which parameters of the network need to be fine-\\ntuned based on BitFit. U-BitFit (Unstructured BitFit) decides\\nwhich PEFT parameters to prune based on the first-order\\napproximation of the change in training loss resulting from\\npruning the PEFT parameter W, i.e., −W · ∇W L(W). While\\nS-BitFit (Structured BitFit) sums the criterion over the overall\\nbias update ∆b (b is the bias term).\\n2) Pretrained Weight Masking: Pretrained weight masking\\nemploys pruning criteria like threshold and Fisher information\\nto measure the importance of pretrained weight to construct\\na binary mask matrix for weight masking. Threshold-Mask\\n[36] utilizes the threshold to construct a binary mask ma-\\ntrix to select pretrained weights W of the attention and\\nFFN layers through element-wise multiplication, expressed as\\nˆW = W ⊙ M (⊙ denotes the Hadamard product). To begin,\\na random uniformly distributed real-valued matrix S, which\\nshares the same dimensions as matrices W and M, is created.\\nSubsequently, if an element in S surpasses a predetermined\\nglobal threshold τ, the corresponding position in the binary\\nmask matrix is assigned a value of 1; otherwise, it is assigned\\n0. FISH Mask (Fisher-Induced Sparse uncHanging) [37] uses\\nthe Fisher information of pretrained weight to measure their\\nimportance and construct a sparse binary mask. FISH Mask\\nselects the top-k parameters with the largest Fisher information\\nto construct the sparse binary mask, where the positions\\ncorresponding to the top- k parameters are set to be 1 and\\nthe rest are set to 0. Note that k is preset based on the desired\\nmask sparsity level of the mask, and the resulting sparse binary\\nmask can be reused across many subsequent iterations.\\n3) Delta Weight Masking: Delta weight masking also em-\\nploys various pruning techniques and criteria to construct a\\nbinary mask matrix to reduce trainable parameters. However,\\nDelta weight pruning typically involves an update at each\\niteration. LT-SFT (Lottery Ticket Sparse Fine-Tuning) [38]\\nis a novel PEFT method inspired by the Lottery Ticket\\nHypothesis2 [86]. LT-SFT first fine-tunes the PLM on target\\ndata using pretrained parameters W0 to obtain the fully fine-\\ntuned parameters W1, and then identifies the top- k pretrained\\nparameters with the greatest absolute differences ( |W1 −W0|).\\nThe top-k parameters are selected for further fine-tuning using\\nbinary mask M, in which the positions corresponding to the\\nselected k parameters are set to 1 and the remaining positions\\nto 0. LT-SFT then resets the model parameters to their original\\npretrained weights W0 but fine-tunes only the selected k\\nparameters while keeping the remaining parameters frozen,\\nand can be expressed as δ = M ⊙∆W(∆W = ∇W L(W). By\\niteratively repeating this process, the method gradually fine-\\ntunes only a small fraction of the model’s parameters. Child-\\nTuning [39] calls the network formed by the parameters to be\\nupdated a child network and masks out the gradients of non-\\nchild networks to improve parameter efficiency. The parameter\\nupdates in Child-Tuning is expressed as δ = M ⊙ ∆W\\n(∆W = η∇W L(W), η denotes learning rate). Child-Tuning\\nprovides two variants: Child-Tuning F (F stands for Task-\\nFree) and Child-Tuning D (D stands for Task-Driven). Child-\\nTuningF generates the binary mask matrix M using Bernoulli\\ndistribution with a probability denoted as p F . Increasing the\\nvalue of pF updates a larger number of parameters, and Child-\\nTuningF is equivalent to full fine-tuning when p F = 1 . In\\ncontrast, Child-TuningD uses Fisher information estimation to\\nidentify a subset of parameters (i.e., child network) that are\\nhighly correlated with a specific downstream task. The binary\\n2Lottery Ticket Hypothesis states that each neural model contains a sub-\\nnetwork (a “winning ticket”) that can match or even outperform the perfor-\\nmance of the original model when trained in isolation.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='8\\nmask matrix in Child-Tuning D is constructed by setting the\\nposition of the child network to be 1 and the non-child network\\nto be 0.\\nDiff Pruning [40] introduces a sparse task-specific “diff”\\nvector δ during fine-tuning while remaining the pretrained\\nmodel parameters fixed. To make the diff vector δ sparse,\\nDiff Pruning introduces a learnable binary mask M on the\\nDelta weight and decomposes δ = M ⊙ ∆W. The binary\\nmask M is learnable and is used as a regularizer during fine-\\ntuning. It acts as a differentiable approximation to the L0-\\nnorm of diff vector δ. This approach is well-suited for multi-\\ntask deployment in edge (mobile) applications with limited\\nstorage. Significantly, Diff pruning incurs higher memory\\nconsumption compared to traditional fine-tuning, which may\\nbecome problematic as model sizes continue to grow. SAM\\n(Second-order Approximation Method) [41] also employs the\\nsparse mask matrix to update the delta weight. However, SAM\\ndirectly optimizes the approximation function to obtain an\\nanalytical solution for the mask matrix, which is then used to\\nupdate the pretrained weight. Concretely, SAM [41] views the\\nPEFT methods as p-sparse fine-tuned model by representing\\nfine-tuned parameter as W = W0 + M∆W, M is a mask\\nmatrix, and the optimization problem is\\nmin∆W,ML(W0 + M∆W), s.t.\\n∥M∥0 = ⌊mp⌋, Mi,j = 0, ∀i ̸= j; and Mi,i ∈ {0, 1}.\\nSAM approximates the loss function using its second-order\\nTaylor expansion as:\\nL(W0 + M∆W) ≈L(W0) + ∆L(W0)T ∆L(W0)T M∆W\\n+ 1\\n2(M∆W)T HM ∆W, (14)\\nin which H is the Hessian matrix. In practice, SAM first\\nobtains the gradient ∇L(W0)i for the i-th parameter Wi, then\\ncalculates\\n\\x0c\\x0c∇L(W0)2\\ni\\n\\x0c\\x0c, and selects the top ⌊mp⌋ delta weight\\nfor optimization.\\nC. Reparameterized Fine-tuning\\nReparameterized fine-tuning methods utilize low-rank trans-\\nformation to reduce the number of trainable parameters while\\nallowing operating with high-dimensional matrices (e.g., pre-\\ntrained weights). We categorize reparameterized fine-tuning\\nmethods into two groups: Low-rank Decomposition [11],\\n[42], [43], in which various low-rank decomposition tech-\\nniques are used to reparameterize the updated matrix, and\\nLoRA derivatives [44], [45], [46], [47], [48], [49], [50], [51],\\n[52], [53], [54], [55], [56], [57], where a series of PEFT\\nmethods are developed based on LoRA. Specific details of\\n∆W parameters reparameterization of various approaches can\\nbe seen in Table II.\\n1) Low-rank Decomposition: This involves finding a lower-\\nrank matrix that captures the essential information of the\\noriginal matrix while reducing computational complexity and\\nmemory usage by reparameterizing the updated delta weight.\\nReparameterization covers transforming the delta weight ma-\\ntrix into a low-rank representation using methods such as\\nFastfood transformation, low-rank down-up projection, or Kro-\\nnecker product projection.\\nIntrinsic SAID (Structure-Aware Intrinsic Dimension) [42]\\nleverages the concept of intrinsic dimensionality to reduce\\nthe number of parameters during fine-tuning. The intrinsic\\ndimensionality refers to the minimum dimensionality required\\nto solve a high-dimensional optimization problem. In the\\ncontext of PLMs, measuring the intrinsic dimensionality helps\\nestimate the minimum number of parameters needed to adapt\\nto new tasks. Instead of optimizing the empirical loss in\\nthe original parameterization, Intrinsic SAID fine-tunes the\\nmodel by reparametrization the model in a lower-dimensional\\nspace, i.e., ∆W = F(Wr), in which Wr is the parameter\\nto be optimized and F : Rr → Rd is a Fastfood transform 3\\n[87] that projects parameters from low-dimensional r to high-\\ndimensional d. However, Intrinsic SAID is not practical for\\nfine-tuning larger networks due to the O(d) memory com-\\nplexity of the Fastfood transform and the need to update all\\nof the model’s parameters.\\nInspired by Intrinsic SAID, LoRA (Low-Rank Adaptation)\\n[11] introduces two trainable low-rank matrices for weight\\nupdate. In LoRA, a down-projection matrix and an up-\\nprojection matrix are utilized in parallel with the query (Q),\\nkey (K), and value (V) matrices in the attention layer of the\\ntransformer, shown in Fig. 3. For a pretrained weight matrix\\nW ∈ Rd×k, LoRA updates W using low-rank decomposition\\n∆W = WdownWup. During training, the weights of PLM\\nare frozen, and only the low-rank matrices of LoRA, i.e.,\\nWdown ∈ Rd×r and Wup ∈ Rr×k are fine-tuned (r ≪ {d, k}).\\nDuring inference, the LoRA weights are merged with the\\noriginal weight matrix of the PLMs without increasing the\\ninference time. Practically, a scaling factor ( s = 1/r) is added\\nto the LoRA module. KronA (Kronecker Adapter) [43] is\\nstructurally similar to LoRA but replaces the low-rank de-\\ncomposition in LoRA with Kronecker product decomposition,\\n∆W = Wdown ⊗ Wup. Kronecker product decomposition\\nmaintains the rank of the input matrix (i.e., rank (A ⊗ B) =\\nrank(A) × rank(B)), ensuring that important information is\\npreserved during the adaptation process. Moreover, Kronecker\\nproduct can speed up computation and reduce the number\\nof required floating-point operations (FLOPS) by avoiding\\nthe explicit reconstruction of the Kronecker product matrix.\\nKronA has two variants: KronA B and KronA B\\nres. KronA B\\ninserts the KronA module in parallel to the FFN layer, while\\nKronAB\\nres inserts the KronA module alongside the FFN layer\\nand incorporates a learnable residual connection.\\n2) LoRA Derivatives: LoRA derivatives refer to a series\\nof PEFT methods that are improved based on LoRA, includ-\\ning Low-Rank Adjustment [44], [45], [46], where different\\nmethods are developed to adjust the rank of LoRA dynami-\\ncally, LoRA-guided Pretrained Weight Update [47], [48],\\nin which LoRA is used to guide the update of pretrained\\nweight, Quantization Adaption [49], [50], [51], in which\\nvarious quantization techniques are proposed to improve the\\nhigh precision fine-tuning and inference of LoRA, LoRA-\\n3Fastfood transform is a computationally efficient dimensionality expansion\\nmethod.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='9\\nTABLE II: Delta weight reparameterization of various reparameterized fine-tuning methods.\\nMethod ∆W Reparameterization Notes\\nIntrinsic SAID ∆W = F(Wr) F : Rr → Rd, Wr ∈ Rr is parameters to be optimized, and r ≪ d.\\nLoRA ∆W = WdowmWup Wdown ∈ Rk×r, Wup ∈ Rr×d, and r ≪ {k, d}.\\nKronA ∆W = Wdown ⊗ Wup rank(Wdown ⊗ Wup) = rank(Wdown) × rank(Wup).\\nDyLoRA ∆W = Wdown↓bWup↓b Wdown↓b = Wdown[: b, :], Wup↓b = Wup[:, : b], b ∈ {rmin, ··· , rmax}.\\nAdaLoRA ∆W = PΛQ PP T = PT P = I = QQT = QT Q, Λ = diag(σ1, σ2, . . . , σr).\\nIncreLoRA ∆W = WdownΛWup Λ = [λ1, λ2, ··· , λr] with λi could be an arbitrary constant.\\nDeltaLoRA ∆W = WdownWup W(t+1) ← W(t) + (W(t+1)\\ndown W(t+1)\\nup − W(t)\\ndownW(t)\\nup ).\\nLoRAPrune ∆W = WdownWup ⊙ M δ = (W + WdownWup) ⊙ M, M ∈ {0, 1}1×G, G is group number.\\nQLoRA ∆W = WBF 16\\ndown WBF 16\\nup Y BF 16 = XBF 16doubleDequant(cFP 32\\n1 , cFP 8\\n2 , WNF 4) + XBF 16WBF 16\\ndown WBF 16\\ndown .\\nQA-LoRA ∆W = WdownWup Wdown ∈ Rk×r, Wup ∈ Rr×L, L is the quantization group number of W.\\nLOFTQ ∆W = SVD(W − Qt) Qt = qN (W − Wt−1\\ndownWt−1\\nup ), qN is N-bit quantization function.\\nKernel-mix ∆Wh = (BLoRA, Bh)\\n \\nAh\\nLoRA\\nAh\\n!\\nBLoRA is shared across all heads, Bh, Ah provide rank-r update in each head.\\nLoRA-FA ∆W = WdownWup = QRWup Wdown is frozen, and only update Wup.\\nbased Improvements [52], [53], [54], in which several novel\\ntechnique are incorporated into LoRA for improvements, and\\nLoRA-based Multi-task Fine-tuning [55], [56], [57], where\\nmultiple LoRA modules are combined for cross-task transfer\\nto fine-tune model on a novel task.\\nLow-rank Adjustment. DyLoRA (Dynamic LoRA) [44] is\\nintroduced to overcome two limitations of LoRA: (a) LoRA’s\\nrank is fixed and prevents any changes after training (b)\\ndetermining the optimal rank for LoRA requires exhaustive\\nsearch and considerable effort. DyLoRA trains LoRA modules\\nfor a range of ranks instead of a single rank, allowing\\nfor adaptability. DyLoRA addresses these limitations during\\ntraining by sorting the representations learned at various ranks.\\nSpecifically, DyLoRA operates within the range of ranks\\ndenoted as r ∈ [rmin, rmax] for a series of iterations. In\\neach iteration, DyLoRA randomly selects a specific rank b\\nfrom {rmin, ··· , rmax}. It then truncates the down-projection\\nmatrix as Wdown↓b = Wdown[: b, :] and the up-projection\\nmatrix as Wup↓b = Wup[:, : b] and only update truncated\\nparameter matrices Wdown↓b and Wup↓b. The parameter up-\\ndates in each iteration of DyLoRA could be expressed as\\n∆W = Wdown↓bWup↓b. By allowing dynamic low-rank adap-\\ntation and search-free low-rank adaptation, DyLoRA reduces\\nthe computational cost and training time required to identify\\nthe optimal rank for a particular task. AdaLoRA (Adaptive\\nLow-Rank Adaptation) [45] extends LoRA by dynamically\\nadjusting the rank of matrices to control the allocation budget.\\nIn AdaLoRA, the incremental update ∆W is reparameterized\\nusing singular value decomposition (SVD) and then truncates\\nthe smallest singular values, i.e., ∆W = PΛQ. Both P\\nand Q are orthogonal matrices, and Λ is a diagonal matrix\\ncontaining the singular values {σ1, σ2, . . . , σr}. Here, r rep-\\nresents the rank of the matrix Λ. During training, P and Q\\nare initialized with Gaussian distribution with a regularizer\\nto ensure the orthogonality, while Λ is initialized with zero\\nand iteratively pruned to adjust the rank. AdaLoRA employs\\nthe sensitivity-base importance scoring [88], [89] with a new\\nmetric to prune the singular values of unimportant updates to\\nupdate the Λ. By doing this, AdaLoRA effectively improves\\nparameter efficiency and allocation budgets. IncreLoRA [46]\\ndynamically incorporates trainable parameters into LoRA by\\nincreasing their ranks, guided by importance scores assigned\\nto each module during training. The allocation process assigns\\nlower ranks, possibly 0 to indicate no parameter updates,\\nto less important modules, while allocating higher ranks to\\nmore important modules. The parameter updates in IncreLoRA\\ncan be expressed as ∆W = WdownΛWup, in which Λ =\\n[λ1, λ2, ··· , λr] is a diagonal matrix with λi could be any\\narbitrary constant, r is the rank of the each LoRA module.\\nBesides, an upper bound on the rank is set for each module\\nto control the parameter growth. Additionally, IncreLoRA\\nintroduces a unique pretraining technique called “advance\\nlearning”, which ensures that the newly added parameters\\nin each module begin with favorable initial states. In this\\nway, it prevents insufficient training of subsequently added\\nparameters, allowing for effective utilization of the incremental\\nparameter allocation. Unlike LoRA, which operates on the\\nquery (Q), key (K), and value (V) projection modules of the\\nattention layer, the parameter updates are applied to all linear\\nlayers in IncreLoRA.\\nLoRA-guided Pretrained Weight Update. Delta-LoRA\\n[47] updates the pretrained weight W as well as two low-\\nrank matrices Wdown and Wup, while using the same memory\\nas the original LoRA. The two low-rank matrices Wdown\\nand Wup are automatically updated as usual. The pretrained\\nweight, however, leverages the mathematical property that\\n∇W L(W, Wdown, Wup) = ∇WdownWupL(W, Wdown, Wup)\\n(it is achieved by removing the dropout layer in the original\\nLoRA module) for parameters update. Specifically, W is\\nupdated with the delta of the product of two low-rank matrices\\nin consecutive iterations, i.e., W ← W + ∆WdownWup =\\nW + (Wdown(t + 1)Wup(t + 1) − Wdown(t)Wup(t)). Lo-\\nRAPrune [48] introduces a LoRA-guided pruning criterion,\\nwhich utilizes the weights and gradients of LoRA instead of\\nthe gradients of pretrained weights for importance estimation\\nto prune parameters of LoRA and pretrained weights. To\\naddress the substantial memory overhead associated with un-\\nstructured pruning and dependency-aware structured pruning,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='10\\nLoRAPrune devises a structured iterative pruning procedure\\nthat selectively eliminates redundant channels and heads.\\nLoRA-guided pruning criterion involves using low-rank matri-\\nces Wdown and Wup, along with their corresponding gradients\\n∇Wdown and ∇Wup, to calculate the importance score 4. This\\nscore determines which weights are deemed unimportant and\\nsubsequently removed. Notably, LoRAPrune not only prunes\\nstructured weights, such as heads and channels, from the pre-\\ntrained weights, but also prunes the corresponding weights in\\nthe LoRA, i.e., δ = (W +WdownWup)⊙M, M ∈ {0, 1}1×G,\\nG is the group number. Binary mask M is set to 0 when\\nthe corresponding group is unimportant, and 1 when it is\\nimportant. Therefore, after pruning and fine-tuning, the LoRA\\nweights can seamlessly merge with the pretrained weights,\\nensuring that no additional computations are necessary during\\ninference.\\nQuantization Adaption. QLoRA [49], a quantized variant\\nof LoRA, effectively addresses the limited computational\\nresource of LoRA for fine-tuning LLMs by quantizing the\\ntransformer model to 4-bit NormalFloat (NF4) precision with\\ndouble quantization processing, and using a paged optimizer\\nto deal with memory spikes. NF4 is a new data type that\\nis theoretically optimal for normally distributed weights. Al-\\nthough QLoRA quantizes pretrained weight W from FP16 into\\nNF4 so that LLMs can be fine-tuned with fewer GPUs, the\\nauxiliary weight of LoRA matrix WdownWup makes the final\\nweight return to FP16 again after fine-tuning. To this end, QA-\\nLoRA (Quantization-Aware Low-rank Adaption) [50] employs\\ngroup-wise quantization with low-rank adaptation to the pre-\\ntrained weight W, in which each column of W is partitioned\\ninto L groups for quantization. In this way, QA-LoRA ensures\\nthat pretrained weights W and auxiliary weights are integrated\\ninto a quantized form after fine-tuning, resulting in a faster and\\nmore accurate computation during inference. While LOFTQ\\n(LoRA-Fine-Tuning-aware Quantization) [51] applies an N-bit\\nquantized weight Q and low-rank approximation Wdown ∈\\nRd1×r, Wup ∈ Rd2×r to approximate the original high-\\nprecision pretrained weight W ∈ Rd1×d2 as the initialization\\nof LoRA fine-tuning. Such an initialization alleviates the\\nquantization discrepancy in QLoRA and significantly improves\\nthe generalization in downstream tasks.\\nLoRA-based Improvements. Kernel-wise Adapter [52]\\ntreats the different attention heads in the transformer as\\nindependent kernel estimators and utilizes the kernel structure\\nin self-attention to guide the assignment of tunable parameters.\\nLoRA is used as the underlying model to combine kernel-wise\\nadaptation for its flexibility in parameter assignment for dif-\\nferent weight matrices. Kernel-wise adapter has two variants:\\nKernel-mix-lite (qv) and Kernel-mix-lite (qvo). Kernel-mix-\\nlite (qv) provides a lightweight solution for scenarios with\\nlimited parameter budgets, while Kernel-mix (qvo) is suitable\\nfor scenarios with intermediate parameter budgets. The suffix\\n(qv) means that the method will adjust Wq and Wv, while the\\nsuffix (qvo) means that the method will modify Wq, Wv, and\\nWo. Laplace-LoRA [53] incorporates Bayesian inference into\\n4Importance score I is calculate via I = ∇W ⊙ W,∇W ≈ Wdown ·\\n∇Wup + ∇Wdown · Wup − ∇Wdown · ∇Wup.\\nthe LoRA parameters to address the issue of overconfidence\\nand improve calibration. A key challenge lies in obtaining\\nthe posterior distribution for Bayesian inference, which is\\nresolved by using Laplace approximation [90]. Laplace-LoRA\\ncan be viewed as an approximation of the posterior distribution\\nover LoRA parameters using Laplace approximation. Hence,\\nLaplace-LoRA maintains existing pretraining and fine-tuning\\nprocedures while reducing the dimensionality of Bayesian\\ninference. LoRA-FA (LoRA with Frozen-A) [54] is pro-\\nposed to reduce the expensive activation memory of LoRA\\nwithout introducing any computational overhead. LoRA-FA\\nkeeps the pretrained weight W and down-projection matrix\\nWdown frozen and only updates the up-projection matrix Wup.\\nWdown is decomposed into Q and R via QR decomposi-\\ntion, and ∆W = WdownWup = QRWup = Q ˆWup =Pr\\ni=1 Q:,i ˆWup,i,:, in which {Q:,i}r\\ni=1 are orthogonal unit vec-\\ntors (r is the rank of Wdown). Thus, ∆W is a combination of\\nr orthogonal vectors, limiting the change of weight residing in\\na low-rank space. Consequently, there is no need to store full-\\nrank input activations simultaneously, alleviating the memory\\nburden associated with activation storage.\\nLoRA-based Multi-task Fine-tuning. LoRAHub [55]\\nleverages a composition of multiple trained LoRA modules\\nfor cross-task transfer to fine-tune the model on new tasks.\\nSpecifically, LoRAHub trains task-specific LoRA modules in\\na variety of tasks to obtain a synthesized module, ˆm =\\n(w1W1\\ndown +··· +wN WN\\ndown)(w1W1\\nup+··· +wN WN\\nup), which\\nis then amalgamated with the LLMs to adapt the new task.\\nThus, the objective of LoRAHub is to find the best weight\\nset {w1, w2, ··· , wN }, which is achieved by the gradient-free\\ncombinatorial optimization approach Shiwa [91]. MOELoRA\\n[56] combines LoRA with mixture-of-experts (MoE) for multi-\\ntask fine-tuning, in which each expert is a LoRA module\\nfor learning task-specific knowledge. Additionally, MOELoRA\\ndevises a task-motivated gate function to produce distinct\\nfine-tuned parameters for various tasks. L-LoRA (Linearized\\nLoRA) [57] is a linearized PEFT method to improve the\\nmulti-task fusion capability of fine-tuned task-specific models\\nwith low computation costs. L-LoRA constructs a linear\\nfunction using a fir-order Taylor expansion, as illustrated in\\nEquation 15. In L-LoRA, only the linearized LoRA modules\\nare fine-tuned in the tangent space, incurring fewer trainable\\nparameters compared to LoRA. For the multi-task fusion meth-\\nods, simple average, task arithmetic [92], [93], ties-merging\\n[94], and LoRAhub [55] are employed for multi-task fusion.\\nfθ0 (x; ϕ(t)) ≈flin\\nθ0 (x; ϕ(t)) = fθ0 (x; ϕ(0))\\n+ ∇ϕfθ0 (x; ϕ(0))T (ϕ(t) − ϕ(0)). (15)\\nD. Hybrid Fine-Tuning\\nHybrid fine-tuning approaches aim to combine various\\nPEFT approaches, such as adapter, prefix-tuning, and LoRA,\\nto leverage the strengths of each method and mitigate their\\nweaknesses. By integrating different features of PEFT meth-\\nods, hybrid fine-tuning achieves improved overall perfor-\\nmance compared to individual PEFT methods. These works\\nare classified into two approaches: Mannual Combination'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='11\\n[16], [35], [58], [59], in which multiple PEFT methods are\\ncombined manually by sophisticated design, and Automatic\\nCombination [60], [61], [62], where various PEFT methods\\nare incorporated automatically via structure search.\\n1) Manual Combination: Manual combination mainly in-\\nvolves integrating the structure or features of one PEFT\\nmethod into another PEFT method to enhance performance\\nwhile achieving parameter efficiency. MAM Adapter (Mix-\\nAnd-Match Adapter) [16] is the combination of scaled parallel\\nadapter and prefix-tuning, which employs prefix-tuning with\\nsmaller bottleneck dimensions at the attention layer and al-\\nlocates more parameter budget to modify the representation\\nof FFN using the scaling parallel adapter. Scaled parallel\\nadapter denotes the parallel adapter with a scaling factor\\nto adjust the adapter output. Concretely, the output of the\\nMAM adapter h can be expressed with h = LN(X +\\nscale ∗ FFN(LN(Attn([Pk, X]) + [Pv, X]))) for the input X.\\nFurther, U-MAM (Unstructured MAM) and S-MAM (struc-\\ntured MAM) [35] are proposed by combining NAS algorithm\\n[85] and pruning technique to automatically determine which\\nparameters of the network need to be fine-tuned based on\\nMAM adapter. NAS algorithm takes the maximum number\\nof parameters required for PEFT architectures as input and\\napplies the pruning operation to reduce trainable parameters.\\nThe criteria for deciding which PEFT parameters to prune\\nare based on the first-order approximation of the change in\\ntraining loss resulting from pruning the PEFT parameter W,\\ni.e., −W ·∇W L(W). U-MAM directly employs this criterion\\nto prune the parameters in MAM, while S-MAM sums the\\ncriterion over each column of Wdown.\\nCompacter [58] is developed based on adapters, low-\\nrank optimization, and a parameterized hypercomplex mul-\\ntiplication (PHM) layer [95]. It follows a similar structure\\nto adapters, consisting of a down-projection, a nonlinear\\nactivation function, and an up-projection. However, Compacter\\nreplaces the down-projection and up-projection in the adapters\\nwith the low-rank parameterized hypercomplex multiplication\\n(LPHM) layer, which is an extension of PHM that incorporates\\nlow-rank optimization. Structurally, PHM layer resembles a\\nfully connected layer, but with the learned W represented as a\\nsum of Kronecker products, i.e., W = Pn\\ni=1 Ai⊗Bi. Notably,\\nwhen the weights of down-projection and up-projection are\\ncalculated as in that of the PHM layer,Ai is a shared parameter\\nacross all adapter layers, while Bi represents adapter-specific\\nparameters. This kind of adapter is called PHM Adapter .\\nSimilarly, Compacter obtains the weight matrix in each LPHM\\nlayer utilizing the sum of Kronecker products, but Compacter\\nreparameterizes Bi as the product of two independent ranks\\nwith one weight, and the weight matrix in Compacter is\\ncalculated as follows:\\nW =\\nnX\\ni=1\\nAi ⊗ Bi =\\nnX\\ni=1\\nAi ⊗ (sitT\\ni ). (16)\\nW ∈ Rk×d, Ai ∈ Rn×n, Bi ∈ R\\nk\\nn × d\\nn ; si ∈ R\\nk\\nn ×r, ti ∈ Rr× d\\nn .\\nCompacter++ is a variant of Compacter that inserts a Com-\\npacter layer after the FFN layer of each transformer module\\nand requires fewer parameters to be updated than Compacter.\\nUniPELT [59] incorporates sequential adapter, prefix-\\ntuning, and LoRA via a gating mechanism. In UniPELT,\\nadapters are added after the feed-forward layer, prefix-tuning\\nis employed to the key (K) and value (V ) vectors of the multi-\\nhead attention layer, and LoRA is used in attention matrices of\\nWq and Wv of the transformer. Each PEFT module is equipped\\nwith a gating mechanism composed of a linear function with\\nthe dimension of the output being 1, a sigmoid function,\\nand a mean function. The gating mechanism controls the\\nactivation of each submodule, dynamically assigning higher\\nweights to submodules that make positive contributions to\\na given task. The trainable parameters encompass low-rank\\nLoRA matrices Wdown and Wup, prefix-tuning parameters\\nPk and Pv, adapter parameters, and weights for the gating\\nfunction. Consequently, UniPELT requires more parameters\\nand inference time than adapter, prefix-tuning, and LoRA, but\\nachieves better performance compared with the performance\\nof the best individual PEFT method.\\n2) Automatic Combination: Automatic combination ex-\\nplores how to configure PEFT methods like adapters, prefix-\\ntuning, BitFit, and LoRA to different layers of the transformers\\nautomatically using various structure search and optimization\\napproaches. However, it typically requires more time and cost\\ndue to the need to perform optimization searches in the model\\nor structure. AutoPEFT [60] integrates sequential adapter,\\nparallel adapter, and prefix-tuning into the transformer block.\\nThe serial adapter receives the hidden state from the FFN\\noutput as input, while the parallel adapter takes the hidden\\nstate before the FFN layer as its input. In addition, the prefix-\\ntuning module concatenates two prefix vectors, Pk and Pv,\\nwith the original key and value vectors, respectively, enabling\\nmulti-head attention to adapt to specific target tasks. Motivated\\nby the success of NAS algorithm, AutoPEFT proposes to use\\nthe Bayesian optimization approach to automatically search\\nfor an appropriate neural architecture network that selectively\\nactivates certain layers to incorporate these PEFT modules.\\nBayesian optimization is not only sample-efficient and zeroth-\\norder but also well-suited for multi-objective setups, enabling\\ncost-efficient optimization and facilitating the trade-off be-\\ntween performance and cost. Moreover, it is more paralleliz-\\nable during search, which can decrease memory usage.\\nS3Delta-M (Search for Sparse Structure of Delta Tun-\\ning Mix) [61] is a mixture of LoRA, Compacter (low-\\nrank adapter), BitFit, and LNFit 5. Different from the simple\\nincorporation of PEFT techniques, S 3Delta-M is developed\\nby conducting a differentiable delta tuning structure search.\\nIt explicitly controls sparsity and searches for an optimal\\ncombination of these techniques in a unified search space.\\nIn S3Delta-M, each PEFT module (LoRA, Compacter, BitFit,\\nand LNFit) is inserted into the corresponding layers of the\\nPLM to ensure the best performance is achieved. The specific\\ncombination and placement of these modules are determined\\nthrough the structure search process, which is guided by\\nexplicit sparsity control. S4 [62] is a combination of Sequential\\nAdapter, Prefix-tuning, BitFit, and LoRA. Unlike previous\\n5LNFit is trained only on the variance vectors in the layer normalization\\nmodule of the PLMs, inspired by [86] which trains only on the batch\\nnormalization module in convolutional neural networks.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='12\\nmethods that utilize the same PEFT module uniformly across\\nall layers of the transformer, S4 is designed by searching\\nfor various layer groupings, trainable parameter allocations,\\ntunable groups, and PEFT module assignments. In S4, the\\nlayers of the PLMs are divided into four groups, G1, G2,\\nG3, G4, in a “spindle” pattern. This means that more layers\\nare allocated to the middle groups ( G2 and G3) while fewer\\nlayers are assigned to the top and bottom groups ( G1 and G4).\\nHowever, all trainable parameters are allocated uniformly, i.e.,\\nthe number of trainable parameters in each layer remains the\\nsame across all groups. Different groups are equipped with dif-\\nferent combinations of sequential adapter, prefix-tuning, BitFit,\\nand LoRA. Extensive experimental results demonstrate that\\nbetter performance is achieved when each group is equipped\\nwith the following combinations of PEFT methods. A denotes\\nsequential adapter, P denotes prefix-tuning, B denotes BitFit,\\nand L denotes LoRA.\\nG1 : (A, L); G2 : (A, P);\\nG3 : (A, P, B); G4 : (P, B, L).\\nE. Unified Fine-tuning\\nUnified fine-tuning presents a unified framework for fine-\\ntuning, which streamlines the incorporation of diverse fine-\\ntuning methods into a cohesive architecture, ensuring consis-\\ntency and efficiency across the adaptation and optimization of\\nmodels. Unlike hybrid fine-tuning methods, unified fine-tuning\\nmethods typically utilize a single PEFT method rather than a\\ncombination of various PEFT methods.\\nAdaMix [63] leverages a mixture of adaptation module\\napproaches to obtain a unified framework for fine-tuning.\\nMotivated by sparsely-activated MoE [96], AdaMix treats\\neach adaptation module as an individual expert and employs\\nstochastic routing to randomly select a down-projection ma-\\ntrix and an up-projection matrix for weight updates. Such\\nstochastic routing allows the adaption module to learn multiple\\nviews for the given task, but it also poses a challenge in\\ndeciding which adaption module to use during inference.\\nTo this end, Adamix utilizes consistency regularization and\\nadaption module merging (i.e., average weights of all down-\\nand up-projection matrices) to select the trained adaption\\nmodule and obtain the same computational cost as that of a\\nsingle module. Notably, adaption modules in Adamix could be\\nadapters like sequential adapter [9] or low-rank decomposition\\nmatrices like LoRA [11].\\nSparseAdapter [64] utilizes network pruning technique to\\nconstruct a unified framework in which various PEFT methods,\\nincluding adapters family and LoRA [9], [11], [16], can be\\nfurther pruned to improve parameter efficiency. SparseAdapter\\nsets a target sparsity, denoted as s, and assigns a score,\\ndenoted as z, to all parameters of adapters and LoRA. Pa-\\nrameters with scores below the threshold zs (corresponding\\nto the s-th lowest percentile of z) are considered redundant\\nand removed. The score z can be computed using pruning\\nmethods, such as random pruning, magnitude pruning [97],\\nErdos-Renyi [98], SNIP [99], or GraSP [100], based on the\\nadapter weight W, with SNIP-based SparseAdapter yielding\\nthe best results. Furthermore, SparseAdapter exhibits improved\\nperformance compared to full fine-tuning when utilizing the\\n“Large-Sparse” setting, which involves larger bottleneck di-\\nmensions and higher sparsity ratios. Notably, the network\\npruning technique proposed in SparseAdapter is a plug-in\\nmethod that can be applied to any adapter variants, such as\\nLoRA [11], MAM Adapter [16], and AdapterFusion [19]. The\\noptimized parameters in SparseAdapter can be represented as\\nˆW = W ⊙ M, in which M is a binary mask matrix with\\nM = I{z≥zs} and z = score(W).\\nProPETL [65] introduces a single prototype network (e.g.,\\nadapter, prefix-tuning, and LoRA) across layers and tasks and\\nconstructs different sub-networks for each layer using various\\nbinary masks. Inspired by ALBERT [101], ProPETL leverages\\nparameter sharing within the prototype network modules in\\neach layer of the transformer, enhancing parameter efficiency\\nand reducing storage requirements. In ProPETL, binary masks\\nM ∈ {0, 1}n are introduced in each layer of the transformer, in\\nwhich n is the number of parameters in a single PEFT module.\\nEach mask corresponds to a specific sub-network of the shared\\nprototype network. By doing so, though each layer shares\\nthe parameters of the same prototype network, each layer\\nhas a different sub-network to capture meaningful semantic\\nrepresentations. The final objective of the task adaptation for\\nthe PLMs can be expressed as follows:\\nmax\\nθpro,m1,m2,···,mL\\nNX\\ni=0\\nlogP(Yi|Xi; θlm, θsub), (17)\\nθsub = [θpro ⊙ m1, θpro ⊙ m2, ··· , θpro ⊙ mL].\\nHere, θlm represents the frozen pretrained parameters of the\\nPLMs, mi (i = 1, 2, ··· , L) is binary mask matrix, and θsub\\ndenotes the parameters to be optimized.\\nIV. E XPERIMENTS\\nA. Experimental Settings\\n1) PLMs and Datasets: We use the encoder-only models\\nRoBERTa-base (125M) and RoBERTa-large (355M) [2] to\\nevaluate on the GLUE benchmark [100], encoder-decoder\\nmodels T5-base (220M) and T5-large (770M) [4] to evaluate\\non the WMT16 En-Ro dataset 6, and decoder-only models\\nLLaMA-7B and LLaMA-13B [7] fine-tuned with the Alpaca\\ndataset [102] to evaluate on the MMLU benchmark [103].\\nAll these PLMs with different model types and model scales\\nare based on the encoder, decoder, or encoder-decoder of the\\nTransformer architecture. The datasets we use for experi-\\nments cover a wide range of tasks, from NLU to MT and\\nNLG. The GLUE benchmark covers a collection of NLU\\ntasks, including single-sentence classification, and sentence-\\npair classification tasks. WMT16 En-Ro dataset consists of\\nparallel data pairs, where each pair consists of an English\\nsentence and its corresponding translation into Romanian.\\nAlpaca [102] is an instruction dataset containing 52k samples.\\nMMLU Benchmark [103] encompasses a comprehensive range\\nof 57 disciplines spanning science, humanities, social sciences,\\nand more. The level of difficulty of the benchmark ranges from\\n6https://huggingface.co/datasets/wmt16'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='13\\nTABLE III: Fine-tuning RoBERTa-base (RoB B) and RoBERTa-large (RoB L) models on the GLUE benchmark. Specifically,\\nwe report the Matthews correlation for COLA, accuracy/F1 score for MRPC and QQP, Pearson/Spearman correlation for\\nSTS-B, averaged matched accuracy for MNLI, and accuracy for other NLU tasks. Higher values indicate better performance\\nacross all metrics. We present the number of trainable parameters (# TPs) of each method, excluding Child-Tuning D due to\\nits randomness during network pruning. We also bold the maximum values and underline the minimum values.\\nModel PEFT Method #TPs CoLA SST2 MRPC STS-B QQP MNLI QNLI RTE Avg.\\nRoBB\\nFT 124.6M 59.07 92.89 88.24/91.58 90.87/90.61 90.81/87.72 86.27 91.07 72.20 84.00/84.00\\nAdapterS 7.41M 63.32 94.31 90.44/93.18 91.25/90.94 90.81/86.55 87.33 92.06 73.56 85.39/85.16\\nPrompt-tuning 0.61M 49.37 92.09 70.83/81.72 82.44/83.11 82.99/78.35 80.57 80.03 58.12 74.56/75.42\\nPrefix-tuning 0.96M 59.31 93.81 87.25/91.03 88.48/88.32 87.75/84.09 85.21 90.77 54.51 80.89/80.88\\n(IA)3 0.66M 59.58 93.92 87.00/90.52 90.30/90.32 87.99/84.10 83.95 90.88 71.12 83.09/83.05\\nBitFit 0.69M 61.32 94.72 89.22/92.41 90.34/90.27 88.12/84.11 84.64 91.09 77.98 84.68/84.57\\nChild-TuningD - 60.33 93.58 89.22/92.20 91.14/90.93 90.98/88.04 87.40 92.20 77.62 85.31/85.29\\nLoRA 0.89M 62.09 94.04 87.50/90.68 90.66/90.83 88.83/85.21 86.54 92.02 72.92 84.33/84.29\\nAdaLoRA 1.03M 59.82 93.92 87.99/91.33 90.83/90.73 88.58/84.98 86.26 91.43 70.04 83.61/83.56\\nMAM Adapter 46.78M 61.42 94.87 89.31/92.21 90.74/90.42 88.31/83.20 86.63 90.19 72.62 84.26/83.95\\nProPELTAdapter 1.87M 66.33 93.85 87.25/90.82 91.33/91.04 89.22/85.79 86.49 92.56 75.54 85.32/85.30\\nProPELTPrefix 10.49M 61.79 94.30 88.73/91.98 90.30/90.19 88.54/85.05 86.22 91.51 63.31 83.08/83.04\\nProPELTLoRA 1.77M 60.38 94.11 87.42/90.87 90.76/90.55 88.90/85.55 86.84 92.04 67.39 83.48/83.47\\nRoBL\\nFT 355.3M 65.78 95.54 89.22/92.28 91.74/91.76 89.30/86.68 89.42 93.61 81.23 86.98/87.04\\nAdapterS 19.77M 67.03 96.37 89.94/92.54 92.58/92.42 92.19/88.50 91.00 94.31 85.25 88.58/88.43\\nPrompt-tuning 1.07M 61.13 94.61 73.04/81.29 78.51/78.99 80.74/75.16 68.15 89.13 60.29 75.70/76.09\\nPrefix-tuning 2.03M 59.01 95.76 88.24/91.37 90.92/91.07 88.88/85.45 89.30 93.32 74.01 84.93/84.91\\n(IA)3 1.22M 61.15 94.61 86.52/90.33 92.22/92.03 89.45/86.25 88.63 94.25 81.23 86.00/86.06\\nBitFit 1.32M 68.01 96.10 90.93/93.38 91.93/91.77 89.48/86.43 89.98 94.47 87.73 88.57/88.47\\nChild-TuningD - 63.08 95.07 90.69/93.43 92.36/92.18 91.52/88.75 35.45 93.15 86.25 80.95/80.92\\nLoRA 1.84M 64.47 96.67 87.50/91.19 91.66/91.44 90.15/86.91 90.76 95.00 79.78 87.00/87.03\\nAdaLoRA 2.23M 65.85 94.95 89.46/92.34 92.05/91.80 89.60/86.30 90.36 94.62 77.98 86.86/86.78\\nMAM Adapter 122.2M 67.39 95.81 90.12/92.77 92.44/92.18 90.87/86.65 90.62 94.31 86.62 88.52/88.29\\nProPELTAdapter 5.40M 65.55 96.27 89.71/92.54 91.92/91.67 90.67/87.74 91.37 95.20 88.89 88.70/88.65\\nProPELTPrefix 26.85M 62.24 96.17 90.04/92.92 90.70/90.49 89.30/86.30 90.33 94.73 79.71 86.65/86.61\\nProPELTLoRA 4.19M 61.90 95.93 89.06/92.19 91.66/91.38 90.93/88.05 90.53 94.93 83.57 87.31/87.31\\nbeginner to advanced levels of expertise, testing both world\\nknowledge and problem-solving abilities.\\n2) PEFT Methods: Eleven representative PEFT methods:\\nsequential adapter (Adapter S) [9], prompt-tuning [24], prefix-\\ntuning [10], (IA) 3 [30], BitFit [34], Child-Tuning [39], LoRA\\n[11], AdaLoRA [45], QLoRA [49], MAM adapter [16], and\\nProPELT [65] are chosen. Since the GLUE benchmark consists\\nof a series of NLU tasks, it serves as the preferred evaluation\\ndataset used by most PLMs to validate the effectiveness of\\nPEFT methods. Ten representative PEFT methods other than\\nQLoRA are selected to fine-tune RoBERTa-base/large. For T5-\\nbase/large, we use (IA) 3 and LoRA for fine-tuning. As for\\nLLaMA-7B/13B, (IA)3, LoRA, and QLoRA are used for fine-\\ntuning.\\n3) Implementation Details: Since “prompt-tuning, prefix-\\ntuning, (IA) 3, LoRA, and AdaLoRA” have been integrated\\ninto the PEFT library7. Therefore, we directly utilize the PEFT\\nlibrary to invoke these PEFT methods for fine-tuning. For\\nBitFit, Child-tuing D, MAM adapter, QLoRA, and ProPELT,\\nwe experiment using their original code. Significantly, we\\nexperiment with sequential adapter using code from the MAM\\nadapter. For RoBERTa-base/large, all PEFT methods are fine-\\ntuned using a batch size of 32 and a sequence length of 128,\\nexcept for (IA) 3 which is fine-tuned using batch size 8. We\\nuse the batch size 64 for T5-base and 32 for T5-large. For\\nLLaMA-7B/13B, we use batch size 16 for fine-tuning. All\\nexperiments are implemented with A800 GPU.\\n7https://huggingface.co/docs/peft/index\\nB. Fine-tuning Performance and Parameter Efficiency\\n1) RoBERTa Base/Large on GLUE: Experimental results\\nof full fine-tuning and 11 representative PEFT methods with\\nRoBERTa-base/large on the GLUE benchmark are presented\\nin Table III, the following findings are observed:\\n• All PEFT methods reduce the number of trainable\\nparameters, and most PEFT methods achieve perfor-\\nmance matching or even better than full fine-tuning on\\nthe GLUE benchmark. For RoBERTa-base, the aver-\\nage performance of prompt-tuning, prefix-tuning, IA 3,\\nAdaLoRA, ProPELTprefix and ProPELTLoRA on GLUE all\\nunderperforms full finetuning, while that of sequential\\nadapter, BitFit, Child-Tuning D, LoRA, MAM adapter,\\nand ProPELT Adapter outperforms full fine-tuning. For\\nRoBERTa-large, the average performance of prompt-\\ntuning, prefix-tuning, IA3, AdaLoRA, ProPELT prefix and\\nChild-TuningD on GLUE underperforms full fine-tuning,\\nwhile that of sequential adapter, BitFit, LoRA, MAM\\nadapter, ProPELT Adapter and ProPELT LoRA outperforms\\nfull fine-tuning.\\n• ProPELTadapter, a unified fine-tuning method that em-\\nploys the AdapterFusion as the backbone, uses about\\n1.50% of the trainable parameters to fine-tune RoBERT-\\nbase and RoBERTa-large, but achieves optimal average\\nperformance on the GLUE benchmark, outperforming\\nRoBERT-base (FT) by about 1.30% and RoBERT-large\\n(FT) by about 1.65%.\\n• MAM Adapter, a hybrid fine-tuning method that com-\\nbines parallel adapters and prefix-tuning, achieves better\\nperformance than prefix-tuning, but also consumes a large'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='14\\nTABLE IV: Fine-tuning T5-base and T5-large models on the\\nWMT16 En-Ro dataset and evaluating their performance using\\nBLEU score. The higher BLEU score indicates a better quality\\nof translation output.\\nModel PEFT Method # TPs BLEU\\nT5-base\\nFT 222.9M 27.42\\n(IA)3 0.07M 27.58\\nLoRA 0.88M 27.78\\nT5-large\\nFT 737.7M 28.13\\n(IA)3 0.19M 28.12\\nLoRA 2.36M 28.12\\namount of trainable parameters.\\n• Sequential adapter requires more trainable parameters\\nthan prompt-tuning, prefix-tuning, (IA) 3, BitFit, Child-\\nTuningD, LoRA, and AdaLoRA, but achieves better\\nperformance than them on the GLUE benchmark.\\n• Prompt-tuning with the virtual marker length set to 20\\nachieves the smallest trainable parameter, but also the\\nworst performance, with its average performance on the\\nGLUE benchmark being about 10% lower than full fine-\\ntuning.\\n• Child-TuningD performs well when fine-tuning RoBERT-\\nbase on the GLUE benchmark and obtains better perfor-\\nmance than full fine-tuning, but performs poorly when\\nfine-tuning RoBERT-large on the MNLI dataset, which\\nwe guess it caused by the learning rate.\\n2) T5 Base/Large on WMT16 En-Ro Dataset: As depicted\\nin Table IV, both (IA) 3 and LoRA significantly reduce the\\nnumber of trainable parameters compared to full fine-tuning,\\nwhile maintaining comparable performance. Specifically, (IA)3\\nemploys only 0.03% of trainable parameters and achieves a\\nBLEU score [104] 0.16 higher than full fine-tuning for T5-\\nbase and 0.01 lower for T5-large. LoRA achieves a BLEU\\nscore 0.36 higher than full fine-tuning on T5-base using only\\n0.39% of trainable parameters, and 0.01 lower than full fine-\\ntuning on T5-large using only 0.32% of trainable parameters.\\n3) LLaMA on MMLU: We first tracked the 5-shot MMLU\\ndev accuracy of LLaMA-7B-Alpaca and LLaMA-13B-Alpaca\\nwith full fine-tuning and PEFT approaches LoRA, QLoRA,\\nand (IA)3, following the work in [49]. As depicted in Fig. 4,\\nthere are significant performance fluctuations in the 5-shot\\nMMLU dev accuracy throughout model training, particularly\\nin LoRA and QLoRA. Moreover, we discovered that full\\nfine-tuning performance of LLaMA-7B-Alpaca on the MMLU\\nbenchmark is extremely sensitive to the learning rate, as shown\\nin Table V. Subsequently, we select the checkpoint with the\\nbest performance on the dev set and perform 5-shot accuracy\\nexperiments on the test set of the MMLU benchmark.\\nAs illustrated in Table VI, full fine-tuning of both LLaMA-\\n7B and LLaMA-13B produces better 5-shot MMLU test\\naccuracy compared to other PEFT methods. (IA) 3, LoRA, and\\nQLoRA methods all greatly reduce the number of trainable\\nparameters with (IA) 3 performs best. Although (IA) 3 only\\nconsumes 0.02% of full fine-tuning parameters, it performs\\n2-4% lower than full fine-tuning. LoRA and QLoRA require\\nTABLE V: Full fine-tuning performance of LLaMA-7B-\\nAlpaca on the test set of MMLU benchmark with different\\nlearning rates.\\nLearning rate 5-shot MMLU Accuracy\\n2e-4 25.71\\n5e-5 26.65\\n1e-6 41.79\\nabout 2% of full fine-tuning parameters, achieving 5-shot\\nMMLU accuracy that is about 2% lower than full fine-tuning.\\nIn particular, QLoRA only uses half the number of trainable\\nparameters of LoRA but achieves comparable performance.\\nThis reduction of parameters in QLoRA can be attributed to\\nthe incorporation of 4-bit NormalFloat quantization.\\nC. Memory Efficiency\\nIt has been demonstrated that PEFT methods effectively re-\\nduce the number of trainable parameters. However, it remains\\nunclear whether they can also reduce GPU memory usage.\\nTo assess the impact of PEFT methods on GPU memory,\\nwe compare the GPU memory cost of full fine-tuning and\\nPEFT methods across various models and benchmarks. The\\nspecific experimental settings can be seen in the section of\\nimplementation details. As presented in Table VII, the mem-\\nory usage of full fine-tuning in RoBERTa, T5, and LLaMA\\nis positively related to total model parameters. RoBERTa,\\nspecifically RoBERTa-base, consumes less memory, requiring\\nonly 5.38GB. In contrast, LLaMA demands significantly larger\\nmemory, notably LLaMA-13B, necessitating approximately\\n290GB for full fine-tuning.\\nIn RoBERTa-base/large, prompt-tuning, prefix-tuning,\\n(IA)3, LoRA and AdaLoRA (implemented using the PEFT\\nlibrary), and BitFit significantly reduce the GPU memory\\nfootprint compared to full fine-tuning. Surprisingly, sequential\\nadapter, MAM adapter, Child-Tuning D, and ProPELT all use\\nmore memory than full fine-tuning. Both sequential adapter\\nand MAM adapter exhibit higher memory consumption,\\naround three times that of full fine-tuning, with the MAM\\nadapter consuming even more memory. For T5-base/large\\nmodels, (IA) 3 and LoRA all demonstrate effective memory\\nreduction during fine-tuning, with LoRA outperforming (IA) 3.\\nNotably, (IA) 3 consumes less GPU memory than LoRA in\\nRoBERTa-base/large, which is caused by the smaller batch\\nsize during (IA) 3 fine-tuning ((IA)3 sets the batch size to 8).\\nLikewise, (IA)3, LoRA, and QLoRA all significantly reduce\\nthe GPU footprint compared to full finetuning in LLaMA-\\n7B/13B. In addition, we discovered that the PEFT method is\\nmore effective in reducing memory usage when the number\\nof model parameters is larger. For example, in LLaMA-\\n7B-Alpaca, compared with full fine-tuning, IA 3, LoRA, and\\nQLoRA reduce memory usage by 24.08%, 26.30%, and\\n66.66%, respectively; while in LLaMA-13B-Alpaca, compared\\nwith full fine-tuning, IA3, LoRA, and QLoRA reduce memory\\nusage by 33.55%, 39.46% and 76.86% of memory usage.\\nNotably, QLoRA dramatically reduces GPU memory con-\\nsumption, with QLoRA fine-tuning the LLaMA-7B requiring'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='15\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca (FT)\\n(a) LLaMA-7B-Alpaca-FT\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca ((IA)3) (b) LLaMA-7B-Alpaca-(IA)3\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca (LoRA) (c) LLaMA-7B-Alpaca-LoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca (QLoRA) (d) LLaMA-7B-Alpaca-QLoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca (FT)\\n(e) LLaMA-13B-Alpaca-FT\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca ((IA)3) (f) LLaMA-13B-Alpaca-(IA)3\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca (LoRA) (g) LLaMA-13B-Alpaca-LoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca (QLoRA) (h) LLaMA-13B-Alpaca-QLoRA\\nFig. 4: The 5-shot accuracy fluctuates on the MMLU dev set with the increase in evaluation steps when fine-tuning LLaMA-\\n7B-Alpaca and LLaMA-7B-Alpaca using the IA 3, LoRA, and QLoRA methods.\\nTABLE VI: Comparison of the average 5-shot MMLU test accuracy of LLaMA-7B and LLaMA-13B models fine-tuned with\\nAlpaca. The higher the MMLU accuracy, the better. We also report total model parameters (# APs) and the ratio of trainable\\nparameters.\\nModel PEFT Method # TPs # APs % Params 5-shot MMLU Accuracy\\nLLaMA-7B-Alpaca\\nFT 6738.4M 6738.4M 100 41.79\\n(IA)3 1.58M 6740.0M 0.02 37.88\\nLoRA 159.9M 6898.3M 2.32 40.67\\nQLoRA 79.9M 3660.3M 2.18 39.96\\nLLaMA-13B-Alpaca\\nFT 13015.9M 13015.9M 100 49.60\\n(IA)3 2.48M 13018.3M 0.02 47.42\\nLoRA 250.3M 13266.2M 1.88 47.49\\nQLoRA 125.2M 6922.3M 1.81 47.29\\nonly 1/3 of the memory required for full fine-tuning, and fine-\\ntuning the LLaMA-13B requiring less than 1/4 of the memory\\nrequired for full fine-tuning. This advancement opens up the\\npossibility of fine-tuning LLMs for various downstream tasks\\nin computational resource-constrained scenarios.\\nV. A PPLICATIONS\\nA. Multi-task Learning\\nMulti-task learning is a method that involves training a\\nmodel on multiple related tasks and exploiting the informa-\\ntion shared and transferred between them to improve the\\nperformance of each task. PEFT methods such as adapters,\\nprompt-tuning, and LoRA utilize additional modules that can\\nbe plugged into PLMs and thus can be used for task-specific\\nfine-tuning to improve generalization of multi-task learning.\\nFor instance, studies from [19], [21], [22], [75] leverage task-\\nspecific adapters to learn information stored in multiple tasks\\nto achieve more robust transfer learning on new tasks. Several\\nworks [26], [27], [28] employ prompt-tuning for multi-task\\nlearning. They either utilize pretrained soft prompts from\\nmultiple source tasks to initialize the soft prompt of the\\ntarget task, based on the similarity between the source and\\ntarget tasks, or employ multi-task data to learn a single\\nshared prompt and transfer it to the target task. Similar to\\nthe adapter, a composition of multiple task-specific LoRA\\nmodules is also leveraged to transfer knowledge to new tasks\\n[55], [56]. L-LoRA [57] enhances the fusion capabilities of\\nmulti-task learning by preventing negative inference between\\ntask-specific representations. Additionally, [93] utilizes arith-\\nmetic operators, such as the addition and negation operators,\\nto merge parameters of various PEFT methods trained on\\ndifferent tasks for multi-task learning.\\nB. Cross-Lingual Transfer\\nCross-lingual transfer involves transferring knowledge or\\nmodels from one language to another. Numerous works have\\nemployed PEFT methods, such as adapters, for cross-lingual\\ntransfer due to their unique modular design. Bapna and Firat\\n[105] utilize sequential adapter [9] to fine-tune and restore\\nthe performance of a multilingual neural machine translation'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='16\\nTABLE VII: The peak GPU memory usage when fine-tuning RoBERT-base, RoBERTa-large, T5-base, T5-large, LLaMA-7B,\\nand LLaMA-13B model using full fine-tuning and various PEFT methods.\\nModel & Method Memory (GB) Model & Method Memory (GB)\\nRoBERTa-base (FT) 5.38 RoBERTa-large (FT) 11.96\\nRoBERTa-base (AdapterS) 15.29 RoBERTa-large (AdapterS) 37.17\\nRoBERTa-base (Prompt-tuning) 3.84 RoBERTa-large (Prompt-tuning) 7.98\\nRoBERTa-base (Prefix-tuning) 3.56 RoBERTa-large (Prefix-tuning) 7.58\\nRoBERTa-base ((IA)3) 2.62 RoBERTa-large ((IA)3) 4.83\\nRoBERTa-base (BitFit) 3.27 RoBERTa-large (BitFit) 7.50\\nRoBERTa-base (Child-TuningD) 6.02 RoBERTa-large (Child-TuningD) 13.67\\nRoBERTa-base (LoRA) 3.59 RoBERTa-large (LoRA) 7.50\\nRoBERTa-base (AdaLoRA) 3.57 RoBERTa-large (AdaLoRA) 7.43\\nRoBERTa-base (MAM Adapter) 15.35 RoBERTa-large (MAM Adapter) 37.82\\nRoBERTa-base (ProPELTAdapter) 8.63 RoBERTa-large (ProPELTAdapter) 19.82\\nRoBERTa-base (ProPELTPrefix) 9.47 RoBERTa-large (ProPELTPrefix) 22.85\\nRoBERTa-base (ProPELTLoRA) 8.25 RoBERTa-large (ProPELTLoRA) 19.52\\nT5-base (FT) 25.17 T5-large (FT) 30.17\\nT5-base ((IA)3) 21.36 T5-large ((IA)3) 25.71\\nT5-base (LoRA) 19.43 T5-large (LoRA) 23.77\\nLLaMA-7B-Alpaca (FT) 169.36 LLaMA-13B-Alpaca (FT) 287.79\\nLLaMA-7B-Alpaca ((IA)3) 128.57 LLaMA-13B-Alpaca ((IA)3) 191.24\\nLLaMA-7B-Alpaca (LoRA) 124.82 LLaMA-13B-Alpaca (LoRA) 174.24\\nLLaMA-7B-Alpaca (QLoRA) 56.46 LLaMA-13B-Alpaca (QLoRA) 66.60\\nmodel on high-resource languages. Artetxe et al. [106] employ\\nsequential adapter [9] to transfer a pretrained monolingual\\nmodel to an unseen language. MAD-X [75], [107] uses\\nlanguage-specific, task-specific, and invertible adapter to learn\\nlanguage-specific and task-specific transformations, as well\\nas address vocabulary mismatches between multilingual and\\ntarget languages in a modular manner, enabling the adaptation\\nof pretrained multilingual models to target languages. MAD-G\\n[108] generates language adapters from language representa-\\ntions based on typological features, allowing the sharing of lin-\\nguistic knowledge across languages for cross-lingual transfer.\\nLT-SFT [38] employs sparse fine-tuning to train the model on\\nthe source language and learn task-specific sparse difference\\nvectors for cross-lingual transfer. While BAD-X [109] trains a\\nbilingual language-pair adapter on both the source and target\\nlanguages for zero-shot cross-lingual transfer.\\nC. Backdoor Attacks and Defense\\nBackdoor attacks pose a significant security threat, where\\na small portion of training samples are contaminated with\\nmalicious backdoor triggers. When trained on such poisoned\\ndatasets, the model behaves normally on benign samples but\\npredicts attacker-selected labels on samples containing the\\npredefined triggers. The susceptibility of PLMs to backdoor at-\\ntacks poses a substantial risk to real-world applications [110].\\nBuilding on the vulnerability of pretrained weights, Gu et al.\\n[111] employ the PEFT methods to construct backdoor attacks,\\nin which backdoor attacks are directly injected into PEFT\\nmodules. However, Zhu et al. [112] discover that PEFT can\\nserve as a backdoor defense solution by reducing the model\\ncapacity via optimizing only a small number of parameters.\\nThe findings from [113] also confirm that PEFT can slightly\\nweaken the backdoor attacks and design a novel trojan attack\\nfor the PEFT paradigm.\\nVI. F URTHER DIRECTIONS\\nA. Lightweight Hybrid PEFT Methods\\nThere exist many approaches [16], [35], [58], [59], [60],\\n[61], [62] to combine multiple PEFT methods, aiming to\\nleverage the distinctive advantages of each PEFT method and\\nachieve enhanced performance. Nevertheless, the exploration\\nhas been limited to PEFT methods such as adapter, LoRA,\\nprefix-tuning, and BitFit, leaving room for further exploitation\\nby incorporating additional combinations of PEFT methods.\\nMoreover, while drawing inspiration from the NAS algorithm,\\nseveral PEFT methods [60], [61] have been investigated using\\ndiverse optimization techniques to explore optimal neural\\nnetwork architectures for configuring these PEFT methods.\\nThere remains potential for continued exploration in utilizing\\nother optimization methods to automatically search for neural\\nnetwork architectures and configure specific combinations of\\nPEFT modules at specific layers. Additionally, utilizing mul-\\ntiple PEFT methods typically results in increased parameter\\nand memory usage, although it enhances performance. Hence,\\nan intriguing research direction involves investigating how\\nto leverage multiple PEFT methods to improve performance\\nwhile minimizing the number of trainable parameters.\\nB. LoRA-derived PEFT Methods\\nRecently, a multitude of LoRA-based PEFT methods have\\nemerged, as demonstrated in Fig. 1. These methods further\\nenhance LoRA by incorporating adaptive rank adjustment, un-\\nstructured pruning techniques, weight quantization, and multi-\\ntask integration. This encourages future research to develop'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='17\\nmore LoRA-derived PEFT approaches build upon LoRA.\\nParticular emphasis should be placed on pruning technol-\\nogy and weight quantification. The application of pruning\\ntechniques can be extended not only to AdaLoRA [45] for\\nrank adjustment but also to LoRAPrune [48] for pruning\\nboth pretrained and LoRA weights. Notably, pruning and\\nweight quantization techniques effectively reduce the number\\nof trainable parameters, compress model size, optimize storage\\nand computational requirements of PLMs (especially LLMs),\\nand enhance their utility and scalability across downstream\\ntasks. These techniques can be further explored in conjunction\\nwith LoRA to unlock synergistic benefits.\\nC. Developing PEFT Library\\nNumerous PEFT methods have emerged, but employing\\nthem is not a straightforward endeavor. To address this\\nchallenge, the PEFT library 8 and AdapterHub 9 have been\\ndeveloped. These libraries integrate commonly used PEFT\\nmethods such as prefix-tuning, LoRA, and AdaLoRA. With\\njust a few lines of code, users can directly invoke these\\nPEFT methods, simplifying their usage. Moreover, both the\\nPEFT and AdapterHub libraries offer a range of examples\\nillustrating how to apply these PEFT methods to various PLMs\\nand LLMs for fine-tuning downstream tasks. However, not\\nall PEFT methods are currently integrated into these two\\nlibraries. Future efforts can be directed towards expanding\\nthe integration of additional methods, further boosting the\\napplication development of PEFT methods.\\nD. Explainability of PEFT Methods\\nThough numerous PEFT methods have been proposed, there\\nis a lack of comprehensive studies exploring the reasons\\nbehind their ability to achieve comparable performance and\\nreduce trainable parameters. Work from [41] unifies PEFT\\nmethods under the concept of sparse fine-tuned models and\\nprovides a theoretical analysis demonstrating that sparsity can\\nserve as a regularization technique for the original model,\\neffectively controlling the upper bound of stability. While\\n[114] explores and analyzes the express power of LoRA for\\nfully connected neural networks and transformer networks,\\nshowing the conditions under which there exist effective low-\\nrank adapters for a given task. These studies shed light on the\\nworking mechanism and effectiveness of certain PEFT meth-\\nods, but still lack generalization. Future research endeavors\\ncould focus on advancing theoretical studies to unravel the\\nunderlying working mechanisms of PEFT methods.\\nE. Exploring PEFT Methods in Computer Vision and Multi-\\nmodal Learning\\nThough PEFT methods have been extensively studied in\\nNLP, their application in computer vision and multimodal\\nlearning also shows great potential for further exploration.\\nThe sequential adapter in NLP, initially inspired by multi-\\ndomain image classification [77], [115], has paved the way for\\n8https://github.com/huggingface/peft/tree/main\\n9https://adapterhub.ml/\\nrapid advancements in PEFT methods for PLMs. Moreover,\\nresearchers have increasingly delved into various PEFT tech-\\nniques for computer vision [116], [117], as well as language-\\nimage and image-audio multimodal learning [118], [119],\\nbuilding upon PEFT methods in NLP [9], [11], [58]. However,\\nthere is still significant room for further exploration and\\nexploitation in these domains. In particular, PEFT methods\\nhold the potential to facilitate cross-modality transfer in multi-\\nmodal learning. By fine-tuning pretrained models using PEFT\\ntechniques, knowledge acquired from one modality can be\\neffectively transferred to another, resulting in improved per-\\nformance in multimodal tasks. Consequently, the application\\nof PEFT methods in computer vision and multimodal learning\\nholds tremendous promise as a future research direction.\\nVII. C ONCLUSIONS\\nThis paper presents a comprehensive and structured study of\\nPEFT methods for PLMs. By classifying the PEFT methods in\\nNLP, we identify the main techniques and challenges associ-\\nated with them. We employ several representative PEFT meth-\\nods to fine-tune encoder-based RoBERTa, encoder-decoder-\\nbased T5, and decoder-based LLaMA on various downstream\\ntasks. Experimental results reveal that most PEFT methods\\nsignificantly improve parameter efficiency and achieve com-\\nparable or even better performance compared to full fine-\\ntuning. Additionally, most PEFT methods lower the memory\\nfootprint, with QLoRA drastically reducing the computational\\nmemory requirement, and alleviating the memory challenge\\nwhen fine-tuning LLMs. Furthermore, we introduce common\\napplications of PEFT methods and outline future research\\ndirections. As the development of LLMs continues, there is\\na clear need to develop PEFT methods that can effectively\\nreduce computational resource demands and memory usage\\nduring fine-tuning. This survey aims to provide a bird’s-\\neye view of PEFT methods for PLMs and inspiring further\\nresearch in this area.\\nREFERENCES\\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\\ntraining of deep bidirectional transformers for language understanding,”\\nin Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Hum.\\nLang. Technol., 2019, pp. 4171–4186.\\n[2] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” in Proc. Int. Conf. Learn. Representations, 2020.\\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n“Language models are unsupervised multitask learners,” OpenAI blog,\\nvol. 1, no. 8, p. 9, 2019.\\n[4] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\\nwith a unified text-to-text transformer,” J. Mach. Learn. Res. , vol. 21,\\nno. 1, pp. 5485–5551, 2020.\\n[5] S. Zhang, M. Diab, and L. Zettlemoyer, “Democratizing access to large-\\nscale language models with opt-175b,” Meta AI, 2022.\\n[6] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\\nR. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “Bloom: A 176b-\\nparameter open-access multilingual language model,” arXiv preprint\\narXiv:2211.05100, 2022.\\n[7] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\\nOpen and efficient foundation language models,” arXiv preprint\\narXiv:2302.13971, 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='18\\n[8] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru,\\nM. Alhammadi, M. Daniele, D. Heslow, J. Launay, Q. Malartic et al.,\\n“The falcon series of language models: Towards open frontier models,”\\nHugging Face repository, 2023.\\n[9] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer\\nlearning for nlp,” in Proc. Int. Conf. Mach. Learn. PMLR, 2019, pp.\\n2790–2799.\\n[10] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts\\nfor generation,” in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 4582–4597.\\n[11] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nand W. Chen, “LoRA: Low-rank adaptation of large language models,”\\nin Proc. Int. Conf. Learn. Representations , 2022.\\n[12] N. Ding, Y . Qin, G. Yang, F. Wei, Z. Yang, Y . Su, S. Hu, Y . Chen,\\nC.-M. Chan, W. Chen et al. , “Delta tuning: A comprehensive study\\nof parameter efficient methods for pre-trained language models,” arXiv\\npreprint arXiv:2203.06904, 2022.\\n[13] V . Lialin, V . Deshpande, and A. Rumshisky, “Scaling down to\\nscale up: A guide to parameter-efficient fine-tuning,” arXiv preprint\\narXiv:2303.15647, 2023.\\n[14] Z. Lin, A. Madotto, and P. Fung, “Exploring versatile generative\\nlanguage model via parameter-efficient transfer learning,” in Proc.\\nFindings Conf. Empir. Methods Natural Lang. Process., 2020, pp. 441–\\n459.\\n[15] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Y .\\nZhao, Y . Wu, B. Li et al. , “Conditional adapters: Parameter-efficient\\ntransfer learning with fast inference,” arXiv preprint arXiv:2304.04947,\\n2023.\\n[16] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Towards\\na unified view of parameter-efficient transfer learning,” in Proc. Int.\\nConf. Learn. Representations , 2022.\\n[17] A. R ¨uckl´e, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers,\\nand I. Gurevych, “AdapterDrop: On the efficiency of adapters in\\ntransformers,” in Proc. Conf. Empir. Methods Natural Lang. Process. ,\\n2021, pp. 7930–7946.\\n[18] H. Zhao, H. Tan, and H. Mei, “Tiny-attention adapter: Contexts are\\nmore important than the number of parameters,” in Proc. Conf. Empir.\\nMethods Natural Lang. Process. , 2022, pp. 6626–6638.\\n[19] J. Pfeiffer, A. Kamath, A. R ¨uckl´e, K. Cho, and I. Gurevych, “Adapter-\\nFusion: Non-destructive task composition for transfer learning,” in\\nProc. Conf. Eur. Chapter Assoc. Comput. Linguistics , 2021, pp. 487–\\n503.\\n[20] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao, “Mera:\\nMerging pretrained adapters for few-shot learning,” arXiv preprint\\narXiv:2308.15982, 2023.\\n[21] R. Karimi Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n“Parameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,” in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 565–576.\\n[22] A. Chronopoulou, M. Peters, A. Fraser, and J. Dodge, “AdapterSoup:\\nWeight averaging to improve generalization of pretrained language\\nmodels,” in Proc. Findings Assoc. Comput. Linguistics, 2023, pp. 2054–\\n2063.\\n[23] K. Hambardzumyan, H. Khachatrian, and J. May, “W ARP: Word-level\\nAdversarial ReProgramming,” in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics, Int. Joint Conf. Natural Lang. Process. , 2021, pp. 4921–\\n4933.\\n[24] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\\nparameter-efficient prompt tuning,” in Proc. Conf. Empir. Methods\\nNatural Lang. Process., 2021, pp. 3045–3059.\\n[25] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang, “Gpt\\nunderstands, too,” arXiv preprint arXiv:2103.10385 , 2021.\\n[26] T. Vu, B. Lester, N. Constant, R. Al-Rfou’, and D. Cer, “SPoT: Better\\nfrozen model adaptation through soft prompt transfer,” in Proc. Annu.\\nMeeting Assoc. Comput. Linguistics , 2022, pp. 5039–5059.\\n[27] A. Asai, M. Salehi, M. Peters, and H. Hajishirzi, “ATTEMPT:\\nParameter-efficient multi-task tuning via attentional mixtures of soft\\nprompts,” in Proc. Conf. Empir. Methods Natural Lang. Process., 2022,\\npp. 6655–6672.\\n[28] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim,\\n“Multitask prompt tuning enables parameter-efficient transfer learning,”\\nin Proc. Int. Conf. Learn. Representations , 2023.\\n[29] Y .-L. Sung, J. Cho, and M. Bansal, “LST: Ladder side-tuning for\\nparameter and memory efficient transfer learning,” in Proc. Adv. Neural\\nInf. Process. Syst. , 2022.\\n[30] H. Liu, D. Tam, M. Mohammed, J. Mohta, T. Huang, M. Bansal,\\nand C. Raffel, “Few-shot parameter-efficient fine-tuning is better and\\ncheaper than in-context learning,” in Proc. Adv. Neural Inf. Process.\\nSyst., 2022.\\n[31] X. Yang, J. Y . Huang, W. Zhou, and M. Chen, “Parameter-efficient\\ntuning with special token adaptation,” in Proc. Conf. Eur. Chapter\\nAssoc. Comput. Linguistics , 2023, pp. 865–872.\\n[32] J. Cao, C. Satya Prakash, and W. Hamza, “Attention fusion: a light yet\\nefficient late fusion mechanism for task adaptation in NLU,” in Proc.\\nFindings Assoc. Comput. Linguistics , 2022, pp. 857–866.\\n[33] Y . Chen, Q. Fu, G. Fan, L. Du, J.-G. Lou, S. Han, D. Zhang, Z. Li, and\\nY . Xiao, “Hadamard adapter: An extreme parameter-efficient adapter\\ntuning method for pre-trained language models,” in Proc. 32nd ACM\\nInt. Conf. Inf. Knowl. Manage. , 2023, pp. 276–285.\\n[34] E. Ben Zaken, Y . Goldberg, and S. Ravfogel, “BitFit: Simple\\nparameter-efficient fine-tuning for transformer-based masked language-\\nmodels,” in Proc. Annu. Meeting Assoc. Comput. Linguistics, 2022, pp.\\n1–9.\\n[35] N. Lawton, A. Kumar, G. Thattai, A. Galstyan, and G. Ver Steeg,\\n“Neural architecture search for parameter-efficient fine-tuning of large\\npre-trained language models,” in Proc. Findings Assoc. Comput. Lin-\\nguistics, 2023, pp. 8506–8515.\\n[36] M. Zhao, T. Lin, F. Mi, M. Jaggi, and H. Sch ¨utze, “Masking as an\\nefficient alternative to finetuning for pretrained language models,” in\\nProc. Conf. Empir. Methods Natural Lang. Process. , 2020, pp. 2226–\\n2241.\\n[37] Y .-L. Sung, V . Nair, and C. Raffel, “Training neural networks with\\nfixed sparse masks,” in Proc. Adv. Neural Inf. Process. Syst. , 2021.\\n[38] A. Ansell, E. Ponti, A. Korhonen, and I. Vuli ´c, “Composable sparse\\nfine-tuning for cross-lingual transfer,” in Proc. Annu. Meeting Assoc.\\nComput. Linguistics, 2022, pp. 1778–1796.\\n[39] R. Xu, F. Luo, Z. Zhang, C. Tan, B. Chang, S. Huang, and F. Huang,\\n“Raise a child in large language model: Towards effective and gen-\\neralizable fine-tuning,” in Proc. Conf. Empir. Methods Natural Lang.\\nProcess., 2021, pp. 9514–9528.\\n[40] D. Guo, A. Rush, and Y . Kim, “Parameter-efficient transfer learning\\nwith diff pruning,” in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 4884–4896.\\n[41] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, and N. Collier, “On the\\neffectiveness of parameter-efficient fine-tuning,” in Proc. AAAI Conf.\\nArtif. Intell., vol. 37, no. 11, 2023, pp. 12 799–12 807.\\n[42] A. Aghajanyan, S. Gupta, and L. Zettlemoyer, “Intrinsic dimensionality\\nexplains the effectiveness of language model fine-tuning,” in Proc.\\nAnnu. Meeting Assoc. Comput. Linguistics, Int. Joint Conf. Natural\\nLang. Process., 2021, pp. 7319–7328.\\n[43] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Reza-\\ngholizadeh, “Krona: Parameter efficient tuning with kronecker adapter,”\\narXiv preprint arXiv:2212.10650 , 2022.\\n[44] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, “Dy-\\nLoRA: Parameter-efficient tuning of pre-trained models using dynamic\\nsearch-free low-rank adaptation,” in Proc. Conf. Eur. Chapter Assoc.\\nComput. Linguistics, 2023, pp. 3274–3287.\\n[45] Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and\\nT. Zhao, “Adaptive budget allocation for parameter-efficient fine-\\ntuning,” in Proc. Int. Conf. Learn. Representations , 2023.\\n[46] F. Zhang, L. Li, J. Chen, Z. Jiang, B. Wang, and Y . Qian, “Increlora:\\nIncremental parameter allocation method for parameter-efficient fine-\\ntuning,” arXiv preprint arXiv:2308.12043 , 2023.\\n[47] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang, “Delta-lora:\\nFine-tuning high-rank parameters with the delta of low-rank matrices,”\\narXiv preprint arXiv:2309.02411 , 2023.\\n[48] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang et al., “Prun-\\ning meets low-rank parameter-efficient fine-tuning,” arXiv preprint\\narXiv:2305.18403, 2023.\\n[49] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-\\nficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\\n2023.\\n[50] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang,\\nand Q. Tian, “Qa-lora: Quantization-aware low-rank adaptation of large\\nlanguage models,” arXiv preprint arXiv:2309.14717 , 2023.\\n[51] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and\\nT. Zhao, “Loftq: Lora-fine-tuning-aware quantization for large language\\nmodels,” arXiv preprint arXiv:2310.08659 , 2023.\\n[52] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-\\nTur, “Empowering parameter-efficient transfer learning by recognizing\\nthe kernel structure in self-attention,” in Proc. Findings Assoc. Comput.\\nLinguistics, 2022, pp. 1375–1388.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='19\\n[53] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison, “Bayesian\\nlow-rank adaptation for large language models,” arXiv preprint\\narXiv:2308.13111, 2023.\\n[54] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li, “Lora-fa: Memory-\\nefficient low-rank adaptation for large language models fine-tuning,”\\narXiv preprint arXiv:2308.03303 , 2023.\\n[55] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin, “Lorahub:\\nEfficient cross-task generalization via dynamic lora composition,”arXiv\\npreprint arXiv:2307.13269, 2023.\\n[56] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng,\\n“Moelora: An moe-based parameter efficient fine-tuning method for\\nmulti-task medical applications,” arXiv preprint arXiv:2310.18339 ,\\n2023.\\n[57] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao,\\n“Parameter efficient multi-task model fusion with partial linearization,”\\narXiv preprint arXiv:2310.04742 , 2023.\\n[58] R. Karimi Mahabadi, J. Henderson, and S. Ruder, “Compacter: Effi-\\ncient low-rank hypercomplex adapter layers,” Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 34, pp. 1022–1035, 2021.\\n[59] Y . Mao, L. Mathias, R. Hou, A. Almahairi, H. Ma, J. Han, S. Yih, and\\nM. Khabsa, “UniPELT: A unified framework for parameter-efficient\\nlanguage model tuning,” in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics, 2022, pp. 6253–6264.\\n[60] H. Zhou, X. Wan, I. Vuli ´c, and A. Korhonen, “Autopeft: Automatic\\nconfiguration search for parameter-efficient fine-tuning,” arXiv preprint\\narXiv:2301.12132, 2023.\\n[61] S. Hu, Z. Zhang, N. Ding, Y . Wang, Y . Wang, Z. Liu, and M. Sun,\\n“Sparse structure search for delta tuning,” Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 35, pp. 9853–9865, 2022.\\n[62] J. Chen, A. Zhang, X. Shi, M. Li, A. Smola, and D. Yang, “Parameter-\\nefficient fine-tuning design spaces,” in Proc. Int. Conf. Learn. Repre-\\nsentations, 2023.\\n[63] Y . Wang, S. Agarwal, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah,\\nand J. Gao, “AdaMix: Mixture-of-adaptations for parameter-efficient\\nmodel tuning,” in Proc. Conf. Empir. Methods Natural Lang. Process. ,\\n2022, pp. 5744–5760.\\n[64] S. He, L. Ding, D. Dong, J. Zhang, and D. Tao, “SparseAdapter: An\\neasy approach for improving the parameter-efficiency of adapters,” in\\nProc. Findings Conf. Empir. Methods Natural Lang. Process. , 2022,\\npp. 2184–2190.\\n[65] G. Zeng, P. Zhang, and W. Lu, “One network, many masks: Towards\\nmore parameter-efficient transfer learning,” in Proc. Annu. Meeting\\nAssoc. Comput. Linguistics , 2023, pp. 7564–7580.\\n[66] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Proc.\\nAdv. Neural Inf. Process. Syst. , vol. 30, 2017.\\n[67] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\\npp. 770–778.\\n[68] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\\npreprint arXiv:1607.06450, 2016.\\n[69] L. Xu and W. Wang, “Improving aspect-based sentiment analysis with\\ncontrastive learning,” Natural Language Processing Journal , vol. 3, p.\\n100009, 2023.\\n[70] Y . Xie, W. Yang, L. Tan, K. Xiong, N. J. Yuan, B. Huai, M. Li, and\\nJ. Lin, “Distant supervision for multi-stage fine-tuning in retrieval-\\nbased question answering,” in Proceedings of The Web Conference ,\\n2020, pp. 2934–2940.\\n[71] R. Dabre, A. Fujita, and C. Chu, “Exploiting multilingualism through\\nmultistage fine-tuning for low-resource neural machine translation,” in\\nProc. Conf. Empir. Methods Natural Lang. Process., Int. Joint Conf.\\nNatural Lang. Process., 2019, pp. 1410–1416.\\n[72] M. T. Hosseini, A. Ghaffari, M. S. Tahaei, M. Rezagholizadeh,\\nM. Asgharian, and V . P. Nia, “Towards fine-tuning pre-trained language\\nmodels with integer forward and backward propagation,” in Proc.\\nFindings Assoc. Comput. Linguistics , 2023, pp. 1867–1876.\\n[73] S.-i. Amari, “Backpropagation and stochastic gradient descent method,”\\nNeurocomputing, vol. 5, no. 4-5, pp. 185–196, 1993.\\n[74] L. Xu, H. Xie, Z. Li, F. L. Wang, W. Wang, and Q. Li, “Contrastive\\nlearning models for sentence representations,” ACM Trans. Intel. Syst.\\nTec., vol. 14, no. 4, pp. 1–34, 2023.\\n[75] J. Pfeiffer, I. Vuli ´c, I. Gurevych, and S. Ruder, “MAD-X: An Adapter-\\nBased Framework for Multi-Task Cross-Lingual Transfer,” in Proc.\\nConf. Empir. Methods Natural Lang. Process. , 2020, pp. 7654–7673.\\n[76] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li, “Counter-\\ninterference adapter for multilingual machine translation,” arXiv\\npreprint arXiv:2104.08154, 2021.\\n[77] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Learning multiple visual\\ndomains with residual adapters,” Proc. Adv. Neural Inf. Process. Syst. ,\\nvol. 30, 2017.\\n[78] J. Solomon, F. De Goes, G. Peyr ´e, M. Cuturi, A. Butscher, A. Nguyen,\\nT. Du, and L. Guibas, “Convolutional wasserstein distances: Efficient\\noptimal transportation on geometric domains,” ACM Trans. Graph. ,\\nvol. 34, no. 4, pp. 1–11, 2015.\\n[79] S. P. Singh and M. Jaggi, “Model fusion via optimal transport,” Proc.\\nAdv. Neural Inf. Process. Syst. , vol. 33, pp. 22 045–22 055, 2020.\\n[80] D. Ha, A. M. Dai, and Q. V . Le, “Hypernetworks,” in Proc. Int. Conf.\\nLearn. Representations, 2017.\\n[81] R. Aharoni and Y . Goldberg, “Unsupervised domain clusters in pre-\\ntrained language models,” in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics, 2020, pp. 7747–7763.\\n[82] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning\\nfilters for efficient convnets,” inProc. Int. Conf. Learn. Representations,\\n2017.\\n[83] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning, “What does\\nBERT look at? an analysis of BERT’s attention,” in Proc. of 2019 ACL\\nWorkshop BlackboxNLP, 2019, pp. 276–286.\\n[84] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky, “Revealing\\nthe dark secrets of BERT,” in Proc. Conf. Empir. Methods Natural\\nLang. Process., Int. Joint Conf. Natural Lang. Process. , 2019, pp.\\n4365–4374.\\n[85] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A\\nsurvey,” J. Mach. Learn. Res. , vol. 20, no. 1, pp. 1997–2017, 2019.\\n[86] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding\\nsparse, trainable neural networks,” in Proc. Int. Conf. Learn. Repre-\\nsentations, 2019.\\n[87] Q. Le, T. Sarl ´os, and A. Smola, “Fastfood-computing hilbert space\\nexpansions in loglinear time,” in Proc. Int. Conf. Mach. Learn. PMLR,\\n2013, pp. 244–252.\\n[88] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Importance\\nestimation for neural network pruning,” in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit., 2019, pp. 11 264–11 272.\\n[89] V . Sanh, T. Wolf, and A. Rush, “Movement pruning: Adaptive sparsity\\nby fine-tuning,” Proc. Adv. Neural Inf. Process. Syst. , vol. 33, pp.\\n20 378–20 389, 2020.\\n[90] D. J. MacKay, “A practical bayesian framework for backpropagation\\nnetworks,” Neural Comput., vol. 4, no. 3, pp. 448–472, 1992.\\n[91] J. Liu, A. Moreau, M. Preuss, J. Rapin, B. Roziere, F. Teytaud, and\\nO. Teytaud, “Versatile black-box optimization,” in Proc. of the 2020\\nGenet. and Evolut. Comput. Conf. , 2020, pp. 620–628.\\n[92] G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, and\\nA. Farhadi, “Editing models with task arithmetic,” in Proc. Int. Conf.\\nLearn. Representations, 2023.\\n[93] J. Zhang, S. Chen, J. Liu, and J. He, “Composing parameter-efficient\\nmodules with arithmetic operations,” arXiv preprint arXiv:2306.14870,\\n2023.\\n[94] P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal, “Resolving\\ninterference when merging models,” arXiv preprint arXiv:2306.01708,\\n2023.\\n[95] A. Zhang, Y . Tay, S. Zhang, A. Chan, A. T. Luu, S. Hui, and J. Fu,\\n“Beyond fully-connected layers with quaternions: Parameterization of\\nhypercomplex multiplications with $1/n$ parameters,” in Proc. Int.\\nConf. Learn. Representations , 2021.\\n[96] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017.\\n[97] J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin, “Pruning neural\\nnetworks at initialization: Why are we missing the mark?” in Proc. Int.\\nConf. Learn. Representations , 2021.\\n[98] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu,\\nand A. Liotta, “Scalable training of artificial neural networks with\\nadaptive sparse connectivity inspired by network science,” Nature\\ncommunications, vol. 9, no. 1, p. 2383, 2018.\\n[99] N. Lee, T. Ajanthan, and P. Torr, “SNIP: Single-shot network pruning\\nbased on connection sensitivity,” in Proc. Int. Conf. Learn. Represen-\\ntations, 2019.\\n[100] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\\n“GLUE: A multi-task benchmark and analysis platform for natural\\nlanguage understanding,” in Proc. of 2018 EMNLP Workshop Black-\\nboxNLP, 2018, pp. 353–355.\\n[101] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n“Albert: A lite bert for self-supervised learning of language represen-\\ntations,” in Proc. Int. Conf. Learn. Representations , 2020.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='20\\n[102] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, “Stanford alpaca: An instruction-following llama\\nmodel,” 2023.\\n[103] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\nin Proc. Int. Conf. Learn. Representations , 2021.\\n[104] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\\nautomatic evaluation of machine translation,” in Proc. Annu. Meeting\\nAssoc. Comput. Linguistics , 2002, pp. 311–318.\\n[105] A. Bapna and O. Firat, “Simple, scalable adaptation for neural machine\\ntranslation,” in Proc. Conf. Empir. Methods Natural Lang. Process., Int.\\nJoint Conf. Natural Lang. Process. , 2019, pp. 1538–1548.\\n[106] M. Artetxe, S. Ruder, and D. Yogatama, “On the cross-lingual transfer-\\nability of monolingual representations,” in Proc. Annu. Meeting Assoc.\\nComput. Linguistics, 2020, pp. 4623–4637.\\n[107] J. Pfeiffer, I. Vuli ´c, I. Gurevych, and S. Ruder, “UNKs everywhere:\\nAdapting multilingual language models to new scripts,” in Proc. Conf.\\nEmpir. Methods Natural Lang. Process. , 2021, pp. 10 186–10 203.\\n[108] A. Ansell, E. M. Ponti, J. Pfeiffer, S. Ruder, G. Glava ˇs, I. Vuli ´c, and\\nA. Korhonen, “MAD-G: Multilingual adapter generation for efficient\\ncross-lingual transfer,” in Proc. Findings Conf. Empir. Methods Natural\\nLang. Process., 2021, pp. 4762–4781.\\n[109] M. Parovi ´c, G. Glavaˇs, I. Vuli´c, and A. Korhonen, “BAD-X: Bilingual\\nadapters improve zero-shot cross-lingual transfer,” in Proc. Conf. North\\nAmer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., 2022,\\npp. 1791–1799.\\n[110] M. T ¨anzer, S. Ruder, and M. Rei, “Memorisation versus generalisa-\\ntion in pre-trained language models,” in Proc. Annu. Meeting Assoc.\\nComput. Linguistics, 2022, pp. 7564–7578.\\n[111] N. Gu, P. Fu, X. Liu, Z. Liu, Z. Lin, and W. Wang, “A gradient control\\nmethod for backdoor attacks on parameter-efficient tuning,” in Proc.\\nAnnu. Meeting Assoc. Comput. Linguistics , 2023, pp. 3508–3520.\\n[112] B. Zhu, Y . Qin, G. Cui, Y . Chen, W. Zhao, C. Fu, Y . Deng, Z. Liu,\\nJ. Wang, W. Wu, M. Sun, and M. Gu, “Moderate-fitting as a natural\\nbackdoor defender for pre-trained language models,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2022.\\n[113] L. Hong and T. Wang, “Fewer is more: Trojan attacks on parameter-\\nefficient fine-tuning,” arXiv preprint arXiv:2310.00648 , 2023.\\n[114] Y . Zeng and K. Lee, “The expressive power of low-rank adaptation,”\\narXiv preprint arXiv:2310.17513 , 2023.\\n[115] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Efficient parametrization of\\nmulti-domain deep neural networks,” in Proc. IEEE Conf. Comput. Vis.\\nPattern Recognit., 2018, pp. 8119–8127.\\n[116] X. He, C. Li, P. Zhang, J. Yang, and X. E. Wang, “Parameter-efficient\\nmodel adaptation for vision transformers,” in Proc. AAAI Conf. Artif.\\nIntell., vol. 37, no. 1, 2023, pp. 817–825.\\n[117] Z. Xu, Z. Chen, Y . Zhang, Y . Song, X. Wan, and G. Li, “Bridging vision\\nand language encoders: Parameter-efficient tuning for referring image\\nsegmentation,” in IEEE Int. Conf. Comput. Vis. , 2023, pp. 17 503–\\n17 512.\\n[118] Y .-L. Sung, J. Cho, and M. Bansal, “Vl-adapter: Parameter-efficient\\ntransfer learning for vision-and-language tasks,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , 2022, pp. 5227–5237.\\n[119] J. Pan, Z. Lin, X. Zhu, J. Shao, and H. Li, “St-adapter: Parameter-\\nefficient image-to-video transfer learning,” Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 35, pp. 26 462–26 477, 2022.\\nLingling Xu (Student Member, IEEE) is cur-\\nrently pursuing her Ph.D. degree at Hong Kong\\nMetropolitan University. She received a Master de-\\ngree in Mathematics from Shandong University. Her\\nresearch interests include parameter-efficient fine-\\ntuning, contrastive learning, representation learning,\\nand aspect-based sentiment analysis.\\nHaoran Xie (Senior Member, IEEE) received a\\nPh.D. degree in Computer Science from City Uni-\\nversity of Hong Kong and an Ed.D degree in Digital\\nLearning from the University of Bristol. He is cur-\\nrently the Department Head and Associate Professor\\nat the Department of Computing and Decision Sci-\\nences, Lingnan University, Hong Kong. His research\\ninterests include artificial intelligence, big data, and\\neducational technology. He has published 393 re-\\nsearch publications, including 224 journal articles\\nsuch as IEEE TPAMI, IEEE TKDE, IEEE TAFFC,\\nand IEEE TCVST. He is the Editor-in-Chief of Natural Language Processing\\nJournal, Computers & Education: Artificial Intelligence and Computers &\\nEducation: X Reality. He has been selected listed as the World’s Top 2%\\nScientists by Stanford University.\\nSi-Zhao Joe Qin (Fellow, IEEE) received the B.S.\\nand M.S. degrees in automatic control from Ts-\\ninghua University, Beijing, China, in 1984 and 1987,\\nrespectively, and the Ph.D. degree in chemical en-\\ngineering from the University of Maryland, College\\nPark, MD, USA, in 1992. He is currently the Wai\\nKee Kau Chair Professor and President of Lingnan\\nUniversity, Hong Kong. His research interests in-\\nclude data science and analytics, machine learning,\\nprocess monitoring, model predictive control, system\\nidentification, smart manufacturing, smart cities, and\\npredictive maintenance. Prof. Qin is a Fellow of the U.S. National Academy\\nof Inventors, IFAC, and AIChE. He was the recipient of the 2022 CAST\\nComputing Award by AIChE, 2022 IEEE CSS Transition to Practice Award,\\nU.S. NSF CAREER Award, and NSF-China Outstanding Young Investigator\\nAward. His h-indices for Web of Science, SCOPUS, and Google Scholar are\\n66, 73, and 84, respectively.\\nXiaohui Tao (Senior Member, IEEE) is currently\\na Full Professor with the University of Southern\\nQueensland, Toowoomba, QLD, Australia. His re-\\nsearch interests include artificial intelligence, data\\nanalytics, machine learning, knowledge engineering,\\ninformation retrieval, and health informatics. His\\nresearch outcomes have been published across more\\nthan 150 papers including many top-tier journals and\\nconferences. He is a Senior Member of ACM and\\nthe Vice Chair of IEEE Technical Committee of\\nIntelligent Informatics. He was the recipient of an\\nARC DP in 2022. He is the Editor-in-Chief of Natural Language Processing\\nJournal.\\nFu Lee Wang (Senior Member, IEEE) received\\nthe B.Eng. degree in computer engineering and the\\nM.Phil. degree in computer science and information\\nsystems from The University of Hong Kong, Hong\\nKong, and the Ph.D. degree in systems engineering\\nand engineering management from The Chinese\\nUniversity of Hong Kong, Hong Kong. Prof. Wang\\nis the Dean of the School of Science and Tech-\\nnology, Hong Kong Metropolitan University, Hong\\nKong. He has over 300 publications in international\\njournals and conferences and led more than 20\\ncompetitive grants with a total greater than HK$20 million. His current\\nresearch interests include educational technology, information retrieval, com-\\nputer graphics, and bioinformatics. Prof. Wang is a fellow of BCS, HKIE and\\nIET and a Senior Member of ACM. He was the Chair of the IEEE Hong\\nKong Section Computer Chapter and ACM Hong Kong Chapter.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = read_doc('documents/')\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks of PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_doc(docs,chunk_size=300, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc =text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='1\\nParameter-Efficient Fine-Tuning Methods for\\nPretrained Language Models: A Critical\\nReview and Assessment\\nLingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee Wang\\nAbstract—With the continuous growth in the number of\\nparameters of transformer-based pretrained language models'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='(PLMs), particularly the emergence of large language models\\n(LLMs) with billions of parameters, many natural language\\nprocessing (NLP) tasks have demonstrated remarkable success.\\nHowever, the enormous size and computational demands of\\nthese models pose significant challenges for adapting them'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='to specific downstream tasks, especially in environments with\\nlimited computational resources. Parameter Efficient Fine-Tuning\\n(PEFT) offers an effective solution by reducing the number\\nof fine-tuning parameters and memory usage while achieving'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='comparable performance to full fine-tuning. The demands for\\nfine-tuning PLMs, especially LLMs, have led to a surge in the\\ndevelopment of PEFT methods, as depicted in Fig. 1. In this\\npaper, we present a comprehensive and systematic review of\\nPEFT methods for PLMs. We summarize these PEFT methods,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='discuss their applications, and outline future directions. Further-\\nmore, we conduct experiments using several representative PEFT\\nmethods to better understand their effectiveness in parameter\\nefficiency and memory efficiency. By offering insights into the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='latest advancements and practical applications, this survey serves\\nas an invaluable resource for researchers and practitioners\\nseeking to navigate the challenges and opportunities presented\\nby PEFT in the context of PLMs.\\nIndex Terms—Parameter-efficient, fine-tuning, pretrained lan-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='guage model, large language model, memory usage.\\nI. I NTRODUCTION\\nT\\nRANSFORMER-BASED PLMs [1], [2], [3], [4] have\\ndemonstrated remarkable performance across a wide\\nrange of NLP tasks. To fully harness the potential of PLMs,\\nfine-tuning is employed to adapt the PLMs to task-specific'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='data to enhance performance on downstream tasks. However,\\ntraditional fine-tuning involves updating all the pretrained\\nparameters of PLMs, which is time-consuming and computa-\\ntionally expensive. As the size of PLMs continues to increase,\\nfrom models like BERT [1] with 110 million parameters to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='T5 [4] with 770 million parameters, computational resource\\nrequirements become a significant challenge. The advent of\\nThis work was supported by a research grant entitled ”Medical Text Feature\\nRepresentations based on Pre-trained Language Models” (871238) and Faculty'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='Research Grant (DB24A4) at Lingnan University, Hong Kong.(Corresponding\\nauthor: Haoran Xie.)\\nLingling Xu and Fu Lee Wang are with the Hong Kong Metropolitan Uni-\\nversity, Hong Kong (email: xxiao199409@gmail.com; pwang@hkmu.edu.hk).'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='Haoran Xie and Si-Zhao Joe Qin are with Lingnan University, Hong Kong\\n(email: hrxie@ln.edu.hk; joeqin@ln.edu.hk).\\nXiaohui Tao is with University of Southern Queensland, Queensland,\\nAustralia (email: xtao@usq.edu.au).\\nLLMs [5], [6], [7], exemplified by Falcon [8] with a stag-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='gering 180 billion parameters, further exacerbates the com-\\nputational demands. To perform task-specific full fine-tuning\\nwith Falcon-180B, a minimum of 5120GB of computational\\nresources may be required 1. The enormous computational\\nresource requirements are prohibitive for anyone but the su-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='perpower players to utilize LLMs for task-specific fine-tuning.\\nTo address this challenge, a prominent method known as\\nPEFT [9] has emerged as a viable solution to compensate\\nfor the tremendous computational cost of full parameter\\nfine-tuning. PEFT involves employing various deep learning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='techniques [9], [10], [11] to reduce the number of trainable\\nparameters while still maintaining comparable performance to\\nthe full fine-tuning. In addition, PEFT updates only a small\\nnumber of additional parameters or updates a subset of the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='pretrained parameters, preserving the knowledge captured by\\nthe PLM while adapting it to the target task and reducing\\nthe risk of catastrophic forgetting. Furthermore, since the size\\nof the fine-tuned dataset is typically much smaller than the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='pretrained dataset, performing full fine-tuning to update all\\nthe pretrained parameters may lead to overfitting, which is\\ncircumvented by the PEFT through selectively or not updating\\npretrained parameters.\\nRecently, there has been a significant surge in interest'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='regarding PEFT methods, as demonstrated by the growing\\nnumber of studies depicted in Fig. 1. This also leads to a\\nfew surveys on PEFT approaches for the PLMs. However,\\nthe existing surveys have certain limitations. Ding et al.\\n[12] conducted a comprehensive study on PEFT methods,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='but this survey did not cover much of the latest work in\\nthe field and only four PEFT methods were quantitatively\\nexperimented with. Lialin et al. [13] delved into the ideas and\\noperational implementations of PEFT methods in detail but\\ndo not perform relevant experiments. In this work, we address'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='these gaps comprehensively. We meticulously categorize the\\nPEFT methods, providing detailed explanations of the ideas\\nand specific implementations of each method. We compare\\nthe similarities and differences among various types of PEFT\\nmethods, facilitating a better understanding of the evolving'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='landscape of PEFT. Moreover, we conduct extensive fine-\\ntuning experiments with 11 representative PEFT methods.\\nIn this paper, we aim to provide a comprehensive and\\nsystematic study of PEFT methods for PLMs in NLP. We\\nundertake an in-depth exploration of these PEFT methods and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 0}, page_content='1https://huggingface.co/blog/falcon-180b#hardware-requirements\\narXiv:2312.12148v1  [cs.CL]  19 Dec 2023'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='2\\nMAM \\nAdapter \\nProPETL \\n2019 \\n2020 \\n2021 \\n2022 \\n2023 \\nAdapter \\nDrop \\nHyperFormer++ \\nResidual \\nAdapter \\n(IA) 3 \\nPrefix- \\ntuning \\nMPT \\nA TTEMPT \\nSPoT \\nLoRA \\nKronA \\nIntrinsic \\nSAID \\nSAM \\nPrompt- \\ntuning W ARP \\nUniPEL T \\nAutoPEFT \\nThreshold \\nMASK \\nBitFit \\nFISH \\nMask \\nChild \\ntuning \\nL T -FST'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='BitFit \\nFISH \\nMask \\nChild \\ntuning \\nL T -FST \\nAdditive Fine-tuning\\nHybrid Fine-tuning\\nSequential \\nAdapter \\nDif f \\nPruning \\nAdapter \\nFusion \\nParallel \\nAdapter \\nKernel-mix-lite(qvo) \\nP-tuning \\nP AST A \\nAttention \\nFusion \\nT iny-Attn \\nAdapter \\nLST \\nCoDA \\nMerA \\nAdapterSoup \\nHardamard \\nAdapter \\nU/S-SAM'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='MerA \\nAdapterSoup \\nHardamard \\nAdapter \\nU/S-SAM \\nSparse \\nAdapter \\nCompacter \\nS 3 Delta-M \\nS 4 \\nAdaMix \\nDyLoRA \\nLOFTQ \\nLoRA-F A \\nLaplace-LoRA \\nMoELoRA \\nLoRAPrune \\nIncreLoRA U/S-BitFit \\nQLoRA \\nQA-LoRA \\nL-LoRA \\nLoRA-Hub \\nAdaLoRA \\nDelta-LoRA \\nPartial Fine-tuning\\nReparameterized Fine-tuning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='Partial Fine-tuning\\nReparameterized Fine-tuning\\nUnified Fine-tuning\\nFig. 1: The evolutionary development of PEFT methods in recent years. Models on the same branch have some common'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='features. The vertical position of the models shows the timeline of their release dates. Notably, the year of the paper’s initial\\npublication is shown as the reference. For instance, if a paper is published in ACL 2022 but listed on arXiv in 2021, the year'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='2021 will be considered as the reference date.\\npresent a comprehensive taxonomy scheme in Section III. By\\ncategorizing PEFT methods into additive fine-tuning, partial\\nfine-tuning, reparameterized fine-tuning, hybrid fine-tuning,\\nand unified fine-tuning, we establish a structured framework'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='for understanding these PEFT approaches, as depicted in\\nFig. 2. In Section IV, we conduct quantitative investigations\\nand analyses to assess the performance, parameters efficiency,\\nand memory usage of these PEFT approaches. Our quantitative\\nstudies primarily focus on natural language understanding'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='(NLU), machine translation (MT), and natural language gen-\\neration (NLG) tasks. Additionally, we extensively explore\\nthe applications of PEFT in multi-task learning, cross-lingual\\ntransfer, and backdoor attack and defense, underscoring its'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='effectiveness. Furthermore, our research also unveils potential\\ndirections for future investigations in this rapidly evolving\\nfield. To summarize, the main contributions of this survey can\\nbe outlined as follows:\\n• We present a comprehensive analysis and review of PEFT'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 1}, page_content='methods for transformer-based PLMs.\\n• We identify the key techniques and approaches employed\\nin PEFT methods, and classify them into additive, partial,\\nreparameterized, hybrid, and unified fine-tuning methods.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='3\\nPEFT Methods for PLMs\\nAdditive\\nFine-tuning\\nAdapter-based\\nFine-tuning\\nSequential Adapter [9], Residual Adapter [14], CoDA [15], Parallel Adapter [16], AdapterDrop [17],\\nTiny-Attn Adapter [18], AdapterFusion [19], MerA [20], Hyperformer++ [21], AdapterSoup [22]\\nSoft Prompt-based'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='Soft Prompt-based\\nFine-tuning W ARP [23], Promt-tuning [24], Prefix-tuning [10], P-tuning [25], SPOT [26], ATTEMPT [27], MPT [28]\\nOthers LST [29], (IA) 3 [30], PASTA [31], AttentionFusion [32], Hadamard Adapter [33]\\nPartial\\nFine-tuning\\nBias Update BitFit [34], U/S-BitFit [35],\\nPretrained Weight'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='Pretrained Weight\\nMasking Threshold-Mask [36], FISH Mask [37]\\nDelta Weight\\nMasking LT-SFT [38], Child-Tuning [39], Diff Pruning [40], SAM [41]\\nReparameterized\\nFine-tuning\\nLow-rank\\nDecomposition Intrinsic SAID [42], LoRA [11], KronA [43]\\nLoRA Derivatives'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='LoRA Derivatives\\nLow-rank Adjustment DyLoRA [44], AdaLoRA [45], IncreLoRA [46]\\nLoRA-guided Pretrained\\nWeight Update Delta-LoRA [47], LoRAPrune [48]\\nQuantization Adaption QLoRA [49], QA-LoRA [50], LOFTQ [51]\\nLoRA-based\\nImprovements Kernel-mix-lite(qv)/(qvo) [52], Laplace-LoRA [53], LoRA-FA [54]'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='LoRA-based\\nMulti-task Fine-tuning LoRAHub [55], MoELoRA [56], L-LoRA [57]\\nHybrid\\nFine-tuning\\nManual Combination MAM Adapter [16], U/S-MAM [35], Compacter [58], UniPELT [59],\\nAutomatic Combination AutoPEFT [60], S 3Delta-M [61], S4 [62]\\nUnified'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='Unified\\nFine-tuning AdaMix [63], SparseAdapter [64], ProPETL [65]\\nFig. 2: Taxonomy of Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models.\\n• We conduct extensive experiments to evaluate the effec-\\ntiveness of several representative PEFT methods, specifi-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='cally examining their impact on parameter efficiency and\\nmemory usage.\\nII. P RELIMINARIES\\nA. Transformer\\nTransformer [66] has emerged as a foundational architecture\\nfor numerous PLMs, it adopts an encoder-decoder architecture,\\ncomprised of a stack of encoder and decoder layers, each'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='equipped with the self-attention mechanism. Both the encoder\\nand decoder in the Transformer architecture consist of a multi-\\nhead self-attention layer and a feed-forward network (FFN)\\nlayer, interconnected by a residual connection [67] followed'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='by layer normalization [68]. Residual connection allows the\\nmodel to effectively propagate information from one layer\\nto the subsequent layer without losing valuable information.\\nLayer normalization further stabilizes the training process by\\nnormalizing the inputs of each layer.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='normalizing the inputs of each layer.\\nMulti-head self-attention layer employs the self-attention\\nfunction with h heads in parallel. For an input sequence\\nX ∈ Rn×d with the sentence length n and hidden dimension\\nsize of d. The query ( Q), key ( K), and value ( V) vectors are'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='the transformation of input sequence X,\\nK = XWk + bk, Q = XWq + bq, V = XWv + bv, (1)\\nwhere Q, K, V∈ Rn×d, bk, bq and bv are typically learnable\\nparameter vectors that help model to better capture specific\\ninformation in the input vector X and adjust the value of the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='query vector Q to better match the key vector K, thereby\\nimproving performance of the self-attention mechanism. The\\nself-attention output of input X is computed as:\\nAttn(Q, K, V) = Softmax(QKT\\n√\\nd\\n)V, (2)\\nthen multi-head self-attention can be described as follows:'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='MHA(Q, K, V) = Concat(head1, ··· , headh)WO, (3)\\nheadi = Attn(QWi\\nQ, KWi\\nK, V Wi\\nV ). (4)\\nWhile the FFN consists of two linear transformations with\\na non-linear ReLU activation function in between:\\nFFN(X) = ReLU(XW1 + b1)W2 + b2, (5)\\nwhere W1, b1, W2 and b2 are the weight matrices of two'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 2}, page_content='linear transformations. Most PEFT methods primarily focus on\\nthe self-attention layer and FFN layer, allowing models like\\nencoder-based RoBERTa [2], encoder-decoder-based T5 [4],\\nand decoder-based LLaMA [7] to leverage relevant techniques\\nfor parameters reduction.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='4\\nB. Full Fine-tuning of PLMs\\nFull fine-tuning of transformer-based PLMs involves train-\\ning the entire model, including all layers and parameters, on\\na specific downstream task using task-specific data. Initially,\\nPLMs are trained on large-scale datasets with unsupervised'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='learning objectives like language modeling or masked lan-\\nguage modeling, to learn general language representations [1],\\n[2], [4], [7]. However, these PLMs may not perform optimally\\nwhen applied to specific tasks like sentiment analysis, question'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='answering, or translation due to a lack of appropriate domain\\nknowledge [69], [70], [71]. Full fine-tuning provides an effec-\\ntive solution to address this limitation.\\nDuring full fine-tuning, the PLM is initialized with pre-\\ntrained weights and subsequently trained on task-specific data'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='using techniques like backpropagation and gradient descent\\n[72], [73]. All model parameters, including pretrained weights,\\nare updated to minimize a task-specific loss that quantifies\\nthe disparity between predicted outputs and ground truth. In'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='this way, full fine-tuning enables the model to learn task-\\nspecific patterns and nuances from the labeled data, facili-\\ntating predictions or outputs tailored to the target tasks [74].\\nNotably, full fine-tuning necessitates substantial computational'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='resources and labeled data, as the model is trained from scratch\\nfor the specific target task. Moreover, as PLMs grow in size\\nand with the advent of LLMs containing billions of parameters,\\nfull fine-tuning places even greater demands on computational'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='resources. In contrast, PEFT methods aim to alleviate these re-\\nquirements by selectively updating or modifying specific parts\\nof the PLMs while still achieving performance comparable to\\nfull fine-tuning [34], [39]. Furthermore, full fine-tuning may'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='give rise to overfitting when the task-specific dataset is small\\nor when the PLMs are already well-suited to the target task\\n[19], [75].\\nIII. P ARAMETER -EFFICIENT FINE -TUNING METHODS\\nA. Additive Fine-tuning\\nAdditive fine-tuning approaches involve introducing new'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='extra trainable parameters for task-specific fine-tuning. We\\nclassify additive fine-tuning into three groups: Adapter-based\\nFine-tuning [9], [14], [15], [16], [17], [18], [19], [20], [21],\\n[22], [76], in which the adapter module is incorporated into'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='the transformer, allowing for fine-tuning without modifying\\nthe pretrained parameters, Soft Prompt-based Fine-tuning\\n[10], [23], [24], [25], [26], [27], [28], where soft prompts or\\nprefix vectors are appended to the input embeddings or hidden'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='states during fine-tuning, and Others [29], [30], [31], [32],\\n[33], in which various methods that introduce supplementary\\nparameters for model fine-tuning are fall into this category.\\n1) Adapters-based Fine-tuning: The idea of Adapter is\\nfirst introduced in multi-domain image classification [77],'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='allowing for the efficient transfer of knowledge across multiple\\nvisual domains. Sequential Adapter [9] extends and applies\\nit to NLP tasks by inserting the adapter (trainable modules)\\ninto the transformer block and fine-tuning the parameters\\nof adapters to make the PLMs adapt to the downstream'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='tasks. Specifically, adapter networks are inserted after the\\nself-attention layer and feed-forward layer of the Transformer\\nsequentially. Each adapter are low-rank module that consists\\nof a down-projection, a non-linear activation function, and an'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='up-projection as well as a residual connection. For the inputX,\\nthe output of a sequential adapter with the ReLU non-linear\\nactivation function can be defined with Equation 6. During\\nfine-tuning, only the parameters of adapter network Wup and\\nWdown need to be updated to make the PLMs adapt to the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='specific downstream tasks. The specific architecture of the\\nsequential adapter is presented in Fig. 3.\\nX = (ReLU(XWdown))Wup + X, (6)\\nWdown ∈ Rd×k, Wup ∈ Rk×d.\\nInspired by sequential adapter, many adapter-based PEFT\\nmethods have been proposed. Residual Adapter [14] fur-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='ther improves efficiency by inserting the adapter module\\nonly after the feed-forward and layer normalization. Parallel\\nAdapter [16], [76] inserts the adapter network in parallel\\nwith both the attention layer and the feed-forward layer,\\nallowing for more efficient integration of the adapter module'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='into the transformer. AdapterDrop [17] removes adapters\\nin each layer of the transformer that are not important to\\nthe given task to improve inference efficiency. While CoDA\\n(Condition Adapter) [15] employs parallel adapter for task-\\nspecific parameter fine-tuning and remains most pretrained'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='parameters fixed. However, unlike prior methods in which all\\ninput tokens are processed with pretrained transformer, CoDA\\nutilizes a router function to select k important input tokens\\nfor conditional computation. In this way, CoDA not only'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='enhances parameter efficiency but also inference efficiency.\\nTiny-Attn Adapter (Tiny-Attention Adapter) [18] introduces\\na dot-product attention module between the down- and up-\\nprojections, which can also be seen as a multi-head attention'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='module with its per-head dimensionality to be extremely small.\\nMoreover, the Tiny-Attn Adapter regards its multiple attention\\nheads as a mixture of experts and averages their weights to\\nfurther reduce inference costs. Akin to the sequential adapter,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='the Tiny-Attn Adapter is also injected right after the multi-\\nhead attention layer.\\nAdapterFusion [19] integrates multiple task-specific\\nadapters into a single adapter module, allowing for effective\\nknowledge transfer across related tasks without modifying'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='the original pretrained model. AdapterFusion provides a prac-\\ntical and efficient approach to task composition, enabling\\nthe transferability of pretrained models across multiple tasks\\nwhile minimizing the computational costs associated with fine-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='tuning the entire model. However, AdapterFusion requires\\nadditional trainable parameters in the composition layers,\\nincreasing computational costs. MerA (Merging Pretrained\\nAdapters) [20] adopts summation and averaging strategies to\\nmerge the parameters of pretrained adapters without introduc-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='ing extra trainable parameters. It employs the optimal transport\\nmethod [78], [79] to align the parameters of adapters based\\non weights and activations, which gives better performance\\nwith fewer trainable parameters compared to AdapterFusion.\\nHyperformer++ [21] utilizes the shared hypernetwork [80]'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 3}, page_content='to learn task-specific and layer-specific adapter parameters\\nthat condition on task and layer id embeddings. By sharing'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='5\\nMulti-Head Attention\\nLayer Normalization\\nLayer Normalization\\nFeed-Forward Network\\n+\\n+\\nAdapter Network\\nAdapter Network\\nNonlinear \\nActivation\\n+\\nDown-projection\\nUp-projection\\nkd\\nk d\\n(a) Sequential Adapter\\nMulti-Head Attention\\nLayer Normalization\\nLayer Normalization\\nFeed-Forward Network\\n+ \\n+'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='Layer Normalization\\nFeed-Forward Network\\n+ \\n+ \\nHidden States\\nW q W k W v \\nQ K V P k P v \\nAttention\\nPr efix-tuning (b) Prefix-tuning\\nMulti-Head Attention\\nLayer Normalization\\nLayer Normalization\\nFeed-Forward Network\\n+ \\n+ \\nHidden States\\nW q W k W v \\nQ K V \\nAttention LoRA \\nLoRA \\nLoRA \\n+ + + \\nDown-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='Q K V \\nAttention LoRA \\nLoRA \\nLoRA \\n+ + + \\nDown- \\nprojection \\nUp- \\nprojection * \\nLoRA (c) LoRA\\nFig. 3: The detailed architecture of (a) Sequential Adapter, (b) Prefix-tuning, and (c) LoRA.\\nknowledge across tasks via hypernetworks while enabling the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='model to adapt to each task through task-specific adapters,\\nsignificantly reducing the number of trainable parameters.\\nAdapterSoup [22] is developed to address cross-domain task\\nadaptation, which first trains multiple adapters based on var-\\nious domains and then employs domain clustering [81] to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='select the most appropriate top-k adapters for the new domain.\\nFine-tuning parameters for the new domain in AdapterSoup\\nare determined by calculating the weighted average of the\\nselected k adapters. Apart from cross-domain task adaptation,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='AdapterSoup can also be used to strengthen in-domain results\\nvia weight averaging of adapters trained on the same domain\\nbut with different hyperparameters.\\n2) Soft Prompt-based Fine-tuning: Soft prompt fine-tuning\\nis a class of methods in which trainable continuous vectors,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='known as soft prompts, are inserted into the input or hidden\\nstate of the model. Unlike manually designed hard prompts,\\nsoft prompts are generated by searching for prompts in a\\ndiscrete token space based on task-specific training data. Soft'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='prompts exhibit more flexibility and adaptability during fine-\\ntuning, as these prompts can be optimized and adjusted based\\non the specific task and training data.\\nW ARP (Word-level Adversarial RePrograming) [23] in-\\nserts special prompt tokens [P1], [P2], ··· , [Pl] and [Mask]'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='token before or after the sentences relying on the prompt\\ntemplate. The training objective is to minimize the cross-\\nentropy loss between the output of MLM and the verbalizer\\ntokens [V1], [V2], ··· , [Vc] for classes {1, 2, ··· , c}. Only the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='parameters of [P1], [P2], ··· , [Pl] and [V1], [V2], ··· , [Vc] are\\ntrainable, resulting in a significant reduction in the number\\nof fine-tuning parameters. Prompt-tuning [24] incorporates\\nadditional l learnable prompt tokens, P = [P1], [P2], ··· , [Pl],'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='into the model input X ∈ Rn×d and then concatenates them to\\ngenerate the final input ˆX, the new input can be expressed with\\nEquation 7. During fine-tuning, only the prompt parameters\\nof P are updated through gradient descent, while pretrained'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='parameters remain frozen. Thus, the parameter cost of prompt-\\ntuning is determined by multiplying the prompt length by\\nthe token embedding dimension, and extending the prompt\\nlength beyond a single token is critical for achieving good\\nperformance.\\nˆX = Concat(P, X) = [P, X] ∈ R(l+n)×d. (7)'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='ˆX = Concat(P, X) = [P, X] ∈ R(l+n)×d. (7)\\nPrefix-tuning [10] proposes to prepend soft prompts P =\\n[P1], [P2], ··· , [Pl] (l denotes the length of the prefix) to\\nthe hidden states of the multi-head attention layer, differing\\nfrom prompt-tuning that adds soft prompts to the input. To'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='ensure stable training, a FFN is introduced to parameterize\\nthe soft prompts, as direct optimization of the soft prompts\\ncan lead to instability. Two sets of prefix vectors ˆPk and\\nˆPv are concatenated to the original key ( K) and value\\n(V ) vectors of the attention layer. The self-attention mech-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='anism with prefix-tuning can be represented by Equation 8.\\nDuring training, only ˆPk, ˆPv, and the parameters of FFN\\nare optimized, while all other parameters of PLMs remain\\nfrozen. The structure of prefix-tuning is illustrated in Fig. 3.\\nAfter training, the FFN is discarded, and only Pk and Pv'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='are used for inference. P-tuning [25] also considers insert-\\ning the soft prompts [P1], ··· , [Pi], [Pi+1], ··· , [Pl] into the\\nmodel input. Nonetheless, P-tuning differs by concatenating\\nthese prompts to form a template and maps it to obtain'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='{h1, ··· , hi, e(x), hi+1, ··· , hl, e(x)}, in which e represents\\npretrained embedding layer. The training goal is to optimize\\nthe continuous prompts {h1, ··· , hl}. As the weights of PLMs\\nare fixed and only a few parameters need to be fine-tuned,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='the template can be effectively learned in few-shot learning\\nscenarios. P-tuning employs a bidirectional long short-term\\nmemory network (LSTM) with a ReLU-activated multilayer\\nperceptron (MLP) to initialize the embedding of soft prompts\\nthrough MLP(LSTM(h1, ··· , hi): LSTM(hi, ··· , hl)).'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='head = Attn(XWq, [ ˆPk, XWk], [ ˆPv, XWv]), (8)\\nˆPk = FFN(Pk), ˆPv = FFN(Pv). (9)\\nSPOT (Soft Prompt Transfer) [26] is a multitask prompt\\nmethod that builds upon the prompt-tuning, in which “prompt\\npertaining” is introduced between PLMs and prompt-tuning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 4}, page_content='of target tasks. There are two variants of SPOT: generic'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='6\\nSPOT and targeted SPOT. Generic SPOT first learns a generic\\nprompt on one or more source tasks and then employs the\\nlearned prompt to initialize target prompts for specific target\\ntasks. Targeted SPOT learns separate prompts for various\\nsource tasks, creating a source prompt library. Subsequently,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='the optimal source prompt, which exhibits higher similarity to\\nthe target task embedding, is retrieved and used to initialize\\nthe target prompt for the target task. ATTEMPT (ATTEntional\\nMixtures of Prompt Tuning) [27] begins by pretraining trans-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='ferable soft prompts (source prompts) on large-scale source\\ntasks that possess valuable knowledge applicable to other\\ntasks. The new target prompt is initialized specifically for a\\ngiven target task. ATTEMPT employs a shared and lightweight'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='network that is trained simultaneously to learn an attention-\\nweighted combination of source prompts and target prompt.\\nThis enables modular multi-task learning, as pretrained soft\\nprompts can be flexibly combined, reused, or removed to\\nleverage knowledge from different tasks. MPT (multitask'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='prompt tuning) [28] utilizes multitask data to decompose and\\ndistill knowledge from the source prompts to learn a single\\nshared prompt. MPT then learns a multiplicative low-rank\\nmatrix to update the shared prompt, efficiently adapting it to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='each downstream target task. Specifically, MPT assumes that\\nthe soft prompts for the k-th source task, denoted as ˆPk, can be\\ndecomposed into the shared prompts across all source tasks P∗\\nand a task-specific low-rank matrix Wk. The decomposition is\\ngiven by ˆPk = P∗ ⊙Wk = P∗ ⊙(uk ⊗vT'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='given by ˆPk = P∗ ⊙Wk = P∗ ⊙(uk ⊗vT\\nk ), where ⊙ denotes\\nthe Hadamard product, ⊗ denotes the Kronecker product, and\\nuk and vk are task-specific vectors for the task k.\\n3) Others: Apart from adapters family and soft prompts\\nfine-tuning methods, there are some other approaches that also'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='incorporate extra trainable parameters during fine-tuning. They\\ninvolve adding a ladder side network operating alongside the\\ntransformer, introducing an additional diff vector to rescale\\nthe attention, incorporating an extra vector to the special token'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='representations, using the late fusion technique to integrate ad-\\nditional attention weight, or combing an extra joint importance\\nweight for each token representation.\\nLST (Ladder Side-Tuning) [29] trains a ladder side network\\nin conjunction with the pretrained network and takes interme-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='diate activations as input via shortcut connections, known as\\nladders, from pretrained network. Since all training parameters\\nare stored in the ladder side network, back-propagation is\\nachieved through side networks and ladder connections rather'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='than pretrained networks, reducing the number of fine-tuned\\nparameters. In addition, LST further boosts parameter effi-\\nciency by utilizing structural pruning [82] to retrieve a smaller\\npruned network to initialize the side network and dropping'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='certain layers of the side network. (IA)3 (Infused Adapter\\nby Inhibiting and Amplifying Inner Activations) [30] lever-\\nages learned vectors to scale activations, leading to improved\\nperformance while introducing a relatively small number of'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='new parameters. (IA)3 introduces three learned vectors, lk, lv,\\nand lff , to rescale the key (K) and value (V) vectors in the\\nattention networks and hidden activations in the position-wise\\nFFN. As a result, the attention output and hidden activation'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='output can be rescaled using the following expressions:\\nAttn(Q, K, V) = (Q(lk ⊙ KT )√dk\\n)(lv ⊙ V ), (10)\\nFFN(X) = (lff ⊙ γ(XW1))W2, (11)\\nin which ⊙ represents element-wise multiplication, W1 and\\nW2 are the weight matrices of FFN, and γ is activation'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='function. (IA)3 only optimizes three learned vectors lk, lv, and\\nlff for each transformer block, resulting in great parameter\\nefficiency. Notably, (IA)3 incurs minimal overhead because lk\\nand lv can be seamlessly integrated into the corresponding'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='linear layers, with the only additional overhead arising from\\nlff . PASTA (PArameter-efficient tuning with Special Token\\nAdaptation) [31] improves parameter efficiency by modifying\\nthe special token representations (e.g., [SEP] and [CLS] in'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='BERT) with an extra special trainable vector before the self-\\nattention layer at each transformer layer. Assuming that the\\ninputs to the transformer layer are denoted as H = {hi}N\\ni=1,\\nPASTA modifies the inputs as follows:\\nHmod = {hi + mi}N\\ni=1, (12)'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='Hmod = {hi + mi}N\\ni=1, (12)\\nmi = e(vp), i is the p-th special token ; otherwise, mi = 0,\\nmi is the special token adaptation. PASTA enables a re-\\nmarkable reduction in trainable parameters by training only\\nthe trainable vector e(vp) to update the representations of'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='special tokens. The reasons for using [CLS] and [SEP] as\\nspecial tokens in PASTA are that the [CLS] representation\\nprovides a global representation of the input text and that\\nthe attention scores in PLMs are primarily allocated to the\\n[CLS] or [SEP] tokens across attention heads [83], [84].'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='AttentionFusion [32] introduces the late fusion technique,\\nwhich involves combining features or representations from\\ndiverse tasks or layers to generate a final joint representation,\\nto adjust the the importance of each token representation. For'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='a given task t, let the attention query vector be denoted by Qt\\nand the representation of token i at layer j be V j\\ni , then the\\nrepresentation of token i for task t, ˆV j\\ni , is expressed as:\\nˆV j\\ni =\\nX\\nj\\nαj\\ni (t)V j\\ni , α j\\ni (t) = exp(QtV j\\ni )P\\nk exp(QtV j\\ni )\\n, (13)\\nwhere αj'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='i )P\\nk exp(QtV j\\ni )\\n, (13)\\nwhere αj\\ni (t) represents the attention weight of token i at layer\\nj for task t. The number of extra parameters that need to be\\nupdated in AttentionFusion is determined by the size of the\\nquery vector Qt, which is the same as the hidden dimension of'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='the pretrained encoder. By employing the attention weight as\\nextra trainable parameters, AttentionFusion adjusts the impor-\\ntance of each token representation dynamically. Hadamard\\nAdapter [33] is an additive fine-tuning method that intro-\\nduces a weight vector and a bias vector in each transformer'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='with the same dimensions as the output of the multi-head\\nattention module for fine-tuning. A weight vector and a bias\\nvector are injected right after the multi-head attention layer\\nto perform element-wise multiplication (Hadamard product)\\nwith the multi-head attention outputs. Notably, the number'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 5}, page_content='of Hadamard adapter is the same as that of the transformer\\nlayers in the PLM. During fine-tuning, only the parameters in'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='7\\nTABLE I: The weight update in pretrained weight masking and delta weight masking. ⊙ denotes the Hadamard product.\\nMethod Weight Update Mask Criterion Mask Matrix\\nThreshold-Mask ˆW = M ⊙ W Threshold M = Isi,j>τ\\nFISH Mask ˆW = M ⊙ W Fisher information M = Itop−k(fi,j)'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='LT-SFT ˆW = W + M ⊙ ∇W L(W) Absolute difference of parameters M = Itop−k(|W1−W0|)\\nChild-TuningF ˆW = W − M ⊙ η∇W L(W) Bernoulli distribution M = {0, 1}n\\nChild-TuningD ˆW = W − M ⊙ η∇W L(W) Fisher information M = Itop−k(fi,j)\\nDiff Pruning ˆW = W + M ⊙ ∆W Fixed sparsity M = {0, 1}n'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='SAM ˆW = W + M∆W Analytical solution Mi,j = 0, ∀i ̸= j; Mi,i ∈ {0, 1}\\nthe Hadamard adapter, layer normalization, and classifier are\\nupdated.\\nB. Partial Fine-tuning\\nPartial fine-tuning methods aim to reduce the number of\\nfine-tuned parameters by selecting a subset of pre-trained pa-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='rameters that are critical to downstream tasks while discarding\\nunimportant ones. We categorize partial fine-tuning methods\\ninto three groups: Bias Update [34], [35], in which only\\nthe bias term in the attention layer, feed-forward layer and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='layer normalization of the transformer is updated, Pretrained\\nWeight Masking [36], [37], where the pretrained weights are\\nmasked using various pruning criterion, and Delta Weight\\nMasking [38], [39], [40], [41], in which delta weights are\\nmasked via pruning techniques and optimization approxima-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='tion. A detailed analysis of pretrained weight and delta weight\\nmasking is provided in Table I.\\n1) Bias Update: Bit-Fit (Bias-term Fine-tuning) [34]\\nachieves parameter efficiency by only updating the bias terms\\nand the task-specific classification layer while keeping the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='majority of parameters in the transformer-based PLMs frozen.\\nThe bias parameters are involved in the attention layer, where\\nthey are involved in calculating query, key, value, and combin-\\ning multiple attention heads, as well as in the fee-forward and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='layer normalization layers. Further, U/S-BitFit [35] combines\\nNAS algorithm [85] and pruning technique to automatically\\ndetermine which parameters of the network need to be fine-\\ntuned based on BitFit. U-BitFit (Unstructured BitFit) decides\\nwhich PEFT parameters to prune based on the first-order'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='approximation of the change in training loss resulting from\\npruning the PEFT parameter W, i.e., −W · ∇W L(W). While\\nS-BitFit (Structured BitFit) sums the criterion over the overall\\nbias update ∆b (b is the bias term).\\n2) Pretrained Weight Masking: Pretrained weight masking'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='employs pruning criteria like threshold and Fisher information\\nto measure the importance of pretrained weight to construct\\na binary mask matrix for weight masking. Threshold-Mask\\n[36] utilizes the threshold to construct a binary mask ma-\\ntrix to select pretrained weights W of the attention and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='FFN layers through element-wise multiplication, expressed as\\nˆW = W ⊙ M (⊙ denotes the Hadamard product). To begin,\\na random uniformly distributed real-valued matrix S, which\\nshares the same dimensions as matrices W and M, is created.\\nSubsequently, if an element in S surpasses a predetermined'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='global threshold τ, the corresponding position in the binary\\nmask matrix is assigned a value of 1; otherwise, it is assigned\\n0. FISH Mask (Fisher-Induced Sparse uncHanging) [37] uses\\nthe Fisher information of pretrained weight to measure their'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='importance and construct a sparse binary mask. FISH Mask\\nselects the top-k parameters with the largest Fisher information\\nto construct the sparse binary mask, where the positions\\ncorresponding to the top- k parameters are set to be 1 and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='the rest are set to 0. Note that k is preset based on the desired\\nmask sparsity level of the mask, and the resulting sparse binary\\nmask can be reused across many subsequent iterations.\\n3) Delta Weight Masking: Delta weight masking also em-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='ploys various pruning techniques and criteria to construct a\\nbinary mask matrix to reduce trainable parameters. However,\\nDelta weight pruning typically involves an update at each\\niteration. LT-SFT (Lottery Ticket Sparse Fine-Tuning) [38]\\nis a novel PEFT method inspired by the Lottery Ticket'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='Hypothesis2 [86]. LT-SFT first fine-tunes the PLM on target\\ndata using pretrained parameters W0 to obtain the fully fine-\\ntuned parameters W1, and then identifies the top- k pretrained\\nparameters with the greatest absolute differences ( |W1 −W0|).'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='The top-k parameters are selected for further fine-tuning using\\nbinary mask M, in which the positions corresponding to the\\nselected k parameters are set to 1 and the remaining positions\\nto 0. LT-SFT then resets the model parameters to their original'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='pretrained weights W0 but fine-tunes only the selected k\\nparameters while keeping the remaining parameters frozen,\\nand can be expressed as δ = M ⊙∆W(∆W = ∇W L(W). By\\niteratively repeating this process, the method gradually fine-\\ntunes only a small fraction of the model’s parameters. Child-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='Tuning [39] calls the network formed by the parameters to be\\nupdated a child network and masks out the gradients of non-\\nchild networks to improve parameter efficiency. The parameter\\nupdates in Child-Tuning is expressed as δ = M ⊙ ∆W\\n(∆W = η∇W L(W), η denotes learning rate). Child-Tuning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='provides two variants: Child-Tuning F (F stands for Task-\\nFree) and Child-Tuning D (D stands for Task-Driven). Child-\\nTuningF generates the binary mask matrix M using Bernoulli\\ndistribution with a probability denoted as p F . Increasing the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='value of pF updates a larger number of parameters, and Child-\\nTuningF is equivalent to full fine-tuning when p F = 1 . In\\ncontrast, Child-TuningD uses Fisher information estimation to\\nidentify a subset of parameters (i.e., child network) that are'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 6}, page_content='highly correlated with a specific downstream task. The binary\\n2Lottery Ticket Hypothesis states that each neural model contains a sub-\\nnetwork (a “winning ticket”) that can match or even outperform the perfor-\\nmance of the original model when trained in isolation.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='8\\nmask matrix in Child-Tuning D is constructed by setting the\\nposition of the child network to be 1 and the non-child network\\nto be 0.\\nDiff Pruning [40] introduces a sparse task-specific “diff”\\nvector δ during fine-tuning while remaining the pretrained'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='model parameters fixed. To make the diff vector δ sparse,\\nDiff Pruning introduces a learnable binary mask M on the\\nDelta weight and decomposes δ = M ⊙ ∆W. The binary\\nmask M is learnable and is used as a regularizer during fine-\\ntuning. It acts as a differentiable approximation to the L0-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='norm of diff vector δ. This approach is well-suited for multi-\\ntask deployment in edge (mobile) applications with limited\\nstorage. Significantly, Diff pruning incurs higher memory\\nconsumption compared to traditional fine-tuning, which may\\nbecome problematic as model sizes continue to grow. SAM'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='(Second-order Approximation Method) [41] also employs the\\nsparse mask matrix to update the delta weight. However, SAM\\ndirectly optimizes the approximation function to obtain an\\nanalytical solution for the mask matrix, which is then used to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='update the pretrained weight. Concretely, SAM [41] views the\\nPEFT methods as p-sparse fine-tuned model by representing\\nfine-tuned parameter as W = W0 + M∆W, M is a mask\\nmatrix, and the optimization problem is\\nmin∆W,ML(W0 + M∆W), s.t.\\n∥M∥0 = ⌊mp⌋, Mi,j = 0, ∀i ̸= j; and Mi,i ∈ {0, 1}.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='SAM approximates the loss function using its second-order\\nTaylor expansion as:\\nL(W0 + M∆W) ≈L(W0) + ∆L(W0)T ∆L(W0)T M∆W\\n+ 1\\n2(M∆W)T HM ∆W, (14)\\nin which H is the Hessian matrix. In practice, SAM first\\nobtains the gradient ∇L(W0)i for the i-th parameter Wi, then\\ncalculates\\n\\x0c\\x0c∇L(W0)2\\ni'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='calculates\\n\\x0c\\x0c∇L(W0)2\\ni\\n\\x0c\\x0c, and selects the top ⌊mp⌋ delta weight\\nfor optimization.\\nC. Reparameterized Fine-tuning\\nReparameterized fine-tuning methods utilize low-rank trans-\\nformation to reduce the number of trainable parameters while\\nallowing operating with high-dimensional matrices (e.g., pre-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='trained weights). We categorize reparameterized fine-tuning\\nmethods into two groups: Low-rank Decomposition [11],\\n[42], [43], in which various low-rank decomposition tech-\\nniques are used to reparameterize the updated matrix, and\\nLoRA derivatives [44], [45], [46], [47], [48], [49], [50], [51],'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='[52], [53], [54], [55], [56], [57], where a series of PEFT\\nmethods are developed based on LoRA. Specific details of\\n∆W parameters reparameterization of various approaches can\\nbe seen in Table II.\\n1) Low-rank Decomposition: This involves finding a lower-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='rank matrix that captures the essential information of the\\noriginal matrix while reducing computational complexity and\\nmemory usage by reparameterizing the updated delta weight.\\nReparameterization covers transforming the delta weight ma-\\ntrix into a low-rank representation using methods such as'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='Fastfood transformation, low-rank down-up projection, or Kro-\\nnecker product projection.\\nIntrinsic SAID (Structure-Aware Intrinsic Dimension) [42]\\nleverages the concept of intrinsic dimensionality to reduce\\nthe number of parameters during fine-tuning. The intrinsic'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='dimensionality refers to the minimum dimensionality required\\nto solve a high-dimensional optimization problem. In the\\ncontext of PLMs, measuring the intrinsic dimensionality helps\\nestimate the minimum number of parameters needed to adapt\\nto new tasks. Instead of optimizing the empirical loss in'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='the original parameterization, Intrinsic SAID fine-tunes the\\nmodel by reparametrization the model in a lower-dimensional\\nspace, i.e., ∆W = F(Wr), in which Wr is the parameter\\nto be optimized and F : Rr → Rd is a Fastfood transform 3\\n[87] that projects parameters from low-dimensional r to high-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='dimensional d. However, Intrinsic SAID is not practical for\\nfine-tuning larger networks due to the O(d) memory com-\\nplexity of the Fastfood transform and the need to update all\\nof the model’s parameters.\\nInspired by Intrinsic SAID, LoRA (Low-Rank Adaptation)'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='[11] introduces two trainable low-rank matrices for weight\\nupdate. In LoRA, a down-projection matrix and an up-\\nprojection matrix are utilized in parallel with the query (Q),\\nkey (K), and value (V) matrices in the attention layer of the\\ntransformer, shown in Fig. 3. For a pretrained weight matrix'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='W ∈ Rd×k, LoRA updates W using low-rank decomposition\\n∆W = WdownWup. During training, the weights of PLM\\nare frozen, and only the low-rank matrices of LoRA, i.e.,\\nWdown ∈ Rd×r and Wup ∈ Rr×k are fine-tuned (r ≪ {d, k}).\\nDuring inference, the LoRA weights are merged with the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='original weight matrix of the PLMs without increasing the\\ninference time. Practically, a scaling factor ( s = 1/r) is added\\nto the LoRA module. KronA (Kronecker Adapter) [43] is\\nstructurally similar to LoRA but replaces the low-rank de-\\ncomposition in LoRA with Kronecker product decomposition,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='∆W = Wdown ⊗ Wup. Kronecker product decomposition\\nmaintains the rank of the input matrix (i.e., rank (A ⊗ B) =\\nrank(A) × rank(B)), ensuring that important information is\\npreserved during the adaptation process. Moreover, Kronecker\\nproduct can speed up computation and reduce the number'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='of required floating-point operations (FLOPS) by avoiding\\nthe explicit reconstruction of the Kronecker product matrix.\\nKronA has two variants: KronA B and KronA B\\nres. KronA B\\ninserts the KronA module in parallel to the FFN layer, while\\nKronAB\\nres inserts the KronA module alongside the FFN layer'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='and incorporates a learnable residual connection.\\n2) LoRA Derivatives: LoRA derivatives refer to a series\\nof PEFT methods that are improved based on LoRA, includ-\\ning Low-Rank Adjustment [44], [45], [46], where different\\nmethods are developed to adjust the rank of LoRA dynami-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='cally, LoRA-guided Pretrained Weight Update [47], [48],\\nin which LoRA is used to guide the update of pretrained\\nweight, Quantization Adaption [49], [50], [51], in which\\nvarious quantization techniques are proposed to improve the\\nhigh precision fine-tuning and inference of LoRA, LoRA-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 7}, page_content='3Fastfood transform is a computationally efficient dimensionality expansion\\nmethod.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='9\\nTABLE II: Delta weight reparameterization of various reparameterized fine-tuning methods.\\nMethod ∆W Reparameterization Notes\\nIntrinsic SAID ∆W = F(Wr) F : Rr → Rd, Wr ∈ Rr is parameters to be optimized, and r ≪ d.\\nLoRA ∆W = WdowmWup Wdown ∈ Rk×r, Wup ∈ Rr×d, and r ≪ {k, d}.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='KronA ∆W = Wdown ⊗ Wup rank(Wdown ⊗ Wup) = rank(Wdown) × rank(Wup).\\nDyLoRA ∆W = Wdown↓bWup↓b Wdown↓b = Wdown[: b, :], Wup↓b = Wup[:, : b], b ∈ {rmin, ··· , rmax}.\\nAdaLoRA ∆W = PΛQ PP T = PT P = I = QQT = QT Q, Λ = diag(σ1, σ2, . . . , σr).'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='IncreLoRA ∆W = WdownΛWup Λ = [λ1, λ2, ··· , λr] with λi could be an arbitrary constant.\\nDeltaLoRA ∆W = WdownWup W(t+1) ← W(t) + (W(t+1)\\ndown W(t+1)\\nup − W(t)\\ndownW(t)\\nup ).\\nLoRAPrune ∆W = WdownWup ⊙ M δ = (W + WdownWup) ⊙ M, M ∈ {0, 1}1×G, G is group number.\\nQLoRA ∆W = WBF 16\\ndown WBF 16'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='QLoRA ∆W = WBF 16\\ndown WBF 16\\nup Y BF 16 = XBF 16doubleDequant(cFP 32\\n1 , cFP 8\\n2 , WNF 4) + XBF 16WBF 16\\ndown WBF 16\\ndown .\\nQA-LoRA ∆W = WdownWup Wdown ∈ Rk×r, Wup ∈ Rr×L, L is the quantization group number of W.\\nLOFTQ ∆W = SVD(W − Qt) Qt = qN (W − Wt−1\\ndownWt−1'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='LOFTQ ∆W = SVD(W − Qt) Qt = qN (W − Wt−1\\ndownWt−1\\nup ), qN is N-bit quantization function.\\nKernel-mix ∆Wh = (BLoRA, Bh)\\n \\nAh\\nLoRA\\nAh\\n!\\nBLoRA is shared across all heads, Bh, Ah provide rank-r update in each head.\\nLoRA-FA ∆W = WdownWup = QRWup Wdown is frozen, and only update Wup.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='based Improvements [52], [53], [54], in which several novel\\ntechnique are incorporated into LoRA for improvements, and\\nLoRA-based Multi-task Fine-tuning [55], [56], [57], where\\nmultiple LoRA modules are combined for cross-task transfer\\nto fine-tune model on a novel task.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='to fine-tune model on a novel task.\\nLow-rank Adjustment. DyLoRA (Dynamic LoRA) [44] is\\nintroduced to overcome two limitations of LoRA: (a) LoRA’s\\nrank is fixed and prevents any changes after training (b)\\ndetermining the optimal rank for LoRA requires exhaustive'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='search and considerable effort. DyLoRA trains LoRA modules\\nfor a range of ranks instead of a single rank, allowing\\nfor adaptability. DyLoRA addresses these limitations during\\ntraining by sorting the representations learned at various ranks.\\nSpecifically, DyLoRA operates within the range of ranks'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='denoted as r ∈ [rmin, rmax] for a series of iterations. In\\neach iteration, DyLoRA randomly selects a specific rank b\\nfrom {rmin, ··· , rmax}. It then truncates the down-projection\\nmatrix as Wdown↓b = Wdown[: b, :] and the up-projection\\nmatrix as Wup↓b = Wup[:, : b] and only update truncated'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='parameter matrices Wdown↓b and Wup↓b. The parameter up-\\ndates in each iteration of DyLoRA could be expressed as\\n∆W = Wdown↓bWup↓b. By allowing dynamic low-rank adap-\\ntation and search-free low-rank adaptation, DyLoRA reduces\\nthe computational cost and training time required to identify'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='the optimal rank for a particular task. AdaLoRA (Adaptive\\nLow-Rank Adaptation) [45] extends LoRA by dynamically\\nadjusting the rank of matrices to control the allocation budget.\\nIn AdaLoRA, the incremental update ∆W is reparameterized\\nusing singular value decomposition (SVD) and then truncates'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='the smallest singular values, i.e., ∆W = PΛQ. Both P\\nand Q are orthogonal matrices, and Λ is a diagonal matrix\\ncontaining the singular values {σ1, σ2, . . . , σr}. Here, r rep-\\nresents the rank of the matrix Λ. During training, P and Q\\nare initialized with Gaussian distribution with a regularizer'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='to ensure the orthogonality, while Λ is initialized with zero\\nand iteratively pruned to adjust the rank. AdaLoRA employs\\nthe sensitivity-base importance scoring [88], [89] with a new\\nmetric to prune the singular values of unimportant updates to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='update the Λ. By doing this, AdaLoRA effectively improves\\nparameter efficiency and allocation budgets. IncreLoRA [46]\\ndynamically incorporates trainable parameters into LoRA by\\nincreasing their ranks, guided by importance scores assigned'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='to each module during training. The allocation process assigns\\nlower ranks, possibly 0 to indicate no parameter updates,\\nto less important modules, while allocating higher ranks to\\nmore important modules. The parameter updates in IncreLoRA\\ncan be expressed as ∆W = WdownΛWup, in which Λ ='),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='can be expressed as ∆W = WdownΛWup, in which Λ =\\n[λ1, λ2, ··· , λr] is a diagonal matrix with λi could be any\\narbitrary constant, r is the rank of the each LoRA module.\\nBesides, an upper bound on the rank is set for each module\\nto control the parameter growth. Additionally, IncreLoRA'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='introduces a unique pretraining technique called “advance\\nlearning”, which ensures that the newly added parameters\\nin each module begin with favorable initial states. In this\\nway, it prevents insufficient training of subsequently added'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='parameters, allowing for effective utilization of the incremental\\nparameter allocation. Unlike LoRA, which operates on the\\nquery (Q), key (K), and value (V) projection modules of the\\nattention layer, the parameter updates are applied to all linear\\nlayers in IncreLoRA.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='layers in IncreLoRA.\\nLoRA-guided Pretrained Weight Update. Delta-LoRA\\n[47] updates the pretrained weight W as well as two low-\\nrank matrices Wdown and Wup, while using the same memory\\nas the original LoRA. The two low-rank matrices Wdown\\nand Wup are automatically updated as usual. The pretrained'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='weight, however, leverages the mathematical property that\\n∇W L(W, Wdown, Wup) = ∇WdownWupL(W, Wdown, Wup)\\n(it is achieved by removing the dropout layer in the original\\nLoRA module) for parameters update. Specifically, W is\\nupdated with the delta of the product of two low-rank matrices'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='in consecutive iterations, i.e., W ← W + ∆WdownWup =\\nW + (Wdown(t + 1)Wup(t + 1) − Wdown(t)Wup(t)). Lo-\\nRAPrune [48] introduces a LoRA-guided pruning criterion,\\nwhich utilizes the weights and gradients of LoRA instead of\\nthe gradients of pretrained weights for importance estimation'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 8}, page_content='to prune parameters of LoRA and pretrained weights. To\\naddress the substantial memory overhead associated with un-\\nstructured pruning and dependency-aware structured pruning,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='10\\nLoRAPrune devises a structured iterative pruning procedure\\nthat selectively eliminates redundant channels and heads.\\nLoRA-guided pruning criterion involves using low-rank matri-\\nces Wdown and Wup, along with their corresponding gradients\\n∇Wdown and ∇Wup, to calculate the importance score 4. This'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='score determines which weights are deemed unimportant and\\nsubsequently removed. Notably, LoRAPrune not only prunes\\nstructured weights, such as heads and channels, from the pre-\\ntrained weights, but also prunes the corresponding weights in\\nthe LoRA, i.e., δ = (W +WdownWup)⊙M, M ∈ {0, 1}1×G,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='G is the group number. Binary mask M is set to 0 when\\nthe corresponding group is unimportant, and 1 when it is\\nimportant. Therefore, after pruning and fine-tuning, the LoRA\\nweights can seamlessly merge with the pretrained weights,\\nensuring that no additional computations are necessary during'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='inference.\\nQuantization Adaption. QLoRA [49], a quantized variant\\nof LoRA, effectively addresses the limited computational\\nresource of LoRA for fine-tuning LLMs by quantizing the\\ntransformer model to 4-bit NormalFloat (NF4) precision with\\ndouble quantization processing, and using a paged optimizer'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='to deal with memory spikes. NF4 is a new data type that\\nis theoretically optimal for normally distributed weights. Al-\\nthough QLoRA quantizes pretrained weight W from FP16 into\\nNF4 so that LLMs can be fine-tuned with fewer GPUs, the\\nauxiliary weight of LoRA matrix WdownWup makes the final'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='weight return to FP16 again after fine-tuning. To this end, QA-\\nLoRA (Quantization-Aware Low-rank Adaption) [50] employs\\ngroup-wise quantization with low-rank adaptation to the pre-\\ntrained weight W, in which each column of W is partitioned'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='into L groups for quantization. In this way, QA-LoRA ensures\\nthat pretrained weights W and auxiliary weights are integrated\\ninto a quantized form after fine-tuning, resulting in a faster and\\nmore accurate computation during inference. While LOFTQ'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='(LoRA-Fine-Tuning-aware Quantization) [51] applies an N-bit\\nquantized weight Q and low-rank approximation Wdown ∈\\nRd1×r, Wup ∈ Rd2×r to approximate the original high-\\nprecision pretrained weight W ∈ Rd1×d2 as the initialization\\nof LoRA fine-tuning. Such an initialization alleviates the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='quantization discrepancy in QLoRA and significantly improves\\nthe generalization in downstream tasks.\\nLoRA-based Improvements. Kernel-wise Adapter [52]\\ntreats the different attention heads in the transformer as\\nindependent kernel estimators and utilizes the kernel structure'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='in self-attention to guide the assignment of tunable parameters.\\nLoRA is used as the underlying model to combine kernel-wise\\nadaptation for its flexibility in parameter assignment for dif-\\nferent weight matrices. Kernel-wise adapter has two variants:'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='Kernel-mix-lite (qv) and Kernel-mix-lite (qvo). Kernel-mix-\\nlite (qv) provides a lightweight solution for scenarios with\\nlimited parameter budgets, while Kernel-mix (qvo) is suitable\\nfor scenarios with intermediate parameter budgets. The suffix'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='(qv) means that the method will adjust Wq and Wv, while the\\nsuffix (qvo) means that the method will modify Wq, Wv, and\\nWo. Laplace-LoRA [53] incorporates Bayesian inference into\\n4Importance score I is calculate via I = ∇W ⊙ W,∇W ≈ Wdown ·\\n∇Wup + ∇Wdown · Wup − ∇Wdown · ∇Wup.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='∇Wup + ∇Wdown · Wup − ∇Wdown · ∇Wup.\\nthe LoRA parameters to address the issue of overconfidence\\nand improve calibration. A key challenge lies in obtaining\\nthe posterior distribution for Bayesian inference, which is\\nresolved by using Laplace approximation [90]. Laplace-LoRA'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='can be viewed as an approximation of the posterior distribution\\nover LoRA parameters using Laplace approximation. Hence,\\nLaplace-LoRA maintains existing pretraining and fine-tuning\\nprocedures while reducing the dimensionality of Bayesian\\ninference. LoRA-FA (LoRA with Frozen-A) [54] is pro-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='posed to reduce the expensive activation memory of LoRA\\nwithout introducing any computational overhead. LoRA-FA\\nkeeps the pretrained weight W and down-projection matrix\\nWdown frozen and only updates the up-projection matrix Wup.\\nWdown is decomposed into Q and R via QR decomposi-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='tion, and ∆W = WdownWup = QRWup = Q ˆWup =Pr\\ni=1 Q:,i ˆWup,i,:, in which {Q:,i}r\\ni=1 are orthogonal unit vec-\\ntors (r is the rank of Wdown). Thus, ∆W is a combination of\\nr orthogonal vectors, limiting the change of weight residing in\\na low-rank space. Consequently, there is no need to store full-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='rank input activations simultaneously, alleviating the memory\\nburden associated with activation storage.\\nLoRA-based Multi-task Fine-tuning. LoRAHub [55]\\nleverages a composition of multiple trained LoRA modules\\nfor cross-task transfer to fine-tune the model on new tasks.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='Specifically, LoRAHub trains task-specific LoRA modules in\\na variety of tasks to obtain a synthesized module, ˆm =\\n(w1W1\\ndown +··· +wN WN\\ndown)(w1W1\\nup+··· +wN WN\\nup), which\\nis then amalgamated with the LLMs to adapt the new task.\\nThus, the objective of LoRAHub is to find the best weight'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='set {w1, w2, ··· , wN }, which is achieved by the gradient-free\\ncombinatorial optimization approach Shiwa [91]. MOELoRA\\n[56] combines LoRA with mixture-of-experts (MoE) for multi-\\ntask fine-tuning, in which each expert is a LoRA module\\nfor learning task-specific knowledge. Additionally, MOELoRA'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='devises a task-motivated gate function to produce distinct\\nfine-tuned parameters for various tasks. L-LoRA (Linearized\\nLoRA) [57] is a linearized PEFT method to improve the\\nmulti-task fusion capability of fine-tuned task-specific models\\nwith low computation costs. L-LoRA constructs a linear'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='function using a fir-order Taylor expansion, as illustrated in\\nEquation 15. In L-LoRA, only the linearized LoRA modules\\nare fine-tuned in the tangent space, incurring fewer trainable\\nparameters compared to LoRA. For the multi-task fusion meth-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='ods, simple average, task arithmetic [92], [93], ties-merging\\n[94], and LoRAhub [55] are employed for multi-task fusion.\\nfθ0 (x; ϕ(t)) ≈flin\\nθ0 (x; ϕ(t)) = fθ0 (x; ϕ(0))\\n+ ∇ϕfθ0 (x; ϕ(0))T (ϕ(t) − ϕ(0)). (15)\\nD. Hybrid Fine-Tuning\\nHybrid fine-tuning approaches aim to combine various'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='PEFT approaches, such as adapter, prefix-tuning, and LoRA,\\nto leverage the strengths of each method and mitigate their\\nweaknesses. By integrating different features of PEFT meth-\\nods, hybrid fine-tuning achieves improved overall perfor-\\nmance compared to individual PEFT methods. These works'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 9}, page_content='are classified into two approaches: Mannual Combination'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='11\\n[16], [35], [58], [59], in which multiple PEFT methods are\\ncombined manually by sophisticated design, and Automatic\\nCombination [60], [61], [62], where various PEFT methods\\nare incorporated automatically via structure search.\\n1) Manual Combination: Manual combination mainly in-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='volves integrating the structure or features of one PEFT\\nmethod into another PEFT method to enhance performance\\nwhile achieving parameter efficiency. MAM Adapter (Mix-\\nAnd-Match Adapter) [16] is the combination of scaled parallel\\nadapter and prefix-tuning, which employs prefix-tuning with'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='smaller bottleneck dimensions at the attention layer and al-\\nlocates more parameter budget to modify the representation\\nof FFN using the scaling parallel adapter. Scaled parallel\\nadapter denotes the parallel adapter with a scaling factor\\nto adjust the adapter output. Concretely, the output of the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='MAM adapter h can be expressed with h = LN(X +\\nscale ∗ FFN(LN(Attn([Pk, X]) + [Pv, X]))) for the input X.\\nFurther, U-MAM (Unstructured MAM) and S-MAM (struc-\\ntured MAM) [35] are proposed by combining NAS algorithm\\n[85] and pruning technique to automatically determine which'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='parameters of the network need to be fine-tuned based on\\nMAM adapter. NAS algorithm takes the maximum number\\nof parameters required for PEFT architectures as input and\\napplies the pruning operation to reduce trainable parameters.\\nThe criteria for deciding which PEFT parameters to prune'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='are based on the first-order approximation of the change in\\ntraining loss resulting from pruning the PEFT parameter W,\\ni.e., −W ·∇W L(W). U-MAM directly employs this criterion\\nto prune the parameters in MAM, while S-MAM sums the\\ncriterion over each column of Wdown.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='criterion over each column of Wdown.\\nCompacter [58] is developed based on adapters, low-\\nrank optimization, and a parameterized hypercomplex mul-\\ntiplication (PHM) layer [95]. It follows a similar structure\\nto adapters, consisting of a down-projection, a nonlinear'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='activation function, and an up-projection. However, Compacter\\nreplaces the down-projection and up-projection in the adapters\\nwith the low-rank parameterized hypercomplex multiplication\\n(LPHM) layer, which is an extension of PHM that incorporates'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='low-rank optimization. Structurally, PHM layer resembles a\\nfully connected layer, but with the learned W represented as a\\nsum of Kronecker products, i.e., W = Pn\\ni=1 Ai⊗Bi. Notably,\\nwhen the weights of down-projection and up-projection are'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='calculated as in that of the PHM layer,Ai is a shared parameter\\nacross all adapter layers, while Bi represents adapter-specific\\nparameters. This kind of adapter is called PHM Adapter .\\nSimilarly, Compacter obtains the weight matrix in each LPHM'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='layer utilizing the sum of Kronecker products, but Compacter\\nreparameterizes Bi as the product of two independent ranks\\nwith one weight, and the weight matrix in Compacter is\\ncalculated as follows:\\nW =\\nnX\\ni=1\\nAi ⊗ Bi =\\nnX\\ni=1\\nAi ⊗ (sitT\\ni ). (16)\\nW ∈ Rk×d, Ai ∈ Rn×n, Bi ∈ R\\nk\\nn × d\\nn ; si ∈ R\\nk'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='W ∈ Rk×d, Ai ∈ Rn×n, Bi ∈ R\\nk\\nn × d\\nn ; si ∈ R\\nk\\nn ×r, ti ∈ Rr× d\\nn .\\nCompacter++ is a variant of Compacter that inserts a Com-\\npacter layer after the FFN layer of each transformer module\\nand requires fewer parameters to be updated than Compacter.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='UniPELT [59] incorporates sequential adapter, prefix-\\ntuning, and LoRA via a gating mechanism. In UniPELT,\\nadapters are added after the feed-forward layer, prefix-tuning\\nis employed to the key (K) and value (V ) vectors of the multi-\\nhead attention layer, and LoRA is used in attention matrices of'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='Wq and Wv of the transformer. Each PEFT module is equipped\\nwith a gating mechanism composed of a linear function with\\nthe dimension of the output being 1, a sigmoid function,\\nand a mean function. The gating mechanism controls the\\nactivation of each submodule, dynamically assigning higher'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='weights to submodules that make positive contributions to\\na given task. The trainable parameters encompass low-rank\\nLoRA matrices Wdown and Wup, prefix-tuning parameters\\nPk and Pv, adapter parameters, and weights for the gating\\nfunction. Consequently, UniPELT requires more parameters'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='and inference time than adapter, prefix-tuning, and LoRA, but\\nachieves better performance compared with the performance\\nof the best individual PEFT method.\\n2) Automatic Combination: Automatic combination ex-\\nplores how to configure PEFT methods like adapters, prefix-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='tuning, BitFit, and LoRA to different layers of the transformers\\nautomatically using various structure search and optimization\\napproaches. However, it typically requires more time and cost\\ndue to the need to perform optimization searches in the model'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='or structure. AutoPEFT [60] integrates sequential adapter,\\nparallel adapter, and prefix-tuning into the transformer block.\\nThe serial adapter receives the hidden state from the FFN\\noutput as input, while the parallel adapter takes the hidden'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='state before the FFN layer as its input. In addition, the prefix-\\ntuning module concatenates two prefix vectors, Pk and Pv,\\nwith the original key and value vectors, respectively, enabling\\nmulti-head attention to adapt to specific target tasks. Motivated'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='by the success of NAS algorithm, AutoPEFT proposes to use\\nthe Bayesian optimization approach to automatically search\\nfor an appropriate neural architecture network that selectively\\nactivates certain layers to incorporate these PEFT modules.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='Bayesian optimization is not only sample-efficient and zeroth-\\norder but also well-suited for multi-objective setups, enabling\\ncost-efficient optimization and facilitating the trade-off be-\\ntween performance and cost. Moreover, it is more paralleliz-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='able during search, which can decrease memory usage.\\nS3Delta-M (Search for Sparse Structure of Delta Tun-\\ning Mix) [61] is a mixture of LoRA, Compacter (low-\\nrank adapter), BitFit, and LNFit 5. Different from the simple\\nincorporation of PEFT techniques, S 3Delta-M is developed'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='by conducting a differentiable delta tuning structure search.\\nIt explicitly controls sparsity and searches for an optimal\\ncombination of these techniques in a unified search space.\\nIn S3Delta-M, each PEFT module (LoRA, Compacter, BitFit,\\nand LNFit) is inserted into the corresponding layers of the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='PLM to ensure the best performance is achieved. The specific\\ncombination and placement of these modules are determined\\nthrough the structure search process, which is guided by\\nexplicit sparsity control. S4 [62] is a combination of Sequential\\nAdapter, Prefix-tuning, BitFit, and LoRA. Unlike previous'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 10}, page_content='5LNFit is trained only on the variance vectors in the layer normalization\\nmodule of the PLMs, inspired by [86] which trains only on the batch\\nnormalization module in convolutional neural networks.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='12\\nmethods that utilize the same PEFT module uniformly across\\nall layers of the transformer, S4 is designed by searching\\nfor various layer groupings, trainable parameter allocations,\\ntunable groups, and PEFT module assignments. In S4, the\\nlayers of the PLMs are divided into four groups, G1, G2,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='G3, G4, in a “spindle” pattern. This means that more layers\\nare allocated to the middle groups ( G2 and G3) while fewer\\nlayers are assigned to the top and bottom groups ( G1 and G4).\\nHowever, all trainable parameters are allocated uniformly, i.e.,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='the number of trainable parameters in each layer remains the\\nsame across all groups. Different groups are equipped with dif-\\nferent combinations of sequential adapter, prefix-tuning, BitFit,\\nand LoRA. Extensive experimental results demonstrate that'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='better performance is achieved when each group is equipped\\nwith the following combinations of PEFT methods. A denotes\\nsequential adapter, P denotes prefix-tuning, B denotes BitFit,\\nand L denotes LoRA.\\nG1 : (A, L); G2 : (A, P);\\nG3 : (A, P, B); G4 : (P, B, L).\\nE. Unified Fine-tuning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='E. Unified Fine-tuning\\nUnified fine-tuning presents a unified framework for fine-\\ntuning, which streamlines the incorporation of diverse fine-\\ntuning methods into a cohesive architecture, ensuring consis-\\ntency and efficiency across the adaptation and optimization of'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='models. Unlike hybrid fine-tuning methods, unified fine-tuning\\nmethods typically utilize a single PEFT method rather than a\\ncombination of various PEFT methods.\\nAdaMix [63] leverages a mixture of adaptation module\\napproaches to obtain a unified framework for fine-tuning.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='Motivated by sparsely-activated MoE [96], AdaMix treats\\neach adaptation module as an individual expert and employs\\nstochastic routing to randomly select a down-projection ma-\\ntrix and an up-projection matrix for weight updates. Such\\nstochastic routing allows the adaption module to learn multiple'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='views for the given task, but it also poses a challenge in\\ndeciding which adaption module to use during inference.\\nTo this end, Adamix utilizes consistency regularization and\\nadaption module merging (i.e., average weights of all down-\\nand up-projection matrices) to select the trained adaption'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='module and obtain the same computational cost as that of a\\nsingle module. Notably, adaption modules in Adamix could be\\nadapters like sequential adapter [9] or low-rank decomposition\\nmatrices like LoRA [11].\\nSparseAdapter [64] utilizes network pruning technique to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='construct a unified framework in which various PEFT methods,\\nincluding adapters family and LoRA [9], [11], [16], can be\\nfurther pruned to improve parameter efficiency. SparseAdapter\\nsets a target sparsity, denoted as s, and assigns a score,\\ndenoted as z, to all parameters of adapters and LoRA. Pa-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='rameters with scores below the threshold zs (corresponding\\nto the s-th lowest percentile of z) are considered redundant\\nand removed. The score z can be computed using pruning\\nmethods, such as random pruning, magnitude pruning [97],\\nErdos-Renyi [98], SNIP [99], or GraSP [100], based on the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='adapter weight W, with SNIP-based SparseAdapter yielding\\nthe best results. Furthermore, SparseAdapter exhibits improved\\nperformance compared to full fine-tuning when utilizing the\\n“Large-Sparse” setting, which involves larger bottleneck di-\\nmensions and higher sparsity ratios. Notably, the network'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='pruning technique proposed in SparseAdapter is a plug-in\\nmethod that can be applied to any adapter variants, such as\\nLoRA [11], MAM Adapter [16], and AdapterFusion [19]. The\\noptimized parameters in SparseAdapter can be represented as\\nˆW = W ⊙ M, in which M is a binary mask matrix with'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='M = I{z≥zs} and z = score(W).\\nProPETL [65] introduces a single prototype network (e.g.,\\nadapter, prefix-tuning, and LoRA) across layers and tasks and\\nconstructs different sub-networks for each layer using various\\nbinary masks. Inspired by ALBERT [101], ProPETL leverages'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='parameter sharing within the prototype network modules in\\neach layer of the transformer, enhancing parameter efficiency\\nand reducing storage requirements. In ProPETL, binary masks\\nM ∈ {0, 1}n are introduced in each layer of the transformer, in'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='which n is the number of parameters in a single PEFT module.\\nEach mask corresponds to a specific sub-network of the shared\\nprototype network. By doing so, though each layer shares\\nthe parameters of the same prototype network, each layer\\nhas a different sub-network to capture meaningful semantic'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='representations. The final objective of the task adaptation for\\nthe PLMs can be expressed as follows:\\nmax\\nθpro,m1,m2,···,mL\\nNX\\ni=0\\nlogP(Yi|Xi; θlm, θsub), (17)\\nθsub = [θpro ⊙ m1, θpro ⊙ m2, ··· , θpro ⊙ mL].\\nHere, θlm represents the frozen pretrained parameters of the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='PLMs, mi (i = 1, 2, ··· , L) is binary mask matrix, and θsub\\ndenotes the parameters to be optimized.\\nIV. E XPERIMENTS\\nA. Experimental Settings\\n1) PLMs and Datasets: We use the encoder-only models\\nRoBERTa-base (125M) and RoBERTa-large (355M) [2] to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='evaluate on the GLUE benchmark [100], encoder-decoder\\nmodels T5-base (220M) and T5-large (770M) [4] to evaluate\\non the WMT16 En-Ro dataset 6, and decoder-only models\\nLLaMA-7B and LLaMA-13B [7] fine-tuned with the Alpaca\\ndataset [102] to evaluate on the MMLU benchmark [103].'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='All these PLMs with different model types and model scales\\nare based on the encoder, decoder, or encoder-decoder of the\\nTransformer architecture. The datasets we use for experi-\\nments cover a wide range of tasks, from NLU to MT and\\nNLG. The GLUE benchmark covers a collection of NLU'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='tasks, including single-sentence classification, and sentence-\\npair classification tasks. WMT16 En-Ro dataset consists of\\nparallel data pairs, where each pair consists of an English\\nsentence and its corresponding translation into Romanian.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 11}, page_content='Alpaca [102] is an instruction dataset containing 52k samples.\\nMMLU Benchmark [103] encompasses a comprehensive range\\nof 57 disciplines spanning science, humanities, social sciences,\\nand more. The level of difficulty of the benchmark ranges from\\n6https://huggingface.co/datasets/wmt16'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='13\\nTABLE III: Fine-tuning RoBERTa-base (RoB B) and RoBERTa-large (RoB L) models on the GLUE benchmark. Specifically,\\nwe report the Matthews correlation for COLA, accuracy/F1 score for MRPC and QQP, Pearson/Spearman correlation for'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='STS-B, averaged matched accuracy for MNLI, and accuracy for other NLU tasks. Higher values indicate better performance\\nacross all metrics. We present the number of trainable parameters (# TPs) of each method, excluding Child-Tuning D due to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='its randomness during network pruning. We also bold the maximum values and underline the minimum values.\\nModel PEFT Method #TPs CoLA SST2 MRPC STS-B QQP MNLI QNLI RTE Avg.\\nRoBB\\nFT 124.6M 59.07 92.89 88.24/91.58 90.87/90.61 90.81/87.72 86.27 91.07 72.20 84.00/84.00'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='AdapterS 7.41M 63.32 94.31 90.44/93.18 91.25/90.94 90.81/86.55 87.33 92.06 73.56 85.39/85.16\\nPrompt-tuning 0.61M 49.37 92.09 70.83/81.72 82.44/83.11 82.99/78.35 80.57 80.03 58.12 74.56/75.42\\nPrefix-tuning 0.96M 59.31 93.81 87.25/91.03 88.48/88.32 87.75/84.09 85.21 90.77 54.51 80.89/80.88'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='(IA)3 0.66M 59.58 93.92 87.00/90.52 90.30/90.32 87.99/84.10 83.95 90.88 71.12 83.09/83.05\\nBitFit 0.69M 61.32 94.72 89.22/92.41 90.34/90.27 88.12/84.11 84.64 91.09 77.98 84.68/84.57\\nChild-TuningD - 60.33 93.58 89.22/92.20 91.14/90.93 90.98/88.04 87.40 92.20 77.62 85.31/85.29'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='LoRA 0.89M 62.09 94.04 87.50/90.68 90.66/90.83 88.83/85.21 86.54 92.02 72.92 84.33/84.29\\nAdaLoRA 1.03M 59.82 93.92 87.99/91.33 90.83/90.73 88.58/84.98 86.26 91.43 70.04 83.61/83.56\\nMAM Adapter 46.78M 61.42 94.87 89.31/92.21 90.74/90.42 88.31/83.20 86.63 90.19 72.62 84.26/83.95'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='ProPELTAdapter 1.87M 66.33 93.85 87.25/90.82 91.33/91.04 89.22/85.79 86.49 92.56 75.54 85.32/85.30\\nProPELTPrefix 10.49M 61.79 94.30 88.73/91.98 90.30/90.19 88.54/85.05 86.22 91.51 63.31 83.08/83.04\\nProPELTLoRA 1.77M 60.38 94.11 87.42/90.87 90.76/90.55 88.90/85.55 86.84 92.04 67.39 83.48/83.47\\nRoBL'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='RoBL\\nFT 355.3M 65.78 95.54 89.22/92.28 91.74/91.76 89.30/86.68 89.42 93.61 81.23 86.98/87.04\\nAdapterS 19.77M 67.03 96.37 89.94/92.54 92.58/92.42 92.19/88.50 91.00 94.31 85.25 88.58/88.43\\nPrompt-tuning 1.07M 61.13 94.61 73.04/81.29 78.51/78.99 80.74/75.16 68.15 89.13 60.29 75.70/76.09'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='Prefix-tuning 2.03M 59.01 95.76 88.24/91.37 90.92/91.07 88.88/85.45 89.30 93.32 74.01 84.93/84.91\\n(IA)3 1.22M 61.15 94.61 86.52/90.33 92.22/92.03 89.45/86.25 88.63 94.25 81.23 86.00/86.06\\nBitFit 1.32M 68.01 96.10 90.93/93.38 91.93/91.77 89.48/86.43 89.98 94.47 87.73 88.57/88.47'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='Child-TuningD - 63.08 95.07 90.69/93.43 92.36/92.18 91.52/88.75 35.45 93.15 86.25 80.95/80.92\\nLoRA 1.84M 64.47 96.67 87.50/91.19 91.66/91.44 90.15/86.91 90.76 95.00 79.78 87.00/87.03\\nAdaLoRA 2.23M 65.85 94.95 89.46/92.34 92.05/91.80 89.60/86.30 90.36 94.62 77.98 86.86/86.78'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='MAM Adapter 122.2M 67.39 95.81 90.12/92.77 92.44/92.18 90.87/86.65 90.62 94.31 86.62 88.52/88.29\\nProPELTAdapter 5.40M 65.55 96.27 89.71/92.54 91.92/91.67 90.67/87.74 91.37 95.20 88.89 88.70/88.65\\nProPELTPrefix 26.85M 62.24 96.17 90.04/92.92 90.70/90.49 89.30/86.30 90.33 94.73 79.71 86.65/86.61'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='ProPELTLoRA 4.19M 61.90 95.93 89.06/92.19 91.66/91.38 90.93/88.05 90.53 94.93 83.57 87.31/87.31\\nbeginner to advanced levels of expertise, testing both world\\nknowledge and problem-solving abilities.\\n2) PEFT Methods: Eleven representative PEFT methods:'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='sequential adapter (Adapter S) [9], prompt-tuning [24], prefix-\\ntuning [10], (IA) 3 [30], BitFit [34], Child-Tuning [39], LoRA\\n[11], AdaLoRA [45], QLoRA [49], MAM adapter [16], and\\nProPELT [65] are chosen. Since the GLUE benchmark consists'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='of a series of NLU tasks, it serves as the preferred evaluation\\ndataset used by most PLMs to validate the effectiveness of\\nPEFT methods. Ten representative PEFT methods other than\\nQLoRA are selected to fine-tune RoBERTa-base/large. For T5-\\nbase/large, we use (IA) 3 and LoRA for fine-tuning. As for'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='LLaMA-7B/13B, (IA)3, LoRA, and QLoRA are used for fine-\\ntuning.\\n3) Implementation Details: Since “prompt-tuning, prefix-\\ntuning, (IA) 3, LoRA, and AdaLoRA” have been integrated\\ninto the PEFT library7. Therefore, we directly utilize the PEFT\\nlibrary to invoke these PEFT methods for fine-tuning. For'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='BitFit, Child-tuing D, MAM adapter, QLoRA, and ProPELT,\\nwe experiment using their original code. Significantly, we\\nexperiment with sequential adapter using code from the MAM\\nadapter. For RoBERTa-base/large, all PEFT methods are fine-\\ntuned using a batch size of 32 and a sequence length of 128,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='except for (IA) 3 which is fine-tuned using batch size 8. We\\nuse the batch size 64 for T5-base and 32 for T5-large. For\\nLLaMA-7B/13B, we use batch size 16 for fine-tuning. All\\nexperiments are implemented with A800 GPU.\\n7https://huggingface.co/docs/peft/index'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='7https://huggingface.co/docs/peft/index\\nB. Fine-tuning Performance and Parameter Efficiency\\n1) RoBERTa Base/Large on GLUE: Experimental results\\nof full fine-tuning and 11 representative PEFT methods with\\nRoBERTa-base/large on the GLUE benchmark are presented'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='in Table III, the following findings are observed:\\n• All PEFT methods reduce the number of trainable\\nparameters, and most PEFT methods achieve perfor-\\nmance matching or even better than full fine-tuning on\\nthe GLUE benchmark. For RoBERTa-base, the aver-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='the GLUE benchmark. For RoBERTa-base, the aver-\\nage performance of prompt-tuning, prefix-tuning, IA 3,\\nAdaLoRA, ProPELTprefix and ProPELTLoRA on GLUE all\\nunderperforms full finetuning, while that of sequential\\nadapter, BitFit, Child-Tuning D, LoRA, MAM adapter,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='and ProPELT Adapter outperforms full fine-tuning. For\\nRoBERTa-large, the average performance of prompt-\\ntuning, prefix-tuning, IA3, AdaLoRA, ProPELT prefix and\\nChild-TuningD on GLUE underperforms full fine-tuning,\\nwhile that of sequential adapter, BitFit, LoRA, MAM'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='adapter, ProPELT Adapter and ProPELT LoRA outperforms\\nfull fine-tuning.\\n• ProPELTadapter, a unified fine-tuning method that em-\\nploys the AdapterFusion as the backbone, uses about\\n1.50% of the trainable parameters to fine-tune RoBERT-\\nbase and RoBERTa-large, but achieves optimal average'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 12}, page_content='performance on the GLUE benchmark, outperforming\\nRoBERT-base (FT) by about 1.30% and RoBERT-large\\n(FT) by about 1.65%.\\n• MAM Adapter, a hybrid fine-tuning method that com-\\nbines parallel adapters and prefix-tuning, achieves better\\nperformance than prefix-tuning, but also consumes a large'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='14\\nTABLE IV: Fine-tuning T5-base and T5-large models on the\\nWMT16 En-Ro dataset and evaluating their performance using\\nBLEU score. The higher BLEU score indicates a better quality\\nof translation output.\\nModel PEFT Method # TPs BLEU\\nT5-base\\nFT 222.9M 27.42\\n(IA)3 0.07M 27.58\\nLoRA 0.88M 27.78\\nT5-large'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='(IA)3 0.07M 27.58\\nLoRA 0.88M 27.78\\nT5-large\\nFT 737.7M 28.13\\n(IA)3 0.19M 28.12\\nLoRA 2.36M 28.12\\namount of trainable parameters.\\n• Sequential adapter requires more trainable parameters\\nthan prompt-tuning, prefix-tuning, (IA) 3, BitFit, Child-\\nTuningD, LoRA, and AdaLoRA, but achieves better'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='TuningD, LoRA, and AdaLoRA, but achieves better\\nperformance than them on the GLUE benchmark.\\n• Prompt-tuning with the virtual marker length set to 20\\nachieves the smallest trainable parameter, but also the\\nworst performance, with its average performance on the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='GLUE benchmark being about 10% lower than full fine-\\ntuning.\\n• Child-TuningD performs well when fine-tuning RoBERT-\\nbase on the GLUE benchmark and obtains better perfor-\\nmance than full fine-tuning, but performs poorly when\\nfine-tuning RoBERT-large on the MNLI dataset, which'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='we guess it caused by the learning rate.\\n2) T5 Base/Large on WMT16 En-Ro Dataset: As depicted\\nin Table IV, both (IA) 3 and LoRA significantly reduce the\\nnumber of trainable parameters compared to full fine-tuning,\\nwhile maintaining comparable performance. Specifically, (IA)3'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='employs only 0.03% of trainable parameters and achieves a\\nBLEU score [104] 0.16 higher than full fine-tuning for T5-\\nbase and 0.01 lower for T5-large. LoRA achieves a BLEU\\nscore 0.36 higher than full fine-tuning on T5-base using only\\n0.39% of trainable parameters, and 0.01 lower than full fine-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='tuning on T5-large using only 0.32% of trainable parameters.\\n3) LLaMA on MMLU: We first tracked the 5-shot MMLU\\ndev accuracy of LLaMA-7B-Alpaca and LLaMA-13B-Alpaca\\nwith full fine-tuning and PEFT approaches LoRA, QLoRA,\\nand (IA)3, following the work in [49]. As depicted in Fig. 4,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='there are significant performance fluctuations in the 5-shot\\nMMLU dev accuracy throughout model training, particularly\\nin LoRA and QLoRA. Moreover, we discovered that full\\nfine-tuning performance of LLaMA-7B-Alpaca on the MMLU\\nbenchmark is extremely sensitive to the learning rate, as shown'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='in Table V. Subsequently, we select the checkpoint with the\\nbest performance on the dev set and perform 5-shot accuracy\\nexperiments on the test set of the MMLU benchmark.\\nAs illustrated in Table VI, full fine-tuning of both LLaMA-\\n7B and LLaMA-13B produces better 5-shot MMLU test'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='7B and LLaMA-13B produces better 5-shot MMLU test\\naccuracy compared to other PEFT methods. (IA) 3, LoRA, and\\nQLoRA methods all greatly reduce the number of trainable\\nparameters with (IA) 3 performs best. Although (IA) 3 only\\nconsumes 0.02% of full fine-tuning parameters, it performs'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='2-4% lower than full fine-tuning. LoRA and QLoRA require\\nTABLE V: Full fine-tuning performance of LLaMA-7B-\\nAlpaca on the test set of MMLU benchmark with different\\nlearning rates.\\nLearning rate 5-shot MMLU Accuracy\\n2e-4 25.71\\n5e-5 26.65\\n1e-6 41.79'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='2e-4 25.71\\n5e-5 26.65\\n1e-6 41.79\\nabout 2% of full fine-tuning parameters, achieving 5-shot\\nMMLU accuracy that is about 2% lower than full fine-tuning.\\nIn particular, QLoRA only uses half the number of trainable\\nparameters of LoRA but achieves comparable performance.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='This reduction of parameters in QLoRA can be attributed to\\nthe incorporation of 4-bit NormalFloat quantization.\\nC. Memory Efficiency\\nIt has been demonstrated that PEFT methods effectively re-\\nduce the number of trainable parameters. However, it remains'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='unclear whether they can also reduce GPU memory usage.\\nTo assess the impact of PEFT methods on GPU memory,\\nwe compare the GPU memory cost of full fine-tuning and\\nPEFT methods across various models and benchmarks. The\\nspecific experimental settings can be seen in the section of'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='implementation details. As presented in Table VII, the mem-\\nory usage of full fine-tuning in RoBERTa, T5, and LLaMA\\nis positively related to total model parameters. RoBERTa,\\nspecifically RoBERTa-base, consumes less memory, requiring\\nonly 5.38GB. In contrast, LLaMA demands significantly larger'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='memory, notably LLaMA-13B, necessitating approximately\\n290GB for full fine-tuning.\\nIn RoBERTa-base/large, prompt-tuning, prefix-tuning,\\n(IA)3, LoRA and AdaLoRA (implemented using the PEFT\\nlibrary), and BitFit significantly reduce the GPU memory'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='footprint compared to full fine-tuning. Surprisingly, sequential\\nadapter, MAM adapter, Child-Tuning D, and ProPELT all use\\nmore memory than full fine-tuning. Both sequential adapter\\nand MAM adapter exhibit higher memory consumption,\\naround three times that of full fine-tuning, with the MAM'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='adapter consuming even more memory. For T5-base/large\\nmodels, (IA) 3 and LoRA all demonstrate effective memory\\nreduction during fine-tuning, with LoRA outperforming (IA) 3.\\nNotably, (IA) 3 consumes less GPU memory than LoRA in\\nRoBERTa-base/large, which is caused by the smaller batch'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='size during (IA) 3 fine-tuning ((IA)3 sets the batch size to 8).\\nLikewise, (IA)3, LoRA, and QLoRA all significantly reduce\\nthe GPU footprint compared to full finetuning in LLaMA-\\n7B/13B. In addition, we discovered that the PEFT method is\\nmore effective in reducing memory usage when the number'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='of model parameters is larger. For example, in LLaMA-\\n7B-Alpaca, compared with full fine-tuning, IA 3, LoRA, and\\nQLoRA reduce memory usage by 24.08%, 26.30%, and\\n66.66%, respectively; while in LLaMA-13B-Alpaca, compared\\nwith full fine-tuning, IA3, LoRA, and QLoRA reduce memory'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 13}, page_content='usage by 33.55%, 39.46% and 76.86% of memory usage.\\nNotably, QLoRA dramatically reduces GPU memory con-\\nsumption, with QLoRA fine-tuning the LLaMA-7B requiring'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='15\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca (FT)\\n(a) LLaMA-7B-Alpaca-FT\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca ((IA)3) (b) LLaMA-7B-Alpaca-(IA)3\\n2000 4000 6000 8000 10000'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca (LoRA) (c) LLaMA-7B-Alpaca-LoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n32\\n34\\n36\\n38\\n40\\n425-shot MMLU Accuracy(%)\\nLLaMA-7B-Alpaca (QLoRA) (d) LLaMA-7B-Alpaca-QLoRA\\n2000 4000 6000 8000 10000'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca (FT)\\n(e) LLaMA-13B-Alpaca-FT\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca ((IA)3) (f) LLaMA-13B-Alpaca-(IA)3'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca (LoRA) (g) LLaMA-13B-Alpaca-LoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n515-shot MMLU Accuracy(%)\\nLLaMA-13B-Alpaca (QLoRA) (h) LLaMA-13B-Alpaca-QLoRA'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='Fig. 4: The 5-shot accuracy fluctuates on the MMLU dev set with the increase in evaluation steps when fine-tuning LLaMA-\\n7B-Alpaca and LLaMA-7B-Alpaca using the IA 3, LoRA, and QLoRA methods.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='TABLE VI: Comparison of the average 5-shot MMLU test accuracy of LLaMA-7B and LLaMA-13B models fine-tuned with\\nAlpaca. The higher the MMLU accuracy, the better. We also report total model parameters (# APs) and the ratio of trainable\\nparameters.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='parameters.\\nModel PEFT Method # TPs # APs % Params 5-shot MMLU Accuracy\\nLLaMA-7B-Alpaca\\nFT 6738.4M 6738.4M 100 41.79\\n(IA)3 1.58M 6740.0M 0.02 37.88\\nLoRA 159.9M 6898.3M 2.32 40.67\\nQLoRA 79.9M 3660.3M 2.18 39.96\\nLLaMA-13B-Alpaca\\nFT 13015.9M 13015.9M 100 49.60\\n(IA)3 2.48M 13018.3M 0.02 47.42'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='(IA)3 2.48M 13018.3M 0.02 47.42\\nLoRA 250.3M 13266.2M 1.88 47.49\\nQLoRA 125.2M 6922.3M 1.81 47.29\\nonly 1/3 of the memory required for full fine-tuning, and fine-\\ntuning the LLaMA-13B requiring less than 1/4 of the memory\\nrequired for full fine-tuning. This advancement opens up the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='possibility of fine-tuning LLMs for various downstream tasks\\nin computational resource-constrained scenarios.\\nV. A PPLICATIONS\\nA. Multi-task Learning\\nMulti-task learning is a method that involves training a\\nmodel on multiple related tasks and exploiting the informa-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='tion shared and transferred between them to improve the\\nperformance of each task. PEFT methods such as adapters,\\nprompt-tuning, and LoRA utilize additional modules that can\\nbe plugged into PLMs and thus can be used for task-specific\\nfine-tuning to improve generalization of multi-task learning.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='For instance, studies from [19], [21], [22], [75] leverage task-\\nspecific adapters to learn information stored in multiple tasks\\nto achieve more robust transfer learning on new tasks. Several\\nworks [26], [27], [28] employ prompt-tuning for multi-task'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='learning. They either utilize pretrained soft prompts from\\nmultiple source tasks to initialize the soft prompt of the\\ntarget task, based on the similarity between the source and\\ntarget tasks, or employ multi-task data to learn a single\\nshared prompt and transfer it to the target task. Similar to'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='the adapter, a composition of multiple task-specific LoRA\\nmodules is also leveraged to transfer knowledge to new tasks\\n[55], [56]. L-LoRA [57] enhances the fusion capabilities of\\nmulti-task learning by preventing negative inference between'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='task-specific representations. Additionally, [93] utilizes arith-\\nmetic operators, such as the addition and negation operators,\\nto merge parameters of various PEFT methods trained on\\ndifferent tasks for multi-task learning.\\nB. Cross-Lingual Transfer'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='B. Cross-Lingual Transfer\\nCross-lingual transfer involves transferring knowledge or\\nmodels from one language to another. Numerous works have\\nemployed PEFT methods, such as adapters, for cross-lingual\\ntransfer due to their unique modular design. Bapna and Firat'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 14}, page_content='[105] utilize sequential adapter [9] to fine-tune and restore\\nthe performance of a multilingual neural machine translation'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='16\\nTABLE VII: The peak GPU memory usage when fine-tuning RoBERT-base, RoBERTa-large, T5-base, T5-large, LLaMA-7B,\\nand LLaMA-13B model using full fine-tuning and various PEFT methods.\\nModel & Method Memory (GB) Model & Method Memory (GB)\\nRoBERTa-base (FT) 5.38 RoBERTa-large (FT) 11.96'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='RoBERTa-base (FT) 5.38 RoBERTa-large (FT) 11.96\\nRoBERTa-base (AdapterS) 15.29 RoBERTa-large (AdapterS) 37.17\\nRoBERTa-base (Prompt-tuning) 3.84 RoBERTa-large (Prompt-tuning) 7.98\\nRoBERTa-base (Prefix-tuning) 3.56 RoBERTa-large (Prefix-tuning) 7.58\\nRoBERTa-base ((IA)3) 2.62 RoBERTa-large ((IA)3) 4.83'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='RoBERTa-base (BitFit) 3.27 RoBERTa-large (BitFit) 7.50\\nRoBERTa-base (Child-TuningD) 6.02 RoBERTa-large (Child-TuningD) 13.67\\nRoBERTa-base (LoRA) 3.59 RoBERTa-large (LoRA) 7.50\\nRoBERTa-base (AdaLoRA) 3.57 RoBERTa-large (AdaLoRA) 7.43\\nRoBERTa-base (MAM Adapter) 15.35 RoBERTa-large (MAM Adapter) 37.82'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='RoBERTa-base (ProPELTAdapter) 8.63 RoBERTa-large (ProPELTAdapter) 19.82\\nRoBERTa-base (ProPELTPrefix) 9.47 RoBERTa-large (ProPELTPrefix) 22.85\\nRoBERTa-base (ProPELTLoRA) 8.25 RoBERTa-large (ProPELTLoRA) 19.52\\nT5-base (FT) 25.17 T5-large (FT) 30.17\\nT5-base ((IA)3) 21.36 T5-large ((IA)3) 25.71'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='T5-base ((IA)3) 21.36 T5-large ((IA)3) 25.71\\nT5-base (LoRA) 19.43 T5-large (LoRA) 23.77\\nLLaMA-7B-Alpaca (FT) 169.36 LLaMA-13B-Alpaca (FT) 287.79\\nLLaMA-7B-Alpaca ((IA)3) 128.57 LLaMA-13B-Alpaca ((IA)3) 191.24\\nLLaMA-7B-Alpaca (LoRA) 124.82 LLaMA-13B-Alpaca (LoRA) 174.24'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='LLaMA-7B-Alpaca (QLoRA) 56.46 LLaMA-13B-Alpaca (QLoRA) 66.60\\nmodel on high-resource languages. Artetxe et al. [106] employ\\nsequential adapter [9] to transfer a pretrained monolingual\\nmodel to an unseen language. MAD-X [75], [107] uses'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='language-specific, task-specific, and invertible adapter to learn\\nlanguage-specific and task-specific transformations, as well\\nas address vocabulary mismatches between multilingual and\\ntarget languages in a modular manner, enabling the adaptation'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='of pretrained multilingual models to target languages. MAD-G\\n[108] generates language adapters from language representa-\\ntions based on typological features, allowing the sharing of lin-\\nguistic knowledge across languages for cross-lingual transfer.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='LT-SFT [38] employs sparse fine-tuning to train the model on\\nthe source language and learn task-specific sparse difference\\nvectors for cross-lingual transfer. While BAD-X [109] trains a\\nbilingual language-pair adapter on both the source and target\\nlanguages for zero-shot cross-lingual transfer.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='languages for zero-shot cross-lingual transfer.\\nC. Backdoor Attacks and Defense\\nBackdoor attacks pose a significant security threat, where\\na small portion of training samples are contaminated with\\nmalicious backdoor triggers. When trained on such poisoned'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='datasets, the model behaves normally on benign samples but\\npredicts attacker-selected labels on samples containing the\\npredefined triggers. The susceptibility of PLMs to backdoor at-\\ntacks poses a substantial risk to real-world applications [110].'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='Building on the vulnerability of pretrained weights, Gu et al.\\n[111] employ the PEFT methods to construct backdoor attacks,\\nin which backdoor attacks are directly injected into PEFT\\nmodules. However, Zhu et al. [112] discover that PEFT can\\nserve as a backdoor defense solution by reducing the model'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='capacity via optimizing only a small number of parameters.\\nThe findings from [113] also confirm that PEFT can slightly\\nweaken the backdoor attacks and design a novel trojan attack\\nfor the PEFT paradigm.\\nVI. F URTHER DIRECTIONS\\nA. Lightweight Hybrid PEFT Methods'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='A. Lightweight Hybrid PEFT Methods\\nThere exist many approaches [16], [35], [58], [59], [60],\\n[61], [62] to combine multiple PEFT methods, aiming to\\nleverage the distinctive advantages of each PEFT method and\\nachieve enhanced performance. Nevertheless, the exploration'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='has been limited to PEFT methods such as adapter, LoRA,\\nprefix-tuning, and BitFit, leaving room for further exploitation\\nby incorporating additional combinations of PEFT methods.\\nMoreover, while drawing inspiration from the NAS algorithm,\\nseveral PEFT methods [60], [61] have been investigated using'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='diverse optimization techniques to explore optimal neural\\nnetwork architectures for configuring these PEFT methods.\\nThere remains potential for continued exploration in utilizing\\nother optimization methods to automatically search for neural'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='network architectures and configure specific combinations of\\nPEFT modules at specific layers. Additionally, utilizing mul-\\ntiple PEFT methods typically results in increased parameter\\nand memory usage, although it enhances performance. Hence,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='an intriguing research direction involves investigating how\\nto leverage multiple PEFT methods to improve performance\\nwhile minimizing the number of trainable parameters.\\nB. LoRA-derived PEFT Methods\\nRecently, a multitude of LoRA-based PEFT methods have'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 15}, page_content='emerged, as demonstrated in Fig. 1. These methods further\\nenhance LoRA by incorporating adaptive rank adjustment, un-\\nstructured pruning techniques, weight quantization, and multi-\\ntask integration. This encourages future research to develop'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='17\\nmore LoRA-derived PEFT approaches build upon LoRA.\\nParticular emphasis should be placed on pruning technol-\\nogy and weight quantification. The application of pruning\\ntechniques can be extended not only to AdaLoRA [45] for\\nrank adjustment but also to LoRAPrune [48] for pruning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='both pretrained and LoRA weights. Notably, pruning and\\nweight quantization techniques effectively reduce the number\\nof trainable parameters, compress model size, optimize storage\\nand computational requirements of PLMs (especially LLMs),\\nand enhance their utility and scalability across downstream'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='tasks. These techniques can be further explored in conjunction\\nwith LoRA to unlock synergistic benefits.\\nC. Developing PEFT Library\\nNumerous PEFT methods have emerged, but employing\\nthem is not a straightforward endeavor. To address this\\nchallenge, the PEFT library 8 and AdapterHub 9 have been'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='developed. These libraries integrate commonly used PEFT\\nmethods such as prefix-tuning, LoRA, and AdaLoRA. With\\njust a few lines of code, users can directly invoke these\\nPEFT methods, simplifying their usage. Moreover, both the\\nPEFT and AdapterHub libraries offer a range of examples'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='illustrating how to apply these PEFT methods to various PLMs\\nand LLMs for fine-tuning downstream tasks. However, not\\nall PEFT methods are currently integrated into these two\\nlibraries. Future efforts can be directed towards expanding\\nthe integration of additional methods, further boosting the'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='application development of PEFT methods.\\nD. Explainability of PEFT Methods\\nThough numerous PEFT methods have been proposed, there\\nis a lack of comprehensive studies exploring the reasons\\nbehind their ability to achieve comparable performance and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='reduce trainable parameters. Work from [41] unifies PEFT\\nmethods under the concept of sparse fine-tuned models and\\nprovides a theoretical analysis demonstrating that sparsity can\\nserve as a regularization technique for the original model,\\neffectively controlling the upper bound of stability. While'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='[114] explores and analyzes the express power of LoRA for\\nfully connected neural networks and transformer networks,\\nshowing the conditions under which there exist effective low-\\nrank adapters for a given task. These studies shed light on the\\nworking mechanism and effectiveness of certain PEFT meth-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='ods, but still lack generalization. Future research endeavors\\ncould focus on advancing theoretical studies to unravel the\\nunderlying working mechanisms of PEFT methods.\\nE. Exploring PEFT Methods in Computer Vision and Multi-\\nmodal Learning\\nThough PEFT methods have been extensively studied in'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='NLP, their application in computer vision and multimodal\\nlearning also shows great potential for further exploration.\\nThe sequential adapter in NLP, initially inspired by multi-\\ndomain image classification [77], [115], has paved the way for\\n8https://github.com/huggingface/peft/tree/main'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='8https://github.com/huggingface/peft/tree/main\\n9https://adapterhub.ml/\\nrapid advancements in PEFT methods for PLMs. Moreover,\\nresearchers have increasingly delved into various PEFT tech-\\nniques for computer vision [116], [117], as well as language-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='image and image-audio multimodal learning [118], [119],\\nbuilding upon PEFT methods in NLP [9], [11], [58]. However,\\nthere is still significant room for further exploration and\\nexploitation in these domains. In particular, PEFT methods'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='hold the potential to facilitate cross-modality transfer in multi-\\nmodal learning. By fine-tuning pretrained models using PEFT\\ntechniques, knowledge acquired from one modality can be\\neffectively transferred to another, resulting in improved per-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='formance in multimodal tasks. Consequently, the application\\nof PEFT methods in computer vision and multimodal learning\\nholds tremendous promise as a future research direction.\\nVII. C ONCLUSIONS\\nThis paper presents a comprehensive and structured study of'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='PEFT methods for PLMs. By classifying the PEFT methods in\\nNLP, we identify the main techniques and challenges associ-\\nated with them. We employ several representative PEFT meth-\\nods to fine-tune encoder-based RoBERTa, encoder-decoder-\\nbased T5, and decoder-based LLaMA on various downstream'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='tasks. Experimental results reveal that most PEFT methods\\nsignificantly improve parameter efficiency and achieve com-\\nparable or even better performance compared to full fine-\\ntuning. Additionally, most PEFT methods lower the memory\\nfootprint, with QLoRA drastically reducing the computational'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='memory requirement, and alleviating the memory challenge\\nwhen fine-tuning LLMs. Furthermore, we introduce common\\napplications of PEFT methods and outline future research\\ndirections. As the development of LLMs continues, there is\\na clear need to develop PEFT methods that can effectively'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='reduce computational resource demands and memory usage\\nduring fine-tuning. This survey aims to provide a bird’s-\\neye view of PEFT methods for PLMs and inspiring further\\nresearch in this area.\\nREFERENCES\\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='training of deep bidirectional transformers for language understanding,”\\nin Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Hum.\\nLang. Technol., 2019, pp. 4171–4186.\\n[2] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” in Proc. Int. Conf. Learn. Representations, 2020.\\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n“Language models are unsupervised multitask learners,” OpenAI blog,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='vol. 1, no. 8, p. 9, 2019.\\n[4] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\\nwith a unified text-to-text transformer,” J. Mach. Learn. Res. , vol. 21,\\nno. 1, pp. 5485–5551, 2020.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='no. 1, pp. 5485–5551, 2020.\\n[5] S. Zhang, M. Diab, and L. Zettlemoyer, “Democratizing access to large-\\nscale language models with opt-175b,” Meta AI, 2022.\\n[6] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\\nR. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “Bloom: A 176b-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='parameter open-access multilingual language model,” arXiv preprint\\narXiv:2211.05100, 2022.\\n[7] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\\nOpen and efficient foundation language models,” arXiv preprint'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 16}, page_content='arXiv:2302.13971, 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='18\\n[8] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru,\\nM. Alhammadi, M. Daniele, D. Heslow, J. Launay, Q. Malartic et al.,\\n“The falcon series of language models: Towards open frontier models,”\\nHugging Face repository, 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Hugging Face repository, 2023.\\n[9] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer\\nlearning for nlp,” in Proc. Int. Conf. Mach. Learn. PMLR, 2019, pp.\\n2790–2799.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='2790–2799.\\n[10] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts\\nfor generation,” in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 4582–4597.\\n[11] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='and W. Chen, “LoRA: Low-rank adaptation of large language models,”\\nin Proc. Int. Conf. Learn. Representations , 2022.\\n[12] N. Ding, Y . Qin, G. Yang, F. Wei, Z. Yang, Y . Su, S. Hu, Y . Chen,\\nC.-M. Chan, W. Chen et al. , “Delta tuning: A comprehensive study'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='of parameter efficient methods for pre-trained language models,” arXiv\\npreprint arXiv:2203.06904, 2022.\\n[13] V . Lialin, V . Deshpande, and A. Rumshisky, “Scaling down to\\nscale up: A guide to parameter-efficient fine-tuning,” arXiv preprint\\narXiv:2303.15647, 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='arXiv:2303.15647, 2023.\\n[14] Z. Lin, A. Madotto, and P. Fung, “Exploring versatile generative\\nlanguage model via parameter-efficient transfer learning,” in Proc.\\nFindings Conf. Empir. Methods Natural Lang. Process., 2020, pp. 441–\\n459.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='459.\\n[15] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Y .\\nZhao, Y . Wu, B. Li et al. , “Conditional adapters: Parameter-efficient\\ntransfer learning with fast inference,” arXiv preprint arXiv:2304.04947,\\n2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='2023.\\n[16] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Towards\\na unified view of parameter-efficient transfer learning,” in Proc. Int.\\nConf. Learn. Representations , 2022.\\n[17] A. R ¨uckl´e, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='and I. Gurevych, “AdapterDrop: On the efficiency of adapters in\\ntransformers,” in Proc. Conf. Empir. Methods Natural Lang. Process. ,\\n2021, pp. 7930–7946.\\n[18] H. Zhao, H. Tan, and H. Mei, “Tiny-attention adapter: Contexts are\\nmore important than the number of parameters,” in Proc. Conf. Empir.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Methods Natural Lang. Process. , 2022, pp. 6626–6638.\\n[19] J. Pfeiffer, A. Kamath, A. R ¨uckl´e, K. Cho, and I. Gurevych, “Adapter-\\nFusion: Non-destructive task composition for transfer learning,” in\\nProc. Conf. Eur. Chapter Assoc. Comput. Linguistics , 2021, pp. 487–\\n503.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='503.\\n[20] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao, “Mera:\\nMerging pretrained adapters for few-shot learning,” arXiv preprint\\narXiv:2308.15982, 2023.\\n[21] R. Karimi Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='“Parameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,” in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 565–576.\\n[22] A. Chronopoulou, M. Peters, A. Fraser, and J. Dodge, “AdapterSoup:'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Weight averaging to improve generalization of pretrained language\\nmodels,” in Proc. Findings Assoc. Comput. Linguistics, 2023, pp. 2054–\\n2063.\\n[23] K. Hambardzumyan, H. Khachatrian, and J. May, “W ARP: Word-level\\nAdversarial ReProgramming,” in Proc. Annu. Meeting Assoc. Comput.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Linguistics, Int. Joint Conf. Natural Lang. Process. , 2021, pp. 4921–\\n4933.\\n[24] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\\nparameter-efficient prompt tuning,” in Proc. Conf. Empir. Methods\\nNatural Lang. Process., 2021, pp. 3045–3059.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Natural Lang. Process., 2021, pp. 3045–3059.\\n[25] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang, “Gpt\\nunderstands, too,” arXiv preprint arXiv:2103.10385 , 2021.\\n[26] T. Vu, B. Lester, N. Constant, R. Al-Rfou’, and D. Cer, “SPoT: Better'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='frozen model adaptation through soft prompt transfer,” in Proc. Annu.\\nMeeting Assoc. Comput. Linguistics , 2022, pp. 5039–5059.\\n[27] A. Asai, M. Salehi, M. Peters, and H. Hajishirzi, “ATTEMPT:\\nParameter-efficient multi-task tuning via attentional mixtures of soft'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='prompts,” in Proc. Conf. Empir. Methods Natural Lang. Process., 2022,\\npp. 6655–6672.\\n[28] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim,\\n“Multitask prompt tuning enables parameter-efficient transfer learning,”\\nin Proc. Int. Conf. Learn. Representations , 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='[29] Y .-L. Sung, J. Cho, and M. Bansal, “LST: Ladder side-tuning for\\nparameter and memory efficient transfer learning,” in Proc. Adv. Neural\\nInf. Process. Syst. , 2022.\\n[30] H. Liu, D. Tam, M. Mohammed, J. Mohta, T. Huang, M. Bansal,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='and C. Raffel, “Few-shot parameter-efficient fine-tuning is better and\\ncheaper than in-context learning,” in Proc. Adv. Neural Inf. Process.\\nSyst., 2022.\\n[31] X. Yang, J. Y . Huang, W. Zhou, and M. Chen, “Parameter-efficient\\ntuning with special token adaptation,” in Proc. Conf. Eur. Chapter'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Assoc. Comput. Linguistics , 2023, pp. 865–872.\\n[32] J. Cao, C. Satya Prakash, and W. Hamza, “Attention fusion: a light yet\\nefficient late fusion mechanism for task adaptation in NLU,” in Proc.\\nFindings Assoc. Comput. Linguistics , 2022, pp. 857–866.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='[33] Y . Chen, Q. Fu, G. Fan, L. Du, J.-G. Lou, S. Han, D. Zhang, Z. Li, and\\nY . Xiao, “Hadamard adapter: An extreme parameter-efficient adapter\\ntuning method for pre-trained language models,” in Proc. 32nd ACM\\nInt. Conf. Inf. Knowl. Manage. , 2023, pp. 276–285.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='[34] E. Ben Zaken, Y . Goldberg, and S. Ravfogel, “BitFit: Simple\\nparameter-efficient fine-tuning for transformer-based masked language-\\nmodels,” in Proc. Annu. Meeting Assoc. Comput. Linguistics, 2022, pp.\\n1–9.\\n[35] N. Lawton, A. Kumar, G. Thattai, A. Galstyan, and G. Ver Steeg,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='“Neural architecture search for parameter-efficient fine-tuning of large\\npre-trained language models,” in Proc. Findings Assoc. Comput. Lin-\\nguistics, 2023, pp. 8506–8515.\\n[36] M. Zhao, T. Lin, F. Mi, M. Jaggi, and H. Sch ¨utze, “Masking as an'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='efficient alternative to finetuning for pretrained language models,” in\\nProc. Conf. Empir. Methods Natural Lang. Process. , 2020, pp. 2226–\\n2241.\\n[37] Y .-L. Sung, V . Nair, and C. Raffel, “Training neural networks with\\nfixed sparse masks,” in Proc. Adv. Neural Inf. Process. Syst. , 2021.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='[38] A. Ansell, E. Ponti, A. Korhonen, and I. Vuli ´c, “Composable sparse\\nfine-tuning for cross-lingual transfer,” in Proc. Annu. Meeting Assoc.\\nComput. Linguistics, 2022, pp. 1778–1796.\\n[39] R. Xu, F. Luo, Z. Zhang, C. Tan, B. Chang, S. Huang, and F. Huang,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='“Raise a child in large language model: Towards effective and gen-\\neralizable fine-tuning,” in Proc. Conf. Empir. Methods Natural Lang.\\nProcess., 2021, pp. 9514–9528.\\n[40] D. Guo, A. Rush, and Y . Kim, “Parameter-efficient transfer learning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='with diff pruning,” in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 4884–4896.\\n[41] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, and N. Collier, “On the\\neffectiveness of parameter-efficient fine-tuning,” in Proc. AAAI Conf.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Artif. Intell., vol. 37, no. 11, 2023, pp. 12 799–12 807.\\n[42] A. Aghajanyan, S. Gupta, and L. Zettlemoyer, “Intrinsic dimensionality\\nexplains the effectiveness of language model fine-tuning,” in Proc.\\nAnnu. Meeting Assoc. Comput. Linguistics, Int. Joint Conf. Natural'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Lang. Process., 2021, pp. 7319–7328.\\n[43] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Reza-\\ngholizadeh, “Krona: Parameter efficient tuning with kronecker adapter,”\\narXiv preprint arXiv:2212.10650 , 2022.\\n[44] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, “Dy-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='LoRA: Parameter-efficient tuning of pre-trained models using dynamic\\nsearch-free low-rank adaptation,” in Proc. Conf. Eur. Chapter Assoc.\\nComput. Linguistics, 2023, pp. 3274–3287.\\n[45] Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='T. Zhao, “Adaptive budget allocation for parameter-efficient fine-\\ntuning,” in Proc. Int. Conf. Learn. Representations , 2023.\\n[46] F. Zhang, L. Li, J. Chen, Z. Jiang, B. Wang, and Y . Qian, “Increlora:\\nIncremental parameter allocation method for parameter-efficient fine-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='tuning,” arXiv preprint arXiv:2308.12043 , 2023.\\n[47] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang, “Delta-lora:\\nFine-tuning high-rank parameters with the delta of low-rank matrices,”\\narXiv preprint arXiv:2309.02411 , 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='arXiv preprint arXiv:2309.02411 , 2023.\\n[48] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang et al., “Prun-\\ning meets low-rank parameter-efficient fine-tuning,” arXiv preprint\\narXiv:2305.18403, 2023.\\n[49] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='ficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\\n2023.\\n[50] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang,\\nand Q. Tian, “Qa-lora: Quantization-aware low-rank adaptation of large\\nlanguage models,” arXiv preprint arXiv:2309.14717 , 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='[51] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and\\nT. Zhao, “Loftq: Lora-fine-tuning-aware quantization for large language\\nmodels,” arXiv preprint arXiv:2310.08659 , 2023.\\n[52] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 17}, page_content='Tur, “Empowering parameter-efficient transfer learning by recognizing\\nthe kernel structure in self-attention,” in Proc. Findings Assoc. Comput.\\nLinguistics, 2022, pp. 1375–1388.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='19\\n[53] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison, “Bayesian\\nlow-rank adaptation for large language models,” arXiv preprint\\narXiv:2308.13111, 2023.\\n[54] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li, “Lora-fa: Memory-\\nefficient low-rank adaptation for large language models fine-tuning,”'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='arXiv preprint arXiv:2308.03303 , 2023.\\n[55] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin, “Lorahub:\\nEfficient cross-task generalization via dynamic lora composition,”arXiv\\npreprint arXiv:2307.13269, 2023.\\n[56] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='“Moelora: An moe-based parameter efficient fine-tuning method for\\nmulti-task medical applications,” arXiv preprint arXiv:2310.18339 ,\\n2023.\\n[57] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao,\\n“Parameter efficient multi-task model fusion with partial linearization,”'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='arXiv preprint arXiv:2310.04742 , 2023.\\n[58] R. Karimi Mahabadi, J. Henderson, and S. Ruder, “Compacter: Effi-\\ncient low-rank hypercomplex adapter layers,” Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 34, pp. 1022–1035, 2021.\\n[59] Y . Mao, L. Mathias, R. Hou, A. Almahairi, H. Ma, J. Han, S. Yih, and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='M. Khabsa, “UniPELT: A unified framework for parameter-efficient\\nlanguage model tuning,” in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics, 2022, pp. 6253–6264.\\n[60] H. Zhou, X. Wan, I. Vuli ´c, and A. Korhonen, “Autopeft: Automatic'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='configuration search for parameter-efficient fine-tuning,” arXiv preprint\\narXiv:2301.12132, 2023.\\n[61] S. Hu, Z. Zhang, N. Ding, Y . Wang, Y . Wang, Z. Liu, and M. Sun,\\n“Sparse structure search for delta tuning,” Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 35, pp. 9853–9865, 2022.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Process. Syst., vol. 35, pp. 9853–9865, 2022.\\n[62] J. Chen, A. Zhang, X. Shi, M. Li, A. Smola, and D. Yang, “Parameter-\\nefficient fine-tuning design spaces,” in Proc. Int. Conf. Learn. Repre-\\nsentations, 2023.\\n[63] Y . Wang, S. Agarwal, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='and J. Gao, “AdaMix: Mixture-of-adaptations for parameter-efficient\\nmodel tuning,” in Proc. Conf. Empir. Methods Natural Lang. Process. ,\\n2022, pp. 5744–5760.\\n[64] S. He, L. Ding, D. Dong, J. Zhang, and D. Tao, “SparseAdapter: An\\neasy approach for improving the parameter-efficiency of adapters,” in'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Proc. Findings Conf. Empir. Methods Natural Lang. Process. , 2022,\\npp. 2184–2190.\\n[65] G. Zeng, P. Zhang, and W. Lu, “One network, many masks: Towards\\nmore parameter-efficient transfer learning,” in Proc. Annu. Meeting\\nAssoc. Comput. Linguistics , 2023, pp. 7564–7580.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Assoc. Comput. Linguistics , 2023, pp. 7564–7580.\\n[66] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Proc.\\nAdv. Neural Inf. Process. Syst. , vol. 30, 2017.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Adv. Neural Inf. Process. Syst. , vol. 30, 2017.\\n[67] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\\npp. 770–778.\\n[68] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='preprint arXiv:1607.06450, 2016.\\n[69] L. Xu and W. Wang, “Improving aspect-based sentiment analysis with\\ncontrastive learning,” Natural Language Processing Journal , vol. 3, p.\\n100009, 2023.\\n[70] Y . Xie, W. Yang, L. Tan, K. Xiong, N. J. Yuan, B. Huai, M. Li, and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='J. Lin, “Distant supervision for multi-stage fine-tuning in retrieval-\\nbased question answering,” in Proceedings of The Web Conference ,\\n2020, pp. 2934–2940.\\n[71] R. Dabre, A. Fujita, and C. Chu, “Exploiting multilingualism through'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='multistage fine-tuning for low-resource neural machine translation,” in\\nProc. Conf. Empir. Methods Natural Lang. Process., Int. Joint Conf.\\nNatural Lang. Process., 2019, pp. 1410–1416.\\n[72] M. T. Hosseini, A. Ghaffari, M. S. Tahaei, M. Rezagholizadeh,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='M. Asgharian, and V . P. Nia, “Towards fine-tuning pre-trained language\\nmodels with integer forward and backward propagation,” in Proc.\\nFindings Assoc. Comput. Linguistics , 2023, pp. 1867–1876.\\n[73] S.-i. Amari, “Backpropagation and stochastic gradient descent method,”'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Neurocomputing, vol. 5, no. 4-5, pp. 185–196, 1993.\\n[74] L. Xu, H. Xie, Z. Li, F. L. Wang, W. Wang, and Q. Li, “Contrastive\\nlearning models for sentence representations,” ACM Trans. Intel. Syst.\\nTec., vol. 14, no. 4, pp. 1–34, 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Tec., vol. 14, no. 4, pp. 1–34, 2023.\\n[75] J. Pfeiffer, I. Vuli ´c, I. Gurevych, and S. Ruder, “MAD-X: An Adapter-\\nBased Framework for Multi-Task Cross-Lingual Transfer,” in Proc.\\nConf. Empir. Methods Natural Lang. Process. , 2020, pp. 7654–7673.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='[76] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li, “Counter-\\ninterference adapter for multilingual machine translation,” arXiv\\npreprint arXiv:2104.08154, 2021.\\n[77] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Learning multiple visual'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='domains with residual adapters,” Proc. Adv. Neural Inf. Process. Syst. ,\\nvol. 30, 2017.\\n[78] J. Solomon, F. De Goes, G. Peyr ´e, M. Cuturi, A. Butscher, A. Nguyen,\\nT. Du, and L. Guibas, “Convolutional wasserstein distances: Efficient\\noptimal transportation on geometric domains,” ACM Trans. Graph. ,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='vol. 34, no. 4, pp. 1–11, 2015.\\n[79] S. P. Singh and M. Jaggi, “Model fusion via optimal transport,” Proc.\\nAdv. Neural Inf. Process. Syst. , vol. 33, pp. 22 045–22 055, 2020.\\n[80] D. Ha, A. M. Dai, and Q. V . Le, “Hypernetworks,” in Proc. Int. Conf.\\nLearn. Representations, 2017.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Learn. Representations, 2017.\\n[81] R. Aharoni and Y . Goldberg, “Unsupervised domain clusters in pre-\\ntrained language models,” in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics, 2020, pp. 7747–7763.\\n[82] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='filters for efficient convnets,” inProc. Int. Conf. Learn. Representations,\\n2017.\\n[83] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning, “What does\\nBERT look at? an analysis of BERT’s attention,” in Proc. of 2019 ACL\\nWorkshop BlackboxNLP, 2019, pp. 276–286.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Workshop BlackboxNLP, 2019, pp. 276–286.\\n[84] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky, “Revealing\\nthe dark secrets of BERT,” in Proc. Conf. Empir. Methods Natural\\nLang. Process., Int. Joint Conf. Natural Lang. Process. , 2019, pp.\\n4365–4374.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='4365–4374.\\n[85] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A\\nsurvey,” J. Mach. Learn. Res. , vol. 20, no. 1, pp. 1997–2017, 2019.\\n[86] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding\\nsparse, trainable neural networks,” in Proc. Int. Conf. Learn. Repre-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='sentations, 2019.\\n[87] Q. Le, T. Sarl ´os, and A. Smola, “Fastfood-computing hilbert space\\nexpansions in loglinear time,” in Proc. Int. Conf. Mach. Learn. PMLR,\\n2013, pp. 244–252.\\n[88] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Importance'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='estimation for neural network pruning,” in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit., 2019, pp. 11 264–11 272.\\n[89] V . Sanh, T. Wolf, and A. Rush, “Movement pruning: Adaptive sparsity\\nby fine-tuning,” Proc. Adv. Neural Inf. Process. Syst. , vol. 33, pp.\\n20 378–20 389, 2020.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='20 378–20 389, 2020.\\n[90] D. J. MacKay, “A practical bayesian framework for backpropagation\\nnetworks,” Neural Comput., vol. 4, no. 3, pp. 448–472, 1992.\\n[91] J. Liu, A. Moreau, M. Preuss, J. Rapin, B. Roziere, F. Teytaud, and\\nO. Teytaud, “Versatile black-box optimization,” in Proc. of the 2020'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Genet. and Evolut. Comput. Conf. , 2020, pp. 620–628.\\n[92] G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, and\\nA. Farhadi, “Editing models with task arithmetic,” in Proc. Int. Conf.\\nLearn. Representations, 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Learn. Representations, 2023.\\n[93] J. Zhang, S. Chen, J. Liu, and J. He, “Composing parameter-efficient\\nmodules with arithmetic operations,” arXiv preprint arXiv:2306.14870,\\n2023.\\n[94] P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal, “Resolving'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='interference when merging models,” arXiv preprint arXiv:2306.01708,\\n2023.\\n[95] A. Zhang, Y . Tay, S. Zhang, A. Chan, A. T. Luu, S. Hui, and J. Fu,\\n“Beyond fully-connected layers with quaternions: Parameterization of\\nhypercomplex multiplications with $1/n$ parameters,” in Proc. Int.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='Conf. Learn. Representations , 2021.\\n[96] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='[97] J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin, “Pruning neural\\nnetworks at initialization: Why are we missing the mark?” in Proc. Int.\\nConf. Learn. Representations , 2021.\\n[98] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='and A. Liotta, “Scalable training of artificial neural networks with\\nadaptive sparse connectivity inspired by network science,” Nature\\ncommunications, vol. 9, no. 1, p. 2383, 2018.\\n[99] N. Lee, T. Ajanthan, and P. Torr, “SNIP: Single-shot network pruning'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='based on connection sensitivity,” in Proc. Int. Conf. Learn. Represen-\\ntations, 2019.\\n[100] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\\n“GLUE: A multi-task benchmark and analysis platform for natural\\nlanguage understanding,” in Proc. of 2018 EMNLP Workshop Black-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 18}, page_content='boxNLP, 2018, pp. 353–355.\\n[101] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n“Albert: A lite bert for self-supervised learning of language represen-\\ntations,” in Proc. Int. Conf. Learn. Representations , 2020.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='20\\n[102] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, “Stanford alpaca: An instruction-following llama\\nmodel,” 2023.\\n[103] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='J. Steinhardt, “Measuring massive multitask language understanding,”\\nin Proc. Int. Conf. Learn. Representations , 2021.\\n[104] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\\nautomatic evaluation of machine translation,” in Proc. Annu. Meeting'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Assoc. Comput. Linguistics , 2002, pp. 311–318.\\n[105] A. Bapna and O. Firat, “Simple, scalable adaptation for neural machine\\ntranslation,” in Proc. Conf. Empir. Methods Natural Lang. Process., Int.\\nJoint Conf. Natural Lang. Process. , 2019, pp. 1538–1548.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='[106] M. Artetxe, S. Ruder, and D. Yogatama, “On the cross-lingual transfer-\\nability of monolingual representations,” in Proc. Annu. Meeting Assoc.\\nComput. Linguistics, 2020, pp. 4623–4637.\\n[107] J. Pfeiffer, I. Vuli ´c, I. Gurevych, and S. Ruder, “UNKs everywhere:'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Adapting multilingual language models to new scripts,” in Proc. Conf.\\nEmpir. Methods Natural Lang. Process. , 2021, pp. 10 186–10 203.\\n[108] A. Ansell, E. M. Ponti, J. Pfeiffer, S. Ruder, G. Glava ˇs, I. Vuli ´c, and\\nA. Korhonen, “MAD-G: Multilingual adapter generation for efficient'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='cross-lingual transfer,” in Proc. Findings Conf. Empir. Methods Natural\\nLang. Process., 2021, pp. 4762–4781.\\n[109] M. Parovi ´c, G. Glavaˇs, I. Vuli´c, and A. Korhonen, “BAD-X: Bilingual\\nadapters improve zero-shot cross-lingual transfer,” in Proc. Conf. North'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., 2022,\\npp. 1791–1799.\\n[110] M. T ¨anzer, S. Ruder, and M. Rei, “Memorisation versus generalisa-\\ntion in pre-trained language models,” in Proc. Annu. Meeting Assoc.\\nComput. Linguistics, 2022, pp. 7564–7578.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Comput. Linguistics, 2022, pp. 7564–7578.\\n[111] N. Gu, P. Fu, X. Liu, Z. Liu, Z. Lin, and W. Wang, “A gradient control\\nmethod for backdoor attacks on parameter-efficient tuning,” in Proc.\\nAnnu. Meeting Assoc. Comput. Linguistics , 2023, pp. 3508–3520.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='[112] B. Zhu, Y . Qin, G. Cui, Y . Chen, W. Zhao, C. Fu, Y . Deng, Z. Liu,\\nJ. Wang, W. Wu, M. Sun, and M. Gu, “Moderate-fitting as a natural\\nbackdoor defender for pre-trained language models,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2022.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Neural Inf. Process. Syst. , 2022.\\n[113] L. Hong and T. Wang, “Fewer is more: Trojan attacks on parameter-\\nefficient fine-tuning,” arXiv preprint arXiv:2310.00648 , 2023.\\n[114] Y . Zeng and K. Lee, “The expressive power of low-rank adaptation,”\\narXiv preprint arXiv:2310.17513 , 2023.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='arXiv preprint arXiv:2310.17513 , 2023.\\n[115] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Efficient parametrization of\\nmulti-domain deep neural networks,” in Proc. IEEE Conf. Comput. Vis.\\nPattern Recognit., 2018, pp. 8119–8127.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Pattern Recognit., 2018, pp. 8119–8127.\\n[116] X. He, C. Li, P. Zhang, J. Yang, and X. E. Wang, “Parameter-efficient\\nmodel adaptation for vision transformers,” in Proc. AAAI Conf. Artif.\\nIntell., vol. 37, no. 1, 2023, pp. 817–825.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Intell., vol. 37, no. 1, 2023, pp. 817–825.\\n[117] Z. Xu, Z. Chen, Y . Zhang, Y . Song, X. Wan, and G. Li, “Bridging vision\\nand language encoders: Parameter-efficient tuning for referring image\\nsegmentation,” in IEEE Int. Conf. Comput. Vis. , 2023, pp. 17 503–\\n17 512.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='17 512.\\n[118] Y .-L. Sung, J. Cho, and M. Bansal, “Vl-adapter: Parameter-efficient\\ntransfer learning for vision-and-language tasks,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , 2022, pp. 5227–5237.\\n[119] J. Pan, Z. Lin, X. Zhu, J. Shao, and H. Li, “St-adapter: Parameter-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='efficient image-to-video transfer learning,” Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 35, pp. 26 462–26 477, 2022.\\nLingling Xu (Student Member, IEEE) is cur-\\nrently pursuing her Ph.D. degree at Hong Kong\\nMetropolitan University. She received a Master de-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='gree in Mathematics from Shandong University. Her\\nresearch interests include parameter-efficient fine-\\ntuning, contrastive learning, representation learning,\\nand aspect-based sentiment analysis.\\nHaoran Xie (Senior Member, IEEE) received a\\nPh.D. degree in Computer Science from City Uni-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Ph.D. degree in Computer Science from City Uni-\\nversity of Hong Kong and an Ed.D degree in Digital\\nLearning from the University of Bristol. He is cur-\\nrently the Department Head and Associate Professor\\nat the Department of Computing and Decision Sci-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='at the Department of Computing and Decision Sci-\\nences, Lingnan University, Hong Kong. His research\\ninterests include artificial intelligence, big data, and\\neducational technology. He has published 393 re-\\nsearch publications, including 224 journal articles'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='such as IEEE TPAMI, IEEE TKDE, IEEE TAFFC,\\nand IEEE TCVST. He is the Editor-in-Chief of Natural Language Processing\\nJournal, Computers & Education: Artificial Intelligence and Computers &\\nEducation: X Reality. He has been selected listed as the World’s Top 2%\\nScientists by Stanford University.'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Scientists by Stanford University.\\nSi-Zhao Joe Qin (Fellow, IEEE) received the B.S.\\nand M.S. degrees in automatic control from Ts-\\ninghua University, Beijing, China, in 1984 and 1987,\\nrespectively, and the Ph.D. degree in chemical en-\\ngineering from the University of Maryland, College'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='Park, MD, USA, in 1992. He is currently the Wai\\nKee Kau Chair Professor and President of Lingnan\\nUniversity, Hong Kong. His research interests in-\\nclude data science and analytics, machine learning,\\nprocess monitoring, model predictive control, system'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='identification, smart manufacturing, smart cities, and\\npredictive maintenance. Prof. Qin is a Fellow of the U.S. National Academy\\nof Inventors, IFAC, and AIChE. He was the recipient of the 2022 CAST\\nComputing Award by AIChE, 2022 IEEE CSS Transition to Practice Award,'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='U.S. NSF CAREER Award, and NSF-China Outstanding Young Investigator\\nAward. His h-indices for Web of Science, SCOPUS, and Google Scholar are\\n66, 73, and 84, respectively.\\nXiaohui Tao (Senior Member, IEEE) is currently\\na Full Professor with the University of Southern'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='a Full Professor with the University of Southern\\nQueensland, Toowoomba, QLD, Australia. His re-\\nsearch interests include artificial intelligence, data\\nanalytics, machine learning, knowledge engineering,\\ninformation retrieval, and health informatics. His'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='research outcomes have been published across more\\nthan 150 papers including many top-tier journals and\\nconferences. He is a Senior Member of ACM and\\nthe Vice Chair of IEEE Technical Committee of\\nIntelligent Informatics. He was the recipient of an'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='ARC DP in 2022. He is the Editor-in-Chief of Natural Language Processing\\nJournal.\\nFu Lee Wang (Senior Member, IEEE) received\\nthe B.Eng. degree in computer engineering and the\\nM.Phil. degree in computer science and information\\nsystems from The University of Hong Kong, Hong'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='systems from The University of Hong Kong, Hong\\nKong, and the Ph.D. degree in systems engineering\\nand engineering management from The Chinese\\nUniversity of Hong Kong, Hong Kong. Prof. Wang\\nis the Dean of the School of Science and Tech-\\nnology, Hong Kong Metropolitan University, Hong'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='nology, Hong Kong Metropolitan University, Hong\\nKong. He has over 300 publications in international\\njournals and conferences and led more than 20\\ncompetitive grants with a total greater than HK$20 million. His current\\nresearch interests include educational technology, information retrieval, com-'),\n",
       " Document(metadata={'source': 'documents\\\\peft.pdf', 'page': 19}, page_content='puter graphics, and bioinformatics. Prof. Wang is a fellow of BCS, HKIE and\\nIET and a Senior Member of ACM. He was the Chair of the IEEE Hong\\nKong Section Computer Chapter and ACM Hong Kong Chapter.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=chunk_doc(docs=doc)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = embeddings.embed_query(\"how are you?\")\n",
    "type(e)\n",
    "len(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index before upsert:\n",
      "{'dimension': 768,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "\n",
    "index_name = \"rag-project\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embeddings.dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "    # Wait for index to be ready\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# See that it is empty\n",
    "print(\"Index before upsert:\")\n",
    "print(pc.Index(index_name).describe_index_stats())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-pinecone\n",
      "  Using cached langchain_pinecone-0.2.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-pinecone) (0.3.25)\n",
      "Requirement already satisfied: numpy<2,>=1 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-pinecone) (1.26.4)\n",
      "Requirement already satisfied: pinecone-client<6.0.0,>=5.0.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-pinecone) (5.0.1)\n",
      "Requirement already satisfied: aiohttp<3.10,>=3.9.5 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-pinecone) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (24.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.18.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (9.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (2.10.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (6.0.2)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (0.2.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (24.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (4.12.2)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (4.67.1)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2024.12.14)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-pinecone) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (3.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (2.32.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.0.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-pinecone) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-pinecone) (0.7.0)\n",
      "Requirement already satisfied: colorama in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (0.4.6)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp<3.10,>=3.9.5->langchain-pinecone) (0.2.1)\n",
      "Requirement already satisfied: idna>=2.0 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp<3.10,>=3.9.5->langchain-pinecone) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.0.7)\n",
      "Requirement already satisfied: anyio in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (4.7.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (3.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.3.1)\n",
      "Installing collected packages: langchain-pinecone\n",
      "Successfully installed langchain-pinecone-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\langchain\\langchain\\end-end-genai-education-industry-project\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\LangChain\\LangChain\\End-End-GenAI-Education-Industry-Project\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index after upsert:\n",
      "{'dimension': 768,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "namespace = \"wondervector5000\"\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=doc,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    "    namespace=namespace\n",
    ")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# See how many vectors have been upserted\n",
    "print(\"Index after upsert:\")\n",
    "print(pc.Index(index_name).describe_index_stats())\n",
    "print(\"\\n\")\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '09fdca80-b308-4305-8874-3fc4e282126b',\n",
      "              'metadata': {'page': 4.0,\n",
      "                           'source': 'documents\\\\peft.pdf',\n",
      "                           'text': '5\\n'\n",
      "                                   'Multi-Head Attention\\n'\n",
      "                                   'Layer Normalization\\n'\n",
      "                                   'Layer Normalization\\n'\n",
      "                                   'Feed-Forward Network\\n'\n",
      "                                   '+\\n'\n",
      "                                   '+\\n'\n",
      "                                   'Adapter Network\\n'\n",
      "                                   'Adapter Network\\n'\n",
      "                                   'Nonlinear \\n'\n",
      "                                   'Activation\\n'\n",
      "                                   '+\\n'\n",
      "                                   'Down-projection\\n'\n",
      "                                   'Up-projection\\n'\n",
      "                                   'kd\\n'\n",
      "                                   'k d\\n'\n",
      "                                   '(a) Sequential Adapter\\n'\n",
      "                                   'Multi-Head Attention\\n'\n",
      "                                   'Layer Normalization\\n'\n",
      "                                   'Layer Normalization\\n'\n",
      "                                   'Feed-Forward Network\\n'\n",
      "                                   '+ \\n'\n",
      "                                   '+ \\n'\n",
      "                                   'Hidden States\\n'\n",
      "                                   'W q W k W v \\n'\n",
      "                                   'Q K V P k P v \\n'\n",
      "                                   'Attention\\n'\n",
      "                                   'Pr efix-tuning (b) Prefix-tuning\\n'\n",
      "                                   'Multi-Head Attention\\n'\n",
      "                                   'Layer Normalization\\n'\n",
      "                                   'Layer Normalization\\n'\n",
      "                                   'Feed-Forward Network\\n'\n",
      "                                   '+ \\n'\n",
      "                                   '+ \\n'\n",
      "                                   'Hidden States\\n'\n",
      "                                   'W q W k W v \\n'\n",
      "                                   'Q K V \\n'\n",
      "                                   'Attention LoRA \\n'\n",
      "                                   'LoRA \\n'\n",
      "                                   'LoRA \\n'\n",
      "                                   '+ + + \\n'\n",
      "                                   'Down- \\n'\n",
      "                                   'projection \\n'\n",
      "                                   'Up- \\n'\n",
      "                                   'projection * \\n'\n",
      "                                   'LoRA (c) LoRA\\n'\n",
      "                                   'Fig. 3: The detailed architecture of (a) '\n",
      "                                   'Sequential Adapter, (b) Prefix-tuning, and '\n",
      "                                   '(c) LoRA.\\n'\n",
      "                                   'knowledge across tasks via hypernetworks '\n",
      "                                   'while enabling the\\n'\n",
      "                                   'model to adapt to each task through '\n",
      "                                   'task-specific adapters,\\n'\n",
      "                                   'significantly reducing the number of '\n",
      "                                   'trainable parameters.\\n'\n",
      "                                   'AdapterSoup [22] is developed to address '\n",
      "                                   'cross-domain task\\n'\n",
      "                                   'adaptation, which first trains multiple '\n",
      "                                   'adapters based on var-\\n'\n",
      "                                   'ious domains and then employs domain '\n",
      "                                   'clustering [81] to\\n'\n",
      "                                   'select the most appropriate top-k adapters '\n",
      "                                   'for the new domain.\\n'\n",
      "                                   'Fine-tuning parameters for the new domain '\n",
      "                                   'in AdapterSoup\\n'\n",
      "                                   'are determined by calculating the weighted '\n",
      "                                   'average of the\\n'\n",
      "                                   'selected k adapters. Apart from '\n",
      "                                   'cross-domain task adaptation,\\n'\n",
      "                                   'AdapterSoup can also be used to strengthen '\n",
      "                                   'in-domain results\\n'\n",
      "                                   'via weight averaging of adapters trained '\n",
      "                                   'on the same domain\\n'\n",
      "                                   'but with different hyperparameters.\\n'\n",
      "                                   '2) Soft Prompt-based Fine-tuning: Soft '\n",
      "                                   'prompt fine-tuning\\n'\n",
      "                                   'is a class of methods in which trainable '\n",
      "                                   'continuous vectors,\\n'\n",
      "                                   'known as soft prompts, are inserted into '\n",
      "                                   'the input or hidden\\n'\n",
      "                                   'state of the model. Unlike manually '\n",
      "                                   'designed hard prompts,\\n'\n",
      "                                   'soft prompts are generated by searching '\n",
      "                                   'for prompts in a\\n'\n",
      "                                   'discrete token space based on '\n",
      "                                   'task-specific training data. Soft\\n'\n",
      "                                   'prompts exhibit more flexibility and '\n",
      "                                   'adaptability during fine-\\n'\n",
      "                                   'tuning, as these prompts can be optimized '\n",
      "                                   'and adjusted based\\n'\n",
      "                                   'on the specific task and training data.\\n'\n",
      "                                   'W ARP (Word-level Adversarial '\n",
      "                                   'RePrograming) [23] in-\\n'\n",
      "                                   'serts special prompt tokens [P1], [P2], '\n",
      "                                   '··· , [Pl] and [Mask]\\n'\n",
      "                                   'token before or after the sentences '\n",
      "                                   'relying on the prompt\\n'\n",
      "                                   'template. The training objective is to '\n",
      "                                   'minimize the cross-\\n'\n",
      "                                   'entropy loss between the output of MLM and '\n",
      "                                   'the verbalizer\\n'\n",
      "                                   'tokens [V1], [V2], ··· , [Vc] for classes '\n",
      "                                   '{1, 2, ··· , c}. Only the\\n'\n",
      "                                   'parameters of [P1], [P2], ··· , [Pl] and '\n",
      "                                   '[V1], [V2], ··· , [Vc] are\\n'\n",
      "                                   'trainable, resulting in a significant '\n",
      "                                   'reduction in the number\\n'\n",
      "                                   'of fine-tuning parameters. Prompt-tuning '\n",
      "                                   '[24] incorporates\\n'\n",
      "                                   'additional l learnable prompt tokens, P = '\n",
      "                                   '[P1], [P2], ··· , [Pl],\\n'\n",
      "                                   'into the model input X ∈ Rn×d and then '\n",
      "                                   'concatenates them to\\n'\n",
      "                                   'generate the final input ˆX, the new input '\n",
      "                                   'can be expressed with\\n'\n",
      "                                   'Equation 7. During fine-tuning, only the '\n",
      "                                   'prompt parameters\\n'\n",
      "                                   'of P are updated through gradient descent, '\n",
      "                                   'while pretrained\\n'\n",
      "                                   'parameters remain frozen. Thus, the '\n",
      "                                   'parameter cost of prompt-\\n'\n",
      "                                   'tuning is determined by multiplying the '\n",
      "                                   'prompt length by\\n'\n",
      "                                   'the token embedding dimension, and '\n",
      "                                   'extending the prompt\\n'\n",
      "                                   'length beyond a single token is critical '\n",
      "                                   'for achieving good\\n'\n",
      "                                   'performance.\\n'\n",
      "                                   'ˆX = Concat(P, X) = [P, X] ∈ R(l+n)×d. '\n",
      "                                   '(7)\\n'\n",
      "                                   'Prefix-tuning [10] proposes to prepend '\n",
      "                                   'soft prompts P =\\n'\n",
      "                                   '[P1], [P2], ··· , [Pl] (l denotes the '\n",
      "                                   'length of the prefix) to\\n'\n",
      "                                   'the hidden states of the multi-head '\n",
      "                                   'attention layer, differing\\n'\n",
      "                                   'from prompt-tuning that adds soft prompts '\n",
      "                                   'to the input. To\\n'\n",
      "                                   'ensure stable training, a FFN is '\n",
      "                                   'introduced to parameterize\\n'\n",
      "                                   'the soft prompts, as direct optimization '\n",
      "                                   'of the soft prompts\\n'\n",
      "                                   'can lead to instability. Two sets of '\n",
      "                                   'prefix vectors ˆPk and\\n'\n",
      "                                   'ˆPv are concatenated to the original key ( '\n",
      "                                   'K) and value\\n'\n",
      "                                   '(V ) vectors of the attention layer. The '\n",
      "                                   'self-attention mech-\\n'\n",
      "                                   'anism with prefix-tuning can be '\n",
      "                                   'represented by Equation 8.\\n'\n",
      "                                   'During training, only ˆPk, ˆPv, and the '\n",
      "                                   'parameters of FFN\\n'\n",
      "                                   'are optimized, while all other parameters '\n",
      "                                   'of PLMs remain\\n'\n",
      "                                   'frozen. The structure of prefix-tuning is '\n",
      "                                   'illustrated in Fig. 3.\\n'\n",
      "                                   'After training, the FFN is discarded, and '\n",
      "                                   'only Pk and Pv\\n'\n",
      "                                   'are used for inference. P-tuning [25] also '\n",
      "                                   'considers insert-\\n'\n",
      "                                   'ing the soft prompts [P1], ··· , [Pi], '\n",
      "                                   '[Pi+1], ··· , [Pl] into the\\n'\n",
      "                                   'model input. Nonetheless, P-tuning differs '\n",
      "                                   'by concatenating\\n'\n",
      "                                   'these prompts to form a template and maps '\n",
      "                                   'it to obtain\\n'\n",
      "                                   '{h1, ··· , hi, e(x), hi+1, ··· , hl, '\n",
      "                                   'e(x)}, in which e represents\\n'\n",
      "                                   'pretrained embedding layer. The training '\n",
      "                                   'goal is to optimize\\n'\n",
      "                                   'the continuous prompts {h1, ··· , hl}. As '\n",
      "                                   'the weights of PLMs\\n'\n",
      "                                   'are fixed and only a few parameters need '\n",
      "                                   'to be fine-tuned,\\n'\n",
      "                                   'the template can be effectively learned in '\n",
      "                                   'few-shot learning\\n'\n",
      "                                   'scenarios. P-tuning employs a '\n",
      "                                   'bidirectional long short-term\\n'\n",
      "                                   'memory network (LSTM) with a '\n",
      "                                   'ReLU-activated multilayer\\n'\n",
      "                                   'perceptron (MLP) to initialize the '\n",
      "                                   'embedding of soft prompts\\n'\n",
      "                                   'through MLP(LSTM(h1, ··· , hi): LSTM(hi, '\n",
      "                                   '··· , hl)).\\n'\n",
      "                                   'head = Attn(XWq, [ ˆPk, XWk], [ ˆPv, '\n",
      "                                   'XWv]), (8)\\n'\n",
      "                                   'ˆPk = FFN(Pk), ˆPv = FFN(Pv). (9)\\n'\n",
      "                                   'SPOT (Soft Prompt Transfer) [26] is a '\n",
      "                                   'multitask prompt\\n'\n",
      "                                   'method that builds upon the prompt-tuning, '\n",
      "                                   'in which “prompt\\n'\n",
      "                                   'pertaining” is introduced between PLMs and '\n",
      "                                   'prompt-tuning\\n'\n",
      "                                   'of target tasks. There are two variants of '\n",
      "                                   'SPOT: generic'},\n",
      "              'score': 1.0,\n",
      "              'values': [0.0141993454,\n",
      "                         -0.0496266633,\n",
      "                         -0.0296423212,\n",
      "                         0.0159281511,\n",
      "                         0.0365006886,\n",
      "                         0.0164698,\n",
      "                         0.021202717,\n",
      "                         -0.0197485685,\n",
      "                         0.0279734079,\n",
      "                         -0.00919845607,\n",
      "                         -0.000680952449,\n",
      "                         0.0170372501,\n",
      "                         -0.0130731864,\n",
      "                         0.0111695528,\n",
      "                         0.0147871142,\n",
      "                         -0.0869095549,\n",
      "                         -0.00331377937,\n",
      "                         0.023881048,\n",
      "                         -0.00990640186,\n",
      "                         -0.036288511,\n",
      "                         -0.0317637064,\n",
      "                         -0.0412858576,\n",
      "                         0.0350512117,\n",
      "                         -0.0206865035,\n",
      "                         0.000121377445,\n",
      "                         0.0499955267,\n",
      "                         0.0299350563,\n",
      "                         -0.0472258553,\n",
      "                         -0.0406000763,\n",
      "                         0.0042887181,\n",
      "                         -0.0159599632,\n",
      "                         0.0376906618,\n",
      "                         -0.0305752624,\n",
      "                         -0.0239530243,\n",
      "                         0.0118796704,\n",
      "                         -0.0349760279,\n",
      "                         0.0117237521,\n",
      "                         -0.00841252506,\n",
      "                         -0.0184098557,\n",
      "                         0.0289220475,\n",
      "                         -0.0196330808,\n",
      "                         0.0264375582,\n",
      "                         -0.00510986242,\n",
      "                         -0.0118452022,\n",
      "                         0.0270758644,\n",
      "                         -0.00503222877,\n",
      "                         0.0293479804,\n",
      "                         0.0594019294,\n",
      "                         -0.000575501472,\n",
      "                         -0.0665362477,\n",
      "                         0.0376594439,\n",
      "                         -0.0250523817,\n",
      "                         0.0254465491,\n",
      "                         0.0102180941,\n",
      "                         -0.0517534725,\n",
      "                         0.0155856293,\n",
      "                         0.00103240681,\n",
      "                         -0.00837169867,\n",
      "                         -0.0318198763,\n",
      "                         0.0165367201,\n",
      "                         -0.00491319271,\n",
      "                         -0.0308643971,\n",
      "                         -0.0418249257,\n",
      "                         0.0466377027,\n",
      "                         -0.0368523635,\n",
      "                         -0.0309587251,\n",
      "                         -0.0468284376,\n",
      "                         0.0543392971,\n",
      "                         0.0453864448,\n",
      "                         0.0229967963,\n",
      "                         0.0555811524,\n",
      "                         -0.0116994036,\n",
      "                         0.0832081959,\n",
      "                         -0.0129336677,\n",
      "                         -0.0308888387,\n",
      "                         -0.0784179568,\n",
      "                         0.0259848405,\n",
      "                         0.00076117157,\n",
      "                         0.0134521862,\n",
      "                         0.0328204408,\n",
      "                         -0.00171519199,\n",
      "                         -0.0390116274,\n",
      "                         -0.0761965662,\n",
      "                         -0.024388127,\n",
      "                         -0.0589130148,\n",
      "                         0.0246291831,\n",
      "                         -0.0467457362,\n",
      "                         -0.0355840847,\n",
      "                         -0.0504970364,\n",
      "                         0.0227876361,\n",
      "                         -0.00872309599,\n",
      "                         -0.0182828214,\n",
      "                         0.0419772603,\n",
      "                         -0.066952616,\n",
      "                         -0.025522761,\n",
      "                         0.0700019896,\n",
      "                         0.0189193636,\n",
      "                         -0.0387519747,\n",
      "                         -0.0064908783,\n",
      "                         -0.0436334722,\n",
      "                         -0.0141802495,\n",
      "                         0.0173044745,\n",
      "                         -0.0286916625,\n",
      "                         0.0178335775,\n",
      "                         0.0626750365,\n",
      "                         0.0693647191,\n",
      "                         -0.00349502871,\n",
      "                         0.0423068106,\n",
      "                         -0.0516875461,\n",
      "                         0.0208582692,\n",
      "                         -0.0497884564,\n",
      "                         -0.0255644023,\n",
      "                         -0.0581938177,\n",
      "                         -0.0234556291,\n",
      "                         0.0546354838,\n",
      "                         -0.0187383927,\n",
      "                         0.0318776779,\n",
      "                         0.062736772,\n",
      "                         0.0410146,\n",
      "                         -0.0325658694,\n",
      "                         -0.0238867365,\n",
      "                         0.00809485838,\n",
      "                         0.0299107246,\n",
      "                         -0.0323653482,\n",
      "                         -0.0155208008,\n",
      "                         0.00756705552,\n",
      "                         -0.0148760201,\n",
      "                         0.0189909078,\n",
      "                         0.084189564,\n",
      "                         0.0390569046,\n",
      "                         -0.0139930854,\n",
      "                         -0.05837458,\n",
      "                         0.000678786251,\n",
      "                         0.0298664179,\n",
      "                         -0.000637882389,\n",
      "                         0.0455713943,\n",
      "                         0.0885180682,\n",
      "                         0.0293176826,\n",
      "                         0.0421285555,\n",
      "                         0.0102997534,\n",
      "                         -0.0634638667,\n",
      "                         0.00466745533,\n",
      "                         0.0333441496,\n",
      "                         0.0204660371,\n",
      "                         -0.0267721079,\n",
      "                         0.021049818,\n",
      "                         0.00542692933,\n",
      "                         0.0283888038,\n",
      "                         0.036475718,\n",
      "                         -0.0560280718,\n",
      "                         -0.0269288104,\n",
      "                         0.00238260976,\n",
      "                         -0.0928191841,\n",
      "                         0.0169662088,\n",
      "                         0.0258172974,\n",
      "                         -0.0192354266,\n",
      "                         -0.0281836577,\n",
      "                         0.0122087924,\n",
      "                         -0.0257541891,\n",
      "                         0.0146317566,\n",
      "                         0.0623638071,\n",
      "                         0.0327425413,\n",
      "                         0.0419651382,\n",
      "                         0.0410159267,\n",
      "                         0.00664084917,\n",
      "                         0.0198543966,\n",
      "                         -0.0033926866,\n",
      "                         0.0034367058,\n",
      "                         -0.00202725292,\n",
      "                         0.0332007632,\n",
      "                         -0.000855608785,\n",
      "                         0.0246710144,\n",
      "                         -0.079729639,\n",
      "                         -0.0336719975,\n",
      "                         -0.00147717551,\n",
      "                         -0.0177637544,\n",
      "                         -0.0136930095,\n",
      "                         0.0246175192,\n",
      "                         -0.0385153033,\n",
      "                         -0.0224650819,\n",
      "                         -0.0378088616,\n",
      "                         -0.0906783044,\n",
      "                         0.0234664492,\n",
      "                         0.047592856,\n",
      "                         0.0493473858,\n",
      "                         -0.00822859909,\n",
      "                         0.0674473494,\n",
      "                         -0.0507932752,\n",
      "                         -0.0515688136,\n",
      "                         -0.0272825081,\n",
      "                         0.0135695012,\n",
      "                         -0.011990387,\n",
      "                         -0.0503570437,\n",
      "                         -0.000970957743,\n",
      "                         -0.040232148,\n",
      "                         0.00378461182,\n",
      "                         0.0248343702,\n",
      "                         0.0114202183,\n",
      "                         0.0243184362,\n",
      "                         -0.0459126979,\n",
      "                         -0.000692542235,\n",
      "                         0.106493413,\n",
      "                         0.00365049532,\n",
      "                         -0.0267293826,\n",
      "                         -0.00843894761,\n",
      "                         -0.00633579586,\n",
      "                         0.0488640107,\n",
      "                         -0.0294158347,\n",
      "                         -0.0366276428,\n",
      "                         0.0545125119,\n",
      "                         -0.0638308153,\n",
      "                         0.00870765932,\n",
      "                         0.0117341103,\n",
      "                         0.0641402677,\n",
      "                         0.060257718,\n",
      "                         0.0265712,\n",
      "                         0.00761563377,\n",
      "                         -0.0249257106,\n",
      "                         0.0228733588,\n",
      "                         -0.0365314037,\n",
      "                         0.0110628679,\n",
      "                         -0.0291449744,\n",
      "                         0.0218568891,\n",
      "                         -0.00717796758,\n",
      "                         -0.0192525517,\n",
      "                         0.0139339687,\n",
      "                         -0.0536625423,\n",
      "                         0.0351317786,\n",
      "                         0.0458078608,\n",
      "                         -0.0602199212,\n",
      "                         0.0222694613,\n",
      "                         0.0537349433,\n",
      "                         0.0489225872,\n",
      "                         -0.00336640794,\n",
      "                         0.0430847816,\n",
      "                         -0.00654201582,\n",
      "                         -0.000949038,\n",
      "                         0.0170435682,\n",
      "                         0.0720501393,\n",
      "                         -0.0189421121,\n",
      "                         -0.0291125607,\n",
      "                         0.0246859286,\n",
      "                         0.0451560766,\n",
      "                         0.0507887825,\n",
      "                         -0.0582368635,\n",
      "                         -0.097117722,\n",
      "                         -0.0291057248,\n",
      "                         0.0473819263,\n",
      "                         -0.00152188679,\n",
      "                         0.023877183,\n",
      "                         -0.0282528345,\n",
      "                         -0.0665381327,\n",
      "                         -0.00438844692,\n",
      "                         0.0237991884,\n",
      "                         -0.106057636,\n",
      "                         0.0354222506,\n",
      "                         -0.0195379201,\n",
      "                         -0.0265580174,\n",
      "                         0.000342985208,\n",
      "                         -0.00821385905,\n",
      "                         -0.00238063489,\n",
      "                         0.0129725235,\n",
      "                         0.0111757061,\n",
      "                         -0.0333594121,\n",
      "                         0.0234096032,\n",
      "                         -0.00228959951,\n",
      "                         0.0290970355,\n",
      "                         -0.0686351955,\n",
      "                         -0.00772833684,\n",
      "                         0.00930796191,\n",
      "                         -0.00967178214,\n",
      "                         -0.0495261773,\n",
      "                         0.0278127734,\n",
      "                         0.00187928812,\n",
      "                         -0.00659835059,\n",
      "                         0.00981824659,\n",
      "                         -0.0232370794,\n",
      "                         0.0356308818,\n",
      "                         0.0383205414,\n",
      "                         -0.00269956863,\n",
      "                         -0.0037962643,\n",
      "                         0.0168993,\n",
      "                         0.0326707028,\n",
      "                         -0.00992211048,\n",
      "                         -0.0247746892,\n",
      "                         0.0166895315,\n",
      "                         -0.00112742151,\n",
      "                         0.000500274473,\n",
      "                         0.00109324791,\n",
      "                         -0.0196930021,\n",
      "                         0.0018664028,\n",
      "                         0.0239018835,\n",
      "                         0.0134541932,\n",
      "                         -0.0509648956,\n",
      "                         0.00614757,\n",
      "                         0.0263731871,\n",
      "                         -0.00862197671,\n",
      "                         0.0140047707,\n",
      "                         0.025307402,\n",
      "                         -0.0196889043,\n",
      "                         0.00192651409,\n",
      "                         0.0113054207,\n",
      "                         -0.0254682563,\n",
      "                         -0.0976378098,\n",
      "                         -0.00332691194,\n",
      "                         0.00458145188,\n",
      "                         -0.0375619382,\n",
      "                         -0.0254085846,\n",
      "                         0.00991840195,\n",
      "                         -0.0311482903,\n",
      "                         0.0354126059,\n",
      "                         -0.00108234363,\n",
      "                         -0.0395472497,\n",
      "                         -0.0230994169,\n",
      "                         0.0444281884,\n",
      "                         0.0685715,\n",
      "                         -0.00234778086,\n",
      "                         0.0115471613,\n",
      "                         0.00372049352,\n",
      "                         0.0281978417,\n",
      "                         -0.0130897081,\n",
      "                         0.0164019447,\n",
      "                         -0.0356199108,\n",
      "                         -0.0310942046,\n",
      "                         -0.0331561305,\n",
      "                         0.0341511704,\n",
      "                         -0.0277007595,\n",
      "                         -0.0198933315,\n",
      "                         -0.0113888374,\n",
      "                         0.0179656968,\n",
      "                         -0.00544529315,\n",
      "                         0.00978555623,\n",
      "                         -0.0371303111,\n",
      "                         0.0214638,\n",
      "                         -0.0498989187,\n",
      "                         -0.015603561,\n",
      "                         -0.0489760712,\n",
      "                         -0.00689887442,\n",
      "                         -0.0185030233,\n",
      "                         -0.0204535723,\n",
      "                         -0.00665310724,\n",
      "                         0.00827666186,\n",
      "                         -0.00452184863,\n",
      "                         -0.0300433375,\n",
      "                         0.0174224749,\n",
      "                         -0.0106644351,\n",
      "                         -0.0274967,\n",
      "                         -0.0164877977,\n",
      "                         0.0668963417,\n",
      "                         -0.033665318,\n",
      "                         0.0246493462,\n",
      "                         0.0117136491,\n",
      "                         -0.0457162447,\n",
      "                         0.00889002718,\n",
      "                         0.0291911941,\n",
      "                         -0.0896786526,\n",
      "                         0.0741972774,\n",
      "                         -0.00365554937,\n",
      "                         0.053589303,\n",
      "                         0.00146119914,\n",
      "                         -0.0197844859,\n",
      "                         0.0183697753,\n",
      "                         -0.00691985199,\n",
      "                         -0.0274739955,\n",
      "                         -0.00747127319,\n",
      "                         0.029524073,\n",
      "                         0.00515802298,\n",
      "                         0.0384594388,\n",
      "                         -0.0348210447,\n",
      "                         0.0564954765,\n",
      "                         0.0194159653,\n",
      "                         -0.0266166758,\n",
      "                         0.0061313347,\n",
      "                         -0.0360491835,\n",
      "                         0.0294630043,\n",
      "                         -0.0318506695,\n",
      "                         -0.0115580978,\n",
      "                         -0.0455627553,\n",
      "                         0.0375662558,\n",
      "                         0.0192737356,\n",
      "                         -0.0387309603,\n",
      "                         -0.0168360472,\n",
      "                         0.0597254075,\n",
      "                         0.0397781543,\n",
      "                         -0.0275032539,\n",
      "                         0.000405321567,\n",
      "                         0.0218253564,\n",
      "                         0.0426880941,\n",
      "                         -0.0490892231,\n",
      "                         0.0614911169,\n",
      "                         0.00796294771,\n",
      "                         0.0108429072,\n",
      "                         0.0685502,\n",
      "                         0.0213342924,\n",
      "                         0.00328166131,\n",
      "                         -0.0109255584,\n",
      "                         -0.0108303595,\n",
      "                         -0.0419339612,\n",
      "                         -0.0129701933,\n",
      "                         0.0115343323,\n",
      "                         -0.00705319364,\n",
      "                         -0.0390285403,\n",
      "                         -0.0743208379,\n",
      "                         -0.0354480967,\n",
      "                         -0.0443818606,\n",
      "                         -0.020913342,\n",
      "                         -0.00324813672,\n",
      "                         -0.0368685909,\n",
      "                         -0.017315397,\n",
      "                         -0.0657577962,\n",
      "                         -0.00548314024,\n",
      "                         0.0375103615,\n",
      "                         0.032525681,\n",
      "                         -0.0299275331,\n",
      "                         -0.0891676545,\n",
      "                         0.0046197637,\n",
      "                         0.0126765,\n",
      "                         -0.024307942,\n",
      "                         -0.0265375376,\n",
      "                         0.00678220857,\n",
      "                         -0.0280115772,\n",
      "                         -0.036780417,\n",
      "                         0.027113406,\n",
      "                         0.0095419,\n",
      "                         -0.0713303536,\n",
      "                         -0.0494935773,\n",
      "                         -0.00896255858,\n",
      "                         0.00544265285,\n",
      "                         0.0443348922,\n",
      "                         0.0190461371,\n",
      "                         0.0313063264,\n",
      "                         0.00729907025,\n",
      "                         -0.0113408724,\n",
      "                         -0.00210803142,\n",
      "                         0.0067897127,\n",
      "                         0.00548894051,\n",
      "                         0.0235058144,\n",
      "                         0.0156758036,\n",
      "                         -0.028567791,\n",
      "                         -0.0432516634,\n",
      "                         -0.00720252702,\n",
      "                         -0.00631792704,\n",
      "                         0.00697581796,\n",
      "                         0.0297123622,\n",
      "                         -0.0549033023,\n",
      "                         -0.0591533929,\n",
      "                         -0.00896589179,\n",
      "                         -0.0197297186,\n",
      "                         0.0426006,\n",
      "                         -0.114203349,\n",
      "                         0.0165139195,\n",
      "                         -0.0257134791,\n",
      "                         -0.00887549203,\n",
      "                         -0.0450618118,\n",
      "                         -0.00160275446,\n",
      "                         -0.00816209801,\n",
      "                         0.0198968016,\n",
      "                         0.0464651734,\n",
      "                         0.0386473835,\n",
      "                         -0.013645947,\n",
      "                         0.0271702055,\n",
      "                         -0.0257062837,\n",
      "                         -0.0246030707,\n",
      "                         -0.0549432486,\n",
      "                         0.017854834,\n",
      "                         -0.0275068097,\n",
      "                         -0.00411915733,\n",
      "                         -0.0501093902,\n",
      "                         0.0391649529,\n",
      "                         0.0548661798,\n",
      "                         -0.0019538675,\n",
      "                         -0.00514466409,\n",
      "                         0.0353017449,\n",
      "                         -0.0914756283,\n",
      "                         -0.0245357137,\n",
      "                         0.0138216028,\n",
      "                         -0.0784943178,\n",
      "                         0.0424704924,\n",
      "                         -0.0395115055,\n",
      "                         0.0147604672,\n",
      "                         -0.0295383707,\n",
      "                         -0.00310626836,\n",
      "                         0.0533926859,\n",
      "                         -0.010572846,\n",
      "                         -0.0292055495,\n",
      "                         0.0107582361,\n",
      "                         0.0280010365,\n",
      "                         0.0151907075,\n",
      "                         -0.0341755077,\n",
      "                         0.0619874112,\n",
      "                         0.0296177603,\n",
      "                         0.0402317643,\n",
      "                         -0.034073744,\n",
      "                         -0.0324640647,\n",
      "                         -0.0320455842,\n",
      "                         0.00048690298,\n",
      "                         -0.00632707775,\n",
      "                         0.0425464213,\n",
      "                         0.0559368879,\n",
      "                         0.0152125815,\n",
      "                         0.0034554631,\n",
      "                         -0.0358290747,\n",
      "                         -0.016024027,\n",
      "                         0.0198415145,\n",
      "                         0.0433374718,\n",
      "                         -0.0683811,\n",
      "                         0.0464589894,\n",
      "                         0.0550475307,\n",
      "                         0.0154425707,\n",
      "                         -0.00975745264,\n",
      "                         0.0143275289,\n",
      "                         -0.00982202496,\n",
      "                         0.00407702709,\n",
      "                         -0.0325463489,\n",
      "                         0.0255985465,\n",
      "                         -0.0374786332,\n",
      "                         -0.0472824499,\n",
      "                         0.0188806932,\n",
      "                         0.0309805404,\n",
      "                         0.00418220554,\n",
      "                         0.0412941,\n",
      "                         -0.0152101796,\n",
      "                         -0.120071396,\n",
      "                         -0.0157235172,\n",
      "                         0.0514927432,\n",
      "                         -0.0233883169,\n",
      "                         -0.0572715141,\n",
      "                         -0.0305697694,\n",
      "                         -0.020951584,\n",
      "                         0.0180419628,\n",
      "                         0.0185764823,\n",
      "                         0.0817481279,\n",
      "                         -0.0180681571,\n",
      "                         0.00495418115,\n",
      "                         0.0189491045,\n",
      "                         -0.00463837059,\n",
      "                         0.0180503186,\n",
      "                         0.00666040368,\n",
      "                         0.0549317673,\n",
      "                         -0.00323592662,\n",
      "                         0.0287715103,\n",
      "                         0.000391676615,\n",
      "                         0.00322620338,\n",
      "                         0.0220254604,\n",
      "                         -0.0207962785,\n",
      "                         -0.0170060731,\n",
      "                         -0.0258969516,\n",
      "                         -0.0902863666,\n",
      "                         0.0732628629,\n",
      "                         -0.0218954515,\n",
      "                         -0.0278154258,\n",
      "                         -0.00413545454,\n",
      "                         -0.00241160858,\n",
      "                         -0.0176266525,\n",
      "                         0.0523181148,\n",
      "                         -0.0368298851,\n",
      "                         -0.00854627229,\n",
      "                         0.0285167806,\n",
      "                         -0.0104036219,\n",
      "                         -0.0701638609,\n",
      "                         -0.000923876243,\n",
      "                         0.00478172535,\n",
      "                         0.000464957207,\n",
      "                         0.0148500446,\n",
      "                         0.0358669721,\n",
      "                         -0.0121234795,\n",
      "                         -0.0215784498,\n",
      "                         -0.0276854672,\n",
      "                         0.046192877,\n",
      "                         -0.0318500698,\n",
      "                         0.0202215202,\n",
      "                         -0.00453095278,\n",
      "                         0.000636258454,\n",
      "                         0.0372183807,\n",
      "                         0.0535678603,\n",
      "                         -0.0511039905,\n",
      "                         -0.0568807,\n",
      "                         0.011813635,\n",
      "                         -0.030295141,\n",
      "                         0.0188602768,\n",
      "                         0.0650347,\n",
      "                         -0.0223300196,\n",
      "                         -0.00150033575,\n",
      "                         0.0558284894,\n",
      "                         0.00486158067,\n",
      "                         0.000986509724,\n",
      "                         0.0458162539,\n",
      "                         0.0576825328,\n",
      "                         0.013844463,\n",
      "                         -0.000638023601,\n",
      "                         0.00383281591,\n",
      "                         0.073385,\n",
      "                         -0.0328789577,\n",
      "                         -0.0459686778,\n",
      "                         -0.0041892454,\n",
      "                         0.0152211329,\n",
      "                         -0.00420416938,\n",
      "                         -0.0635812134,\n",
      "                         -0.0129661737,\n",
      "                         0.0380702838,\n",
      "                         0.0182675272,\n",
      "                         -0.0647373348,\n",
      "                         0.0612910874,\n",
      "                         -0.0400199816,\n",
      "                         0.0452840738,\n",
      "                         -0.0181287117,\n",
      "                         0.0201990195,\n",
      "                         -0.00598954828,\n",
      "                         -0.00182331225,\n",
      "                         0.0199321955,\n",
      "                         -0.0550679341,\n",
      "                         -0.00683923298,\n",
      "                         -0.00871626846,\n",
      "                         -0.0114470208,\n",
      "                         -0.0549271405,\n",
      "                         -0.0217359178,\n",
      "                         0.0815719813,\n",
      "                         0.0547315404,\n",
      "                         -0.0266411882,\n",
      "                         -0.0233535562,\n",
      "                         0.00816623494,\n",
      "                         0.0211053565,\n",
      "                         0.0145897148,\n",
      "                         -0.0306742433,\n",
      "                         0.0116965557,\n",
      "                         0.00831502862,\n",
      "                         -0.00496493327,\n",
      "                         -0.0756913349,\n",
      "                         0.0656456277,\n",
      "                         0.0332161039,\n",
      "                         0.0437414907,\n",
      "                         -0.012587796,\n",
      "                         0.0158991292,\n",
      "                         -0.00574342487,\n",
      "                         -0.0343652368,\n",
      "                         0.0159317683,\n",
      "                         -0.0224786531,\n",
      "                         0.0168486889,\n",
      "                         0.0196393225,\n",
      "                         0.0132588204,\n",
      "                         -0.0548161305,\n",
      "                         0.0134693515,\n",
      "                         0.0214757528,\n",
      "                         -0.0183521453,\n",
      "                         -0.00952054374,\n",
      "                         0.0439548455,\n",
      "                         0.0384667292,\n",
      "                         -0.135174587,\n",
      "                         -0.0660510585,\n",
      "                         -0.00227351277,\n",
      "                         -0.0638283491,\n",
      "                         0.0145176118,\n",
      "                         0.0366522036,\n",
      "                         -0.00600980222,\n",
      "                         -0.00399034703,\n",
      "                         0.0366265923,\n",
      "                         -0.014688571,\n",
      "                         -0.0569449291,\n",
      "                         0.022497151,\n",
      "                         -0.0118919145,\n",
      "                         -0.0266422983,\n",
      "                         -0.0303797796,\n",
      "                         -0.00471949,\n",
      "                         0.0185203329,\n",
      "                         0.0378694423,\n",
      "                         -0.0571196154,\n",
      "                         -0.0414678603,\n",
      "                         -0.0642413348,\n",
      "                         -0.0516726784,\n",
      "                         0.00380635308,\n",
      "                         -0.0534401946,\n",
      "                         0.0338603444,\n",
      "                         0.0250331257,\n",
      "                         -0.00782784354,\n",
      "                         0.0631462857,\n",
      "                         -0.0321551189,\n",
      "                         -0.0140140941,\n",
      "                         0.0448310748,\n",
      "                         0.0213464536,\n",
      "                         0.0639706105,\n",
      "                         -0.0305531248,\n",
      "                         -0.0195161588,\n",
      "                         -0.0367991515,\n",
      "                         0.0110787898,\n",
      "                         -0.0304690264,\n",
      "                         -0.0033733577,\n",
      "                         0.0389827117,\n",
      "                         0.0122150397,\n",
      "                         -0.0423318371,\n",
      "                         -0.0159406029,\n",
      "                         0.00254869857,\n",
      "                         -0.0362239517,\n",
      "                         -0.0111809364,\n",
      "                         0.0191056393,\n",
      "                         0.0465543456,\n",
      "                         0.0378140435,\n",
      "                         -0.0374286957,\n",
      "                         -0.0524405912,\n",
      "                         0.0200619809,\n",
      "                         0.0730031729,\n",
      "                         0.016392963,\n",
      "                         -0.0332125388,\n",
      "                         -0.0266324617,\n",
      "                         0.0288113598,\n",
      "                         0.0290231183,\n",
      "                         -0.022446312,\n",
      "                         0.0287810862,\n",
      "                         0.00128555845,\n",
      "                         0.0326528698,\n",
      "                         0.0158876646,\n",
      "                         0.0669979677,\n",
      "                         -0.0720322132,\n",
      "                         0.000501299684,\n",
      "                         0.0101770535,\n",
      "                         0.0030236009,\n",
      "                         -0.0378155485,\n",
      "                         0.0170912016,\n",
      "                         -0.0265827645,\n",
      "                         -0.0351189226,\n",
      "                         0.0392953493,\n",
      "                         0.0110166902,\n",
      "                         0.0487154685,\n",
      "                         0.00269958,\n",
      "                         -0.0395115763,\n",
      "                         -0.0179793127,\n",
      "                         0.0738647878,\n",
      "                         -0.0259074084,\n",
      "                         -0.0306102261,\n",
      "                         -0.0179780554,\n",
      "                         -0.00434775185,\n",
      "                         0.0427178442,\n",
      "                         -0.012424984,\n",
      "                         0.0214446243,\n",
      "                         -0.00732938945,\n",
      "                         -0.0355128273,\n",
      "                         0.0665955544,\n",
      "                         0.0108056078,\n",
      "                         0.0264549349,\n",
      "                         -0.0391715094,\n",
      "                         -0.0621454976,\n",
      "                         -0.0559792928,\n",
      "                         0.0116009982,\n",
      "                         0.0546093807,\n",
      "                         0.0687530339,\n",
      "                         -0.00656257663,\n",
      "                         -0.105560824,\n",
      "                         0.00432644039,\n",
      "                         -0.000507290766,\n",
      "                         0.0244229157,\n",
      "                         -0.0889500529,\n",
      "                         -0.0896450058,\n",
      "                         0.0179537646,\n",
      "                         -0.018188661,\n",
      "                         0.0297510251,\n",
      "                         0.0259775389,\n",
      "                         -0.0459951945,\n",
      "                         0.00332449889,\n",
      "                         0.0124872588,\n",
      "                         0.0109223956,\n",
      "                         0.0142357247,\n",
      "                         -0.012047654,\n",
      "                         0.00412455108,\n",
      "                         -0.0194523726,\n",
      "                         0.00632468611,\n",
      "                         0.0144383218,\n",
      "                         -0.015203584,\n",
      "                         -0.0324220695,\n",
      "                         0.0488239229]}],\n",
      " 'namespace': 'wondervector5000',\n",
      " 'usage': {'read_units': 6}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "namespace = \"wondervector5000\"\n",
    "\n",
    "for ids in index.list(namespace=namespace):\n",
    "    query = index.query(\n",
    "        id=ids[0], \n",
    "        namespace=namespace, \n",
    "        top_k=1,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    print(query)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retiver_query(query,k=2):\n",
    "    matching_result=index.similarity_search(query,k=k)\n",
    "    return matching_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LangChain\\LangChain\\End-End-GenAI-Education-Industry-Project\\venv\\lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "retriever=docsearch.as_retriever()\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What are the peft?\n",
      "\n",
      "Answer without knowledge:\n",
      "\n",
      " PEFT stands for **Parameter-Efficient Fine-Tuning**.  It's a collection of techniques in machine learning, specifically for large language models (LLMs), that aim to adapt a pre-trained model to a new task or dataset with minimal changes to the model's parameters.  Instead of fine-tuning all the parameters of the massive model (which is computationally expensive and resource-intensive), PEFT methods focus on modifying only a small subset of parameters, or adding a small number of new parameters.\n",
      "\n",
      "This makes PEFT techniques significantly more efficient than full fine-tuning in terms of:\n",
      "\n",
      "* **Computational cost:** Requires less computing power and time.\n",
      "* **Memory usage:** Needs less memory to train and deploy.\n",
      "* **Storage space:** The adapted model requires less storage.\n",
      "\n",
      "Several popular PEFT methods exist, including:\n",
      "\n",
      "* **Adapter modules:**  These add small, task-specific modules to the pre-trained model, allowing the model to learn new information without altering the original weights.\n",
      "* **Prefix-tuning:**  Learns a small set of prefix tokens that are prepended to the input, guiding the model's behavior for the specific task.\n",
      "* **Prompt tuning:**  Similar to prefix tuning, but optimizes the prompt itself instead of adding new parameters.\n",
      "* **LoRA (Low-Rank Adaptation):** Decomposes the weight updates into low-rank matrices, significantly reducing the number of parameters that need to be trained. This is currently one of the most popular PEFT methods.\n",
      "* **BitFit:** Only updates the bias terms of the model's layers.\n",
      "\n",
      "The choice of PEFT method depends on the specific task and the available resources.  They are crucial for making LLMs more accessible and adaptable to a wider range of applications, especially when dealing with limited computational resources.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What are the peft?\"\n",
    "answer1_without_knowledge = llm.invoke(query1)\n",
    "\n",
    "print(\"Query 1:\", query1)\n",
    "print(\"\\nAnswer without knowledge:\\n\\n\", answer1_without_knowledge.content)\n",
    "print(\"\\n\")\n",
    "time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Explain Full Fine-tuning of PLMs?\n",
      "\n",
      "Answer without knowledge:\n",
      "\n",
      " Full fine-tuning, in the context of Pre-trained Language Models (PLMs), refers to a training approach where **all** the parameters of the pre-trained model are updated during the fine-tuning process.  This contrasts with other methods like adapter-based fine-tuning or parameter-efficient fine-tuning, which only adjust a small subset of the parameters.\n",
      "\n",
      "Here's a breakdown of full fine-tuning:\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "1. **Start with a pre-trained PLM:**  You begin with a large language model (like BERT, RoBERTa, GPT-3, etc.) that has already been trained on a massive text corpus.  This pre-training provides the model with a strong foundation in language understanding.\n",
      "\n",
      "2. **Prepare a downstream task dataset:** You gather a dataset specific to the task you want the model to perform (e.g., sentiment analysis, question answering, text classification). This dataset needs to be properly formatted and prepared for training.\n",
      "\n",
      "3. **Fine-tune the entire model:**  The entire model's parameters are treated as trainable. The model is then trained on the downstream task dataset using backpropagation and an optimization algorithm (like Adam). The model's weights are adjusted to minimize the loss function specific to the downstream task.\n",
      "\n",
      "4. **Evaluate performance:** After training, the model's performance is evaluated on a held-out test set to assess its effectiveness on the target task.\n",
      "\n",
      "\n",
      "**Advantages of Full Fine-tuning:**\n",
      "\n",
      "* **High performance:** Often achieves state-of-the-art results on many downstream tasks, especially when sufficient data is available.  The model can fully adapt to the nuances of the specific task.\n",
      "* **Simplicity:**  The process is relatively straightforward to implement.  No complex architectural modifications are needed.\n",
      "\n",
      "**Disadvantages of Full Fine-tuning:**\n",
      "\n",
      "* **Computational cost:** Requires significant computational resources (GPU memory and processing power) due to the large number of parameters being updated. This can be prohibitive for resource-constrained environments.\n",
      "* **Catastrophic forgetting:**  The model might \"forget\" some of the knowledge learned during pre-training, especially if the downstream task dataset is small or significantly different from the pre-training data.\n",
      "* **Overfitting:**  With limited data, the model might overfit to the training data, leading to poor generalization to unseen data.  Regularization techniques are often necessary to mitigate this risk.\n",
      "\n",
      "\n",
      "**When to use Full Fine-tuning:**\n",
      "\n",
      "Full fine-tuning is a good choice when:\n",
      "\n",
      "* You have sufficient computational resources.\n",
      "* You have a relatively large downstream task dataset.\n",
      "* High performance is paramount, and you're willing to invest the computational cost.\n",
      "\n",
      "\n",
      "In summary, full fine-tuning is a powerful technique for adapting PLMs to specific downstream tasks, but it comes with significant computational demands and potential risks.  The decision to use it should be made carefully, considering the available resources and the characteristics of the downstream task and dataset.  Parameter-efficient fine-tuning methods are often preferred when resources are limited.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"Explain Full Fine-tuning of PLMs?\"\n",
    "answer1_without_knowledge = llm.invoke(query1)\n",
    "\n",
    "print(\"Query 1:\", query1)\n",
    "print(\"\\nAnswer without knowledge:\\n\\n\", answer1_without_knowledge.content)\n",
    "print(\"\\n\")\n",
    "time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
